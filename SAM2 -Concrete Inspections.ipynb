{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Collecting torch==2.5.1\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp312-cp312-win_amd64.whl (2449.3 MB)\n",
      "     ---------------------------------------- 0.0/2.4 GB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.4 GB 30.8 MB/s eta 0:01:20\n",
      "     ---------------------------------------- 0.0/2.4 GB 28.2 MB/s eta 0:01:27\n",
      "     ---------------------------------------- 0.0/2.4 GB 29.6 MB/s eta 0:01:23\n",
      "     ---------------------------------------- 0.0/2.4 GB 30.3 MB/s eta 0:01:20\n",
      "      --------------------------------------- 0.0/2.4 GB 31.0 MB/s eta 0:01:19\n",
      "      --------------------------------------- 0.0/2.4 GB 31.4 MB/s eta 0:01:17\n",
      "      --------------------------------------- 0.0/2.4 GB 31.3 MB/s eta 0:01:17\n",
      "      --------------------------------------- 0.1/2.4 GB 31.9 MB/s eta 0:01:16\n",
      "      --------------------------------------- 0.1/2.4 GB 32.3 MB/s eta 0:01:15\n",
      "     - -------------------------------------- 0.1/2.4 GB 32.3 MB/s eta 0:01:14\n",
      "     - -------------------------------------- 0.1/2.4 GB 32.2 MB/s eta 0:01:14\n",
      "     - -------------------------------------- 0.1/2.4 GB 31.8 MB/s eta 0:01:15\n",
      "     - -------------------------------------- 0.1/2.4 GB 31.8 MB/s eta 0:01:15\n",
      "     - -------------------------------------- 0.1/2.4 GB 31.6 MB/s eta 0:01:15\n",
      "     - -------------------------------------- 0.1/2.4 GB 31.8 MB/s eta 0:01:14\n",
      "     - -------------------------------------- 0.1/2.4 GB 30.8 MB/s eta 0:01:17\n",
      "     - -------------------------------------- 0.1/2.4 GB 30.8 MB/s eta 0:01:17\n",
      "     - -------------------------------------- 0.1/2.4 GB 30.2 MB/s eta 0:01:18\n",
      "     - -------------------------------------- 0.1/2.4 GB 30.1 MB/s eta 0:01:18\n",
      "     -- ------------------------------------- 0.1/2.4 GB 30.0 MB/s eta 0:01:18\n",
      "     -- ------------------------------------- 0.1/2.4 GB 30.2 MB/s eta 0:01:17\n",
      "     -- ------------------------------------- 0.1/2.4 GB 30.2 MB/s eta 0:01:17\n",
      "     -- ------------------------------------- 0.1/2.4 GB 30.0 MB/s eta 0:01:17\n",
      "     -- ------------------------------------- 0.1/2.4 GB 29.8 MB/s eta 0:01:18\n",
      "     -- ------------------------------------- 0.2/2.4 GB 29.6 MB/s eta 0:01:18\n",
      "     -- ------------------------------------- 0.2/2.4 GB 29.5 MB/s eta 0:01:18\n",
      "     -- ------------------------------------- 0.2/2.4 GB 29.4 MB/s eta 0:01:18\n",
      "     -- ------------------------------------- 0.2/2.4 GB 29.2 MB/s eta 0:01:19\n",
      "     -- ------------------------------------- 0.2/2.4 GB 29.1 MB/s eta 0:01:19\n",
      "     -- ------------------------------------- 0.2/2.4 GB 29.0 MB/s eta 0:01:19\n",
      "     --- ------------------------------------ 0.2/2.4 GB 28.9 MB/s eta 0:01:19\n",
      "     --- ------------------------------------ 0.2/2.4 GB 28.9 MB/s eta 0:01:19\n",
      "     --- ------------------------------------ 0.2/2.4 GB 28.8 MB/s eta 0:01:19\n",
      "     --- ------------------------------------ 0.2/2.4 GB 28.7 MB/s eta 0:01:19\n",
      "     --- ------------------------------------ 0.2/2.4 GB 28.7 MB/s eta 0:01:19\n",
      "     --- ------------------------------------ 0.2/2.4 GB 28.6 MB/s eta 0:01:19\n",
      "     --- ------------------------------------ 0.2/2.4 GB 28.5 MB/s eta 0:01:19\n",
      "     --- ------------------------------------ 0.2/2.4 GB 28.5 MB/s eta 0:01:19\n",
      "     --- ------------------------------------ 0.2/2.4 GB 28.6 MB/s eta 0:01:18\n",
      "     --- ------------------------------------ 0.2/2.4 GB 28.6 MB/s eta 0:01:18\n",
      "     --- ------------------------------------ 0.2/2.4 GB 28.6 MB/s eta 0:01:18\n",
      "     ---- ----------------------------------- 0.2/2.4 GB 28.4 MB/s eta 0:01:18\n",
      "     ---- ----------------------------------- 0.3/2.4 GB 28.5 MB/s eta 0:01:18\n",
      "     ---- ----------------------------------- 0.3/2.4 GB 28.6 MB/s eta 0:01:17\n",
      "     ---- ----------------------------------- 0.3/2.4 GB 28.7 MB/s eta 0:01:17\n",
      "     ---- ----------------------------------- 0.3/2.4 GB 28.6 MB/s eta 0:01:17\n",
      "     ---- ----------------------------------- 0.3/2.4 GB 28.7 MB/s eta 0:01:16\n",
      "     ---- ----------------------------------- 0.3/2.4 GB 28.6 MB/s eta 0:01:16\n",
      "     ---- ----------------------------------- 0.3/2.4 GB 28.5 MB/s eta 0:01:16\n",
      "     ---- ----------------------------------- 0.3/2.4 GB 27.9 MB/s eta 0:01:18\n",
      "     ---- ----------------------------------- 0.3/2.4 GB 27.9 MB/s eta 0:01:18\n",
      "     ---- ----------------------------------- 0.3/2.4 GB 28.0 MB/s eta 0:01:17\n",
      "     ----- ---------------------------------- 0.3/2.4 GB 27.7 MB/s eta 0:01:18\n",
      "     ----- ---------------------------------- 0.3/2.4 GB 27.3 MB/s eta 0:01:19\n",
      "     ----- ---------------------------------- 0.3/2.4 GB 27.1 MB/s eta 0:01:19\n",
      "     ----- ---------------------------------- 0.3/2.4 GB 26.8 MB/s eta 0:01:20\n",
      "     ----- ---------------------------------- 0.3/2.4 GB 26.4 MB/s eta 0:01:21\n",
      "     ----- ---------------------------------- 0.3/2.4 GB 26.2 MB/s eta 0:01:22\n",
      "     ----- ---------------------------------- 0.3/2.4 GB 26.0 MB/s eta 0:01:22\n",
      "     ----- ---------------------------------- 0.3/2.4 GB 25.9 MB/s eta 0:01:22\n",
      "     ----- ---------------------------------- 0.3/2.4 GB 25.9 MB/s eta 0:01:22\n",
      "     ----- ---------------------------------- 0.3/2.4 GB 25.9 MB/s eta 0:01:22\n",
      "     ----- ---------------------------------- 0.4/2.4 GB 25.9 MB/s eta 0:01:21\n",
      "     ----- ---------------------------------- 0.4/2.4 GB 25.9 MB/s eta 0:01:21\n",
      "     ----- ---------------------------------- 0.4/2.4 GB 26.2 MB/s eta 0:01:20\n",
      "     ------ --------------------------------- 0.4/2.4 GB 26.4 MB/s eta 0:01:19\n",
      "     ------ --------------------------------- 0.4/2.4 GB 26.4 MB/s eta 0:01:19\n",
      "     ------ --------------------------------- 0.4/2.4 GB 26.4 MB/s eta 0:01:19\n",
      "     ------ --------------------------------- 0.4/2.4 GB 26.4 MB/s eta 0:01:19\n",
      "     ------ --------------------------------- 0.4/2.4 GB 26.3 MB/s eta 0:01:19\n",
      "     ------ --------------------------------- 0.4/2.4 GB 26.4 MB/s eta 0:01:18\n",
      "     ------ --------------------------------- 0.4/2.4 GB 26.6 MB/s eta 0:01:17\n",
      "     ------ --------------------------------- 0.4/2.4 GB 26.9 MB/s eta 0:01:16\n",
      "     ------ --------------------------------- 0.4/2.4 GB 26.9 MB/s eta 0:01:16\n",
      "     ------ --------------------------------- 0.4/2.4 GB 26.9 MB/s eta 0:01:16\n",
      "     ------- -------------------------------- 0.4/2.4 GB 26.9 MB/s eta 0:01:15\n",
      "     ------- -------------------------------- 0.4/2.4 GB 26.9 MB/s eta 0:01:15\n",
      "     ------- -------------------------------- 0.4/2.4 GB 26.9 MB/s eta 0:01:15\n",
      "     ------- -------------------------------- 0.5/2.4 GB 27.0 MB/s eta 0:01:14\n",
      "     ------- -------------------------------- 0.5/2.4 GB 27.2 MB/s eta 0:01:14\n",
      "     ------- -------------------------------- 0.5/2.4 GB 27.3 MB/s eta 0:01:13\n",
      "     ------- -------------------------------- 0.5/2.4 GB 27.4 MB/s eta 0:01:13\n",
      "     ------- -------------------------------- 0.5/2.4 GB 27.6 MB/s eta 0:01:12\n",
      "     ------- -------------------------------- 0.5/2.4 GB 27.8 MB/s eta 0:01:11\n",
      "     -------- ------------------------------- 0.5/2.4 GB 27.9 MB/s eta 0:01:11\n",
      "     -------- ------------------------------- 0.5/2.4 GB 28.0 MB/s eta 0:01:10\n",
      "     -------- ------------------------------- 0.5/2.4 GB 28.1 MB/s eta 0:01:10\n",
      "     -------- ------------------------------- 0.5/2.4 GB 28.1 MB/s eta 0:01:09\n",
      "     -------- ------------------------------- 0.5/2.4 GB 28.3 MB/s eta 0:01:09\n",
      "     -------- ------------------------------- 0.5/2.4 GB 28.3 MB/s eta 0:01:08\n",
      "     -------- ------------------------------- 0.5/2.4 GB 28.4 MB/s eta 0:01:08\n",
      "     -------- ------------------------------- 0.5/2.4 GB 28.4 MB/s eta 0:01:08\n",
      "     -------- ------------------------------- 0.5/2.4 GB 28.3 MB/s eta 0:01:08\n",
      "     --------- ------------------------------ 0.6/2.4 GB 29.0 MB/s eta 0:01:06\n",
      "     --------- ------------------------------ 0.6/2.4 GB 29.0 MB/s eta 0:01:06\n",
      "     --------- ------------------------------ 0.6/2.4 GB 29.0 MB/s eta 0:01:06\n",
      "     --------- ------------------------------ 0.6/2.4 GB 29.3 MB/s eta 0:01:05\n",
      "     --------- ------------------------------ 0.6/2.4 GB 30.0 MB/s eta 0:01:03\n",
      "     --------- ------------------------------ 0.6/2.4 GB 29.3 MB/s eta 0:01:05\n",
      "     --------- ------------------------------ 0.6/2.4 GB 29.8 MB/s eta 0:01:03\n",
      "     --------- ------------------------------ 0.6/2.4 GB 30.3 MB/s eta 0:01:02\n",
      "     --------- ------------------------------ 0.6/2.4 GB 30.4 MB/s eta 0:01:01\n",
      "     --------- ------------------------------ 0.6/2.4 GB 30.4 MB/s eta 0:01:01\n",
      "     --------- ------------------------------ 0.6/2.4 GB 30.4 MB/s eta 0:01:01\n",
      "     ---------- ----------------------------- 0.6/2.4 GB 30.4 MB/s eta 0:01:01\n",
      "     ---------- ----------------------------- 0.6/2.4 GB 30.3 MB/s eta 0:01:01\n",
      "     ---------- ----------------------------- 0.6/2.4 GB 30.4 MB/s eta 0:01:01\n",
      "     ---------- ----------------------------- 0.6/2.4 GB 30.4 MB/s eta 0:01:00\n",
      "     ---------- ----------------------------- 0.6/2.4 GB 30.5 MB/s eta 0:01:00\n",
      "     ---------- ----------------------------- 0.6/2.4 GB 30.7 MB/s eta 0:00:59\n",
      "     ---------- ----------------------------- 0.7/2.4 GB 30.8 MB/s eta 0:00:59\n",
      "     ---------- ----------------------------- 0.7/2.4 GB 30.6 MB/s eta 0:00:59\n",
      "     ---------- ----------------------------- 0.7/2.4 GB 30.6 MB/s eta 0:00:59\n",
      "     ---------- ----------------------------- 0.7/2.4 GB 30.6 MB/s eta 0:00:59\n",
      "     ----------- ---------------------------- 0.7/2.4 GB 30.5 MB/s eta 0:00:58\n",
      "     ----------- ---------------------------- 0.7/2.4 GB 30.7 MB/s eta 0:00:58\n",
      "     ----------- ---------------------------- 0.7/2.4 GB 30.8 MB/s eta 0:00:58\n",
      "     ----------- ---------------------------- 0.7/2.4 GB 31.1 MB/s eta 0:00:57\n",
      "     ----------- ---------------------------- 0.7/2.4 GB 31.3 MB/s eta 0:00:56\n",
      "     ----------- ---------------------------- 0.7/2.4 GB 31.3 MB/s eta 0:00:56\n",
      "     ----------- ---------------------------- 0.7/2.4 GB 31.3 MB/s eta 0:00:56\n",
      "     ----------- ---------------------------- 0.7/2.4 GB 31.5 MB/s eta 0:00:55\n",
      "     ----------- ---------------------------- 0.7/2.4 GB 31.3 MB/s eta 0:00:55\n",
      "     ------------ --------------------------- 0.7/2.4 GB 31.1 MB/s eta 0:00:56\n",
      "     ------------ --------------------------- 0.7/2.4 GB 30.8 MB/s eta 0:00:56\n",
      "     ------------ --------------------------- 0.7/2.4 GB 30.5 MB/s eta 0:00:56\n",
      "     ------------ --------------------------- 0.8/2.4 GB 30.3 MB/s eta 0:00:57\n",
      "     ------------ --------------------------- 0.8/2.4 GB 30.3 MB/s eta 0:00:56\n",
      "     ------------ --------------------------- 0.8/2.4 GB 30.2 MB/s eta 0:00:56\n",
      "     ------------ --------------------------- 0.8/2.4 GB 30.2 MB/s eta 0:00:56\n",
      "     ------------ --------------------------- 0.8/2.4 GB 30.2 MB/s eta 0:00:56\n",
      "     ------------ --------------------------- 0.8/2.4 GB 30.2 MB/s eta 0:00:56\n",
      "     ------------ --------------------------- 0.8/2.4 GB 30.2 MB/s eta 0:00:55\n",
      "     ------------- -------------------------- 0.8/2.4 GB 30.1 MB/s eta 0:00:55\n",
      "     ------------- -------------------------- 0.8/2.4 GB 30.2 MB/s eta 0:00:55\n",
      "     ------------- -------------------------- 0.8/2.4 GB 30.3 MB/s eta 0:00:55\n",
      "     ------------- -------------------------- 0.8/2.4 GB 30.4 MB/s eta 0:00:54\n",
      "     ------------- -------------------------- 0.8/2.4 GB 30.4 MB/s eta 0:00:54\n",
      "     ------------- -------------------------- 0.8/2.4 GB 30.6 MB/s eta 0:00:53\n",
      "     ------------- -------------------------- 0.8/2.4 GB 31.5 MB/s eta 0:00:52\n",
      "     ------------- -------------------------- 0.8/2.4 GB 31.7 MB/s eta 0:00:51\n",
      "     ------------- -------------------------- 0.9/2.4 GB 31.8 MB/s eta 0:00:51\n",
      "     -------------- ------------------------- 0.9/2.4 GB 31.9 MB/s eta 0:00:50\n",
      "     -------------- ------------------------- 0.9/2.4 GB 32.1 MB/s eta 0:00:50\n",
      "     -------------- ------------------------- 0.9/2.4 GB 32.3 MB/s eta 0:00:49\n",
      "     -------------- ------------------------- 0.9/2.4 GB 32.4 MB/s eta 0:00:49\n",
      "     -------------- ------------------------- 0.9/2.4 GB 32.4 MB/s eta 0:00:49\n",
      "     -------------- ------------------------- 0.9/2.4 GB 32.4 MB/s eta 0:00:48\n",
      "     -------------- ------------------------- 0.9/2.4 GB 31.6 MB/s eta 0:00:50\n",
      "     -------------- ------------------------- 0.9/2.4 GB 31.4 MB/s eta 0:00:50\n",
      "     -------------- ------------------------- 0.9/2.4 GB 31.3 MB/s eta 0:00:50\n",
      "     -------------- ------------------------- 0.9/2.4 GB 31.1 MB/s eta 0:00:50\n",
      "     --------------- ------------------------ 0.9/2.4 GB 31.2 MB/s eta 0:00:49\n",
      "     --------------- ------------------------ 0.9/2.4 GB 31.2 MB/s eta 0:00:49\n",
      "     --------------- ------------------------ 0.9/2.4 GB 31.2 MB/s eta 0:00:49\n",
      "     --------------- ------------------------ 0.9/2.4 GB 31.2 MB/s eta 0:00:49\n",
      "     --------------- ------------------------ 0.9/2.4 GB 30.7 MB/s eta 0:00:50\n",
      "     --------------- ------------------------ 0.9/2.4 GB 30.7 MB/s eta 0:00:49\n",
      "     --------------- ------------------------ 1.0/2.4 GB 30.7 MB/s eta 0:00:49\n",
      "     --------------- ------------------------ 1.0/2.4 GB 30.6 MB/s eta 0:00:49\n",
      "     --------------- ------------------------ 1.0/2.4 GB 30.5 MB/s eta 0:00:49\n",
      "     --------------- ------------------------ 1.0/2.4 GB 30.5 MB/s eta 0:00:49\n",
      "     ---------------- ----------------------- 1.0/2.4 GB 30.5 MB/s eta 0:00:49\n",
      "     ---------------- ----------------------- 1.0/2.4 GB 30.4 MB/s eta 0:00:49\n",
      "     ---------------- ----------------------- 1.0/2.4 GB 30.3 MB/s eta 0:00:49\n",
      "     ---------------- ----------------------- 1.0/2.4 GB 30.4 MB/s eta 0:00:48\n",
      "     ---------------- ----------------------- 1.0/2.4 GB 30.8 MB/s eta 0:00:47\n",
      "     ---------------- ----------------------- 1.0/2.4 GB 31.0 MB/s eta 0:00:47\n",
      "     ---------------- ----------------------- 1.0/2.4 GB 31.2 MB/s eta 0:00:46\n",
      "     ---------------- ----------------------- 1.0/2.4 GB 31.2 MB/s eta 0:00:46\n",
      "     ---------------- ----------------------- 1.0/2.4 GB 31.3 MB/s eta 0:00:46\n",
      "     ---------------- ----------------------- 1.0/2.4 GB 31.2 MB/s eta 0:00:46\n",
      "     ----------------- ---------------------- 1.0/2.4 GB 31.2 MB/s eta 0:00:46\n",
      "     ----------------- ---------------------- 1.1/2.4 GB 31.2 MB/s eta 0:00:45\n",
      "     ----------------- ---------------------- 1.1/2.4 GB 31.2 MB/s eta 0:00:45\n",
      "     ----------------- ---------------------- 1.1/2.4 GB 31.2 MB/s eta 0:00:45\n",
      "     ----------------- ---------------------- 1.1/2.4 GB 31.2 MB/s eta 0:00:45\n",
      "     ----------------- ---------------------- 1.1/2.4 GB 31.2 MB/s eta 0:00:44\n",
      "     ----------------- ---------------------- 1.1/2.4 GB 31.1 MB/s eta 0:00:44\n",
      "     ----------------- ---------------------- 1.1/2.4 GB 30.8 MB/s eta 0:00:45\n",
      "     ----------------- ---------------------- 1.1/2.4 GB 30.8 MB/s eta 0:00:44\n",
      "     ------------------ --------------------- 1.1/2.4 GB 30.8 MB/s eta 0:00:44\n",
      "     ------------------ --------------------- 1.1/2.4 GB 30.6 MB/s eta 0:00:44\n",
      "     ------------------ --------------------- 1.1/2.4 GB 30.5 MB/s eta 0:00:44\n",
      "     ------------------ --------------------- 1.1/2.4 GB 30.4 MB/s eta 0:00:44\n",
      "     ------------------ --------------------- 1.1/2.4 GB 30.1 MB/s eta 0:00:44\n",
      "     ------------------ --------------------- 1.1/2.4 GB 30.0 MB/s eta 0:00:44\n",
      "     ------------------ --------------------- 1.1/2.4 GB 29.9 MB/s eta 0:00:44\n",
      "     ------------------ --------------------- 1.1/2.4 GB 29.7 MB/s eta 0:00:44\n",
      "     ------------------ --------------------- 1.2/2.4 GB 29.6 MB/s eta 0:00:44\n",
      "     ------------------ --------------------- 1.2/2.4 GB 29.5 MB/s eta 0:00:44\n",
      "     ------------------- -------------------- 1.2/2.4 GB 30.2 MB/s eta 0:00:43\n",
      "     ------------------- -------------------- 1.2/2.4 GB 30.3 MB/s eta 0:00:43\n",
      "     ------------------- -------------------- 1.2/2.4 GB 30.4 MB/s eta 0:00:42\n",
      "     ------------------- -------------------- 1.2/2.4 GB 30.4 MB/s eta 0:00:42\n",
      "     ------------------- -------------------- 1.2/2.4 GB 30.4 MB/s eta 0:00:42\n",
      "     ------------------- -------------------- 1.2/2.4 GB 30.3 MB/s eta 0:00:42\n",
      "     ------------------- -------------------- 1.2/2.4 GB 30.6 MB/s eta 0:00:41\n",
      "     ------------------- -------------------- 1.2/2.4 GB 30.2 MB/s eta 0:00:42\n",
      "     ------------------- -------------------- 1.2/2.4 GB 29.9 MB/s eta 0:00:42\n",
      "     ------------------- -------------------- 1.2/2.4 GB 29.8 MB/s eta 0:00:42\n",
      "     ------------------- -------------------- 1.2/2.4 GB 29.6 MB/s eta 0:00:42\n",
      "     ------------------- -------------------- 1.2/2.4 GB 29.3 MB/s eta 0:00:42\n",
      "     -------------------- ------------------- 1.2/2.4 GB 29.1 MB/s eta 0:00:42\n",
      "     -------------------- ------------------- 1.2/2.4 GB 29.0 MB/s eta 0:00:43\n",
      "     -------------------- ------------------- 1.2/2.4 GB 29.0 MB/s eta 0:00:42\n",
      "     -------------------- ------------------- 1.2/2.4 GB 28.8 MB/s eta 0:00:42\n",
      "     -------------------- ------------------- 1.2/2.4 GB 28.8 MB/s eta 0:00:42\n",
      "     -------------------- ------------------- 1.3/2.4 GB 29.0 MB/s eta 0:00:42\n",
      "     -------------------- ------------------- 1.3/2.4 GB 29.0 MB/s eta 0:00:41\n",
      "     -------------------- ------------------- 1.3/2.4 GB 29.0 MB/s eta 0:00:41\n",
      "     -------------------- ------------------- 1.3/2.4 GB 29.0 MB/s eta 0:00:41\n",
      "     -------------------- ------------------- 1.3/2.4 GB 28.9 MB/s eta 0:00:41\n",
      "     --------------------- ------------------ 1.3/2.4 GB 28.9 MB/s eta 0:00:41\n",
      "     --------------------- ------------------ 1.3/2.4 GB 28.9 MB/s eta 0:00:40\n",
      "     --------------------- ------------------ 1.3/2.4 GB 28.9 MB/s eta 0:00:40\n",
      "     --------------------- ------------------ 1.3/2.4 GB 28.6 MB/s eta 0:00:40\n",
      "     --------------------- ------------------ 1.3/2.4 GB 28.6 MB/s eta 0:00:40\n",
      "     --------------------- ------------------ 1.3/2.4 GB 28.6 MB/s eta 0:00:40\n",
      "     --------------------- ------------------ 1.3/2.4 GB 28.5 MB/s eta 0:00:40\n",
      "     --------------------- ------------------ 1.3/2.4 GB 28.5 MB/s eta 0:00:40\n",
      "     --------------------- ------------------ 1.3/2.4 GB 28.4 MB/s eta 0:00:40\n",
      "     --------------------- ------------------ 1.3/2.4 GB 28.3 MB/s eta 0:00:40\n",
      "     ---------------------- ----------------- 1.3/2.4 GB 28.3 MB/s eta 0:00:39\n",
      "     ---------------------- ----------------- 1.4/2.4 GB 28.4 MB/s eta 0:00:39\n",
      "     ---------------------- ----------------- 1.4/2.4 GB 28.4 MB/s eta 0:00:39\n",
      "     ---------------------- ----------------- 1.4/2.4 GB 28.5 MB/s eta 0:00:38\n",
      "     ---------------------- ----------------- 1.4/2.4 GB 28.5 MB/s eta 0:00:38\n",
      "     ---------------------- ----------------- 1.4/2.4 GB 28.5 MB/s eta 0:00:38\n",
      "     ---------------------- ----------------- 1.4/2.4 GB 28.7 MB/s eta 0:00:37\n",
      "     ---------------------- ----------------- 1.4/2.4 GB 28.7 MB/s eta 0:00:37\n",
      "     ---------------------- ----------------- 1.4/2.4 GB 28.9 MB/s eta 0:00:37\n",
      "     ---------------------- ----------------- 1.4/2.4 GB 29.0 MB/s eta 0:00:36\n",
      "     ----------------------- ---------------- 1.4/2.4 GB 29.1 MB/s eta 0:00:36\n",
      "     ----------------------- ---------------- 1.4/2.4 GB 29.1 MB/s eta 0:00:36\n",
      "     ----------------------- ---------------- 1.4/2.4 GB 29.3 MB/s eta 0:00:35\n",
      "     ----------------------- ---------------- 1.4/2.4 GB 29.6 MB/s eta 0:00:35\n",
      "     ----------------------- ---------------- 1.4/2.4 GB 29.5 MB/s eta 0:00:35\n",
      "     ----------------------- ---------------- 1.4/2.4 GB 29.6 MB/s eta 0:00:34\n",
      "     ----------------------- ---------------- 1.5/2.4 GB 29.6 MB/s eta 0:00:34\n",
      "     ----------------------- ---------------- 1.5/2.4 GB 29.7 MB/s eta 0:00:34\n",
      "     ------------------------ --------------- 1.5/2.4 GB 30.6 MB/s eta 0:00:32\n",
      "     ------------------------ --------------- 1.5/2.4 GB 31.0 MB/s eta 0:00:32\n",
      "     ------------------------ --------------- 1.5/2.4 GB 31.4 MB/s eta 0:00:31\n",
      "     ------------------------ --------------- 1.5/2.4 GB 31.7 MB/s eta 0:00:31\n",
      "     ------------------------ --------------- 1.5/2.4 GB 31.8 MB/s eta 0:00:30\n",
      "     ------------------------ --------------- 1.5/2.4 GB 31.7 MB/s eta 0:00:30\n",
      "     ------------------------ --------------- 1.5/2.4 GB 31.7 MB/s eta 0:00:30\n",
      "     ------------------------ --------------- 1.5/2.4 GB 30.9 MB/s eta 0:00:31\n",
      "     ------------------------ --------------- 1.5/2.4 GB 30.9 MB/s eta 0:00:31\n",
      "     ------------------------ --------------- 1.5/2.4 GB 30.9 MB/s eta 0:00:30\n",
      "     ------------------------ --------------- 1.5/2.4 GB 31.0 MB/s eta 0:00:30\n",
      "     ------------------------- -------------- 1.5/2.4 GB 31.0 MB/s eta 0:00:30\n",
      "     ------------------------- -------------- 1.5/2.4 GB 31.2 MB/s eta 0:00:30\n",
      "     ------------------------- -------------- 1.5/2.4 GB 31.2 MB/s eta 0:00:29\n",
      "     ------------------------- -------------- 1.6/2.4 GB 31.2 MB/s eta 0:00:29\n",
      "     ------------------------- -------------- 1.6/2.4 GB 31.4 MB/s eta 0:00:29\n",
      "     ------------------------- -------------- 1.6/2.4 GB 31.7 MB/s eta 0:00:28\n",
      "     ------------------------- -------------- 1.6/2.4 GB 31.6 MB/s eta 0:00:28\n",
      "     ------------------------- -------------- 1.6/2.4 GB 31.8 MB/s eta 0:00:28\n",
      "     ------------------------- -------------- 1.6/2.4 GB 31.9 MB/s eta 0:00:27\n",
      "     -------------------------- ------------- 1.6/2.4 GB 32.0 MB/s eta 0:00:27\n",
      "     -------------------------- ------------- 1.6/2.4 GB 32.2 MB/s eta 0:00:27\n",
      "     -------------------------- ------------- 1.6/2.4 GB 32.2 MB/s eta 0:00:27\n",
      "     -------------------------- ------------- 1.6/2.4 GB 32.3 MB/s eta 0:00:26\n",
      "     -------------------------- ------------- 1.6/2.4 GB 32.2 MB/s eta 0:00:26\n",
      "     -------------------------- ------------- 1.6/2.4 GB 32.0 MB/s eta 0:00:26\n",
      "     -------------------------- ------------- 1.6/2.4 GB 32.2 MB/s eta 0:00:26\n",
      "     -------------------------- ------------- 1.6/2.4 GB 32.0 MB/s eta 0:00:26\n",
      "     -------------------------- ------------- 1.6/2.4 GB 32.2 MB/s eta 0:00:25\n",
      "     --------------------------- ------------ 1.7/2.4 GB 32.0 MB/s eta 0:00:25\n",
      "     --------------------------- ------------ 1.7/2.4 GB 31.9 MB/s eta 0:00:25\n",
      "     --------------------------- ------------ 1.7/2.4 GB 31.9 MB/s eta 0:00:25\n",
      "     --------------------------- ------------ 1.7/2.4 GB 31.7 MB/s eta 0:00:25\n",
      "     --------------------------- ------------ 1.7/2.4 GB 31.7 MB/s eta 0:00:25\n",
      "     --------------------------- ------------ 1.7/2.4 GB 31.4 MB/s eta 0:00:25\n",
      "     --------------------------- ------------ 1.7/2.4 GB 31.4 MB/s eta 0:00:25\n",
      "     --------------------------- ------------ 1.7/2.4 GB 31.4 MB/s eta 0:00:25\n",
      "     --------------------------- ------------ 1.7/2.4 GB 30.1 MB/s eta 0:00:26\n",
      "     --------------------------- ------------ 1.7/2.4 GB 30.0 MB/s eta 0:00:26\n",
      "     --------------------------- ------------ 1.7/2.4 GB 30.0 MB/s eta 0:00:25\n",
      "     --------------------------- ------------ 1.7/2.4 GB 29.9 MB/s eta 0:00:25\n",
      "     ---------------------------- ----------- 1.7/2.4 GB 30.0 MB/s eta 0:00:25\n",
      "     ---------------------------- ----------- 1.7/2.4 GB 30.0 MB/s eta 0:00:25\n",
      "     ---------------------------- ----------- 1.7/2.4 GB 29.9 MB/s eta 0:00:25\n",
      "     ---------------------------- ----------- 1.7/2.4 GB 29.8 MB/s eta 0:00:24\n",
      "     ---------------------------- ----------- 1.7/2.4 GB 29.8 MB/s eta 0:00:24\n",
      "     ---------------------------- ----------- 1.8/2.4 GB 29.9 MB/s eta 0:00:24\n",
      "     ---------------------------- ----------- 1.8/2.4 GB 30.0 MB/s eta 0:00:23\n",
      "     ---------------------------- ----------- 1.8/2.4 GB 30.1 MB/s eta 0:00:23\n",
      "     ---------------------------- ----------- 1.8/2.4 GB 30.9 MB/s eta 0:00:22\n",
      "     ----------------------------- ---------- 1.8/2.4 GB 30.9 MB/s eta 0:00:22\n",
      "     ----------------------------- ---------- 1.8/2.4 GB 30.7 MB/s eta 0:00:22\n",
      "     ----------------------------- ---------- 1.8/2.4 GB 30.6 MB/s eta 0:00:22\n",
      "     ----------------------------- ---------- 1.8/2.4 GB 30.6 MB/s eta 0:00:22\n",
      "     ----------------------------- ---------- 1.8/2.4 GB 30.5 MB/s eta 0:00:22\n",
      "     ----------------------------- ---------- 1.8/2.4 GB 30.5 MB/s eta 0:00:21\n",
      "     ----------------------------- ---------- 1.8/2.4 GB 30.5 MB/s eta 0:00:21\n",
      "     ----------------------------- ---------- 1.8/2.4 GB 29.8 MB/s eta 0:00:22\n",
      "     ----------------------------- ---------- 1.8/2.4 GB 29.6 MB/s eta 0:00:22\n",
      "     ----------------------------- ---------- 1.8/2.4 GB 29.5 MB/s eta 0:00:22\n",
      "     ----------------------------- ---------- 1.8/2.4 GB 29.4 MB/s eta 0:00:21\n",
      "     ------------------------------ --------- 1.8/2.4 GB 29.5 MB/s eta 0:00:21\n",
      "     ------------------------------ --------- 1.8/2.4 GB 29.3 MB/s eta 0:00:21\n",
      "     ------------------------------ --------- 1.9/2.4 GB 29.3 MB/s eta 0:00:21\n",
      "     ------------------------------ --------- 1.9/2.4 GB 29.1 MB/s eta 0:00:21\n",
      "     ------------------------------ --------- 1.9/2.4 GB 29.1 MB/s eta 0:00:20\n",
      "     ------------------------------ --------- 1.9/2.4 GB 29.1 MB/s eta 0:00:20\n",
      "     ------------------------------ --------- 1.9/2.4 GB 29.2 MB/s eta 0:00:20\n",
      "     ------------------------------ --------- 1.9/2.4 GB 29.3 MB/s eta 0:00:20\n",
      "     ------------------------------ --------- 1.9/2.4 GB 29.5 MB/s eta 0:00:19\n",
      "     ------------------------------- -------- 1.9/2.4 GB 29.5 MB/s eta 0:00:19\n",
      "     ------------------------------- -------- 1.9/2.4 GB 29.6 MB/s eta 0:00:19\n",
      "     ------------------------------- -------- 1.9/2.4 GB 29.7 MB/s eta 0:00:18\n",
      "     ------------------------------- -------- 1.9/2.4 GB 29.7 MB/s eta 0:00:18\n",
      "     ------------------------------- -------- 1.9/2.4 GB 29.7 MB/s eta 0:00:18\n",
      "     ------------------------------- -------- 1.9/2.4 GB 29.1 MB/s eta 0:00:18\n",
      "     ------------------------------- -------- 1.9/2.4 GB 29.4 MB/s eta 0:00:18\n",
      "     ------------------------------- -------- 1.9/2.4 GB 31.2 MB/s eta 0:00:17\n",
      "     ------------------------------- -------- 2.0/2.4 GB 30.9 MB/s eta 0:00:17\n",
      "     ------------------------------- -------- 2.0/2.4 GB 31.0 MB/s eta 0:00:16\n",
      "     -------------------------------- ------- 2.0/2.4 GB 31.0 MB/s eta 0:00:16\n",
      "     -------------------------------- ------- 2.0/2.4 GB 31.0 MB/s eta 0:00:16\n",
      "     -------------------------------- ------- 2.0/2.4 GB 31.0 MB/s eta 0:00:16\n",
      "     -------------------------------- ------- 2.0/2.4 GB 31.1 MB/s eta 0:00:15\n",
      "     -------------------------------- ------- 2.0/2.4 GB 31.2 MB/s eta 0:00:15\n",
      "     -------------------------------- ------- 2.0/2.4 GB 31.1 MB/s eta 0:00:15\n",
      "     -------------------------------- ------- 2.0/2.4 GB 31.1 MB/s eta 0:00:15\n",
      "     -------------------------------- ------- 2.0/2.4 GB 31.0 MB/s eta 0:00:15\n",
      "     -------------------------------- ------- 2.0/2.4 GB 30.9 MB/s eta 0:00:14\n",
      "     --------------------------------- ------ 2.0/2.4 GB 30.9 MB/s eta 0:00:14\n",
      "     --------------------------------- ------ 2.0/2.4 GB 30.9 MB/s eta 0:00:14\n",
      "     --------------------------------- ------ 2.0/2.4 GB 30.8 MB/s eta 0:00:14\n",
      "     --------------------------------- ------ 2.0/2.4 GB 30.9 MB/s eta 0:00:14\n",
      "     --------------------------------- ------ 2.1/2.4 GB 30.9 MB/s eta 0:00:13\n",
      "     --------------------------------- ------ 2.1/2.4 GB 30.9 MB/s eta 0:00:13\n",
      "     --------------------------------- ------ 2.1/2.4 GB 31.0 MB/s eta 0:00:13\n",
      "     --------------------------------- ------ 2.1/2.4 GB 31.0 MB/s eta 0:00:13\n",
      "     --------------------------------- ------ 2.1/2.4 GB 31.9 MB/s eta 0:00:12\n",
      "     ---------------------------------- ----- 2.1/2.4 GB 32.3 MB/s eta 0:00:12\n",
      "     ---------------------------------- ----- 2.1/2.4 GB 32.4 MB/s eta 0:00:11\n",
      "     ---------------------------------- ----- 2.1/2.4 GB 32.6 MB/s eta 0:00:11\n",
      "     ---------------------------------- ----- 2.1/2.4 GB 32.7 MB/s eta 0:00:11\n",
      "     ---------------------------------- ----- 2.1/2.4 GB 32.7 MB/s eta 0:00:11\n",
      "     ---------------------------------- ----- 2.1/2.4 GB 32.8 MB/s eta 0:00:10\n",
      "     ---------------------------------- ----- 2.1/2.4 GB 32.8 MB/s eta 0:00:10\n",
      "     ---------------------------------- ----- 2.1/2.4 GB 32.8 MB/s eta 0:00:10\n",
      "     ---------------------------------- ----- 2.1/2.4 GB 31.2 MB/s eta 0:00:11\n",
      "     ---------------------------------- ----- 2.1/2.4 GB 30.7 MB/s eta 0:00:11\n",
      "     ---------------------------------- ----- 2.1/2.4 GB 30.6 MB/s eta 0:00:11\n",
      "     ---------------------------------- ----- 2.1/2.4 GB 30.5 MB/s eta 0:00:11\n",
      "     ----------------------------------- ---- 2.1/2.4 GB 30.4 MB/s eta 0:00:10\n",
      "     ----------------------------------- ---- 2.2/2.4 GB 30.3 MB/s eta 0:00:10\n",
      "     ----------------------------------- ---- 2.2/2.4 GB 30.3 MB/s eta 0:00:10\n",
      "     ----------------------------------- ---- 2.2/2.4 GB 30.2 MB/s eta 0:00:10\n",
      "     ----------------------------------- ---- 2.2/2.4 GB 30.1 MB/s eta 0:00:10\n",
      "     ----------------------------------- ---- 2.2/2.4 GB 30.0 MB/s eta 0:00:09\n",
      "     ----------------------------------- ---- 2.2/2.4 GB 30.8 MB/s eta 0:00:09\n",
      "     ----------------------------------- ---- 2.2/2.4 GB 30.6 MB/s eta 0:00:09\n",
      "     ----------------------------------- ---- 2.2/2.4 GB 30.5 MB/s eta 0:00:09\n",
      "     ------------------------------------ --- 2.2/2.4 GB 30.5 MB/s eta 0:00:08\n",
      "     ------------------------------------ --- 2.2/2.4 GB 30.5 MB/s eta 0:00:08\n",
      "     ------------------------------------ --- 2.2/2.4 GB 30.5 MB/s eta 0:00:08\n",
      "     ------------------------------------ --- 2.2/2.4 GB 30.6 MB/s eta 0:00:08\n",
      "     ------------------------------------ --- 2.2/2.4 GB 30.6 MB/s eta 0:00:07\n",
      "     ------------------------------------ --- 2.2/2.4 GB 30.5 MB/s eta 0:00:07\n",
      "     ------------------------------------ --- 2.2/2.4 GB 30.5 MB/s eta 0:00:07\n",
      "     ------------------------------------ --- 2.3/2.4 GB 30.4 MB/s eta 0:00:07\n",
      "     ------------------------------------ --- 2.3/2.4 GB 30.5 MB/s eta 0:00:07\n",
      "     ------------------------------------- -- 2.3/2.4 GB 30.5 MB/s eta 0:00:06\n",
      "     ------------------------------------- -- 2.3/2.4 GB 30.6 MB/s eta 0:00:06\n",
      "     ------------------------------------- -- 2.3/2.4 GB 30.6 MB/s eta 0:00:06\n",
      "     ------------------------------------- -- 2.3/2.4 GB 30.8 MB/s eta 0:00:06\n",
      "     ------------------------------------- -- 2.3/2.4 GB 30.9 MB/s eta 0:00:05\n",
      "     ------------------------------------- -- 2.3/2.4 GB 30.9 MB/s eta 0:00:05\n",
      "     ------------------------------------- -- 2.3/2.4 GB 31.0 MB/s eta 0:00:05\n",
      "     ------------------------------------- -- 2.3/2.4 GB 30.9 MB/s eta 0:00:05\n",
      "     ------------------------------------- -- 2.3/2.4 GB 31.0 MB/s eta 0:00:04\n",
      "     -------------------------------------- - 2.3/2.4 GB 31.0 MB/s eta 0:00:04\n",
      "     -------------------------------------- - 2.3/2.4 GB 31.0 MB/s eta 0:00:04\n",
      "     -------------------------------------- - 2.3/2.4 GB 31.0 MB/s eta 0:00:04\n",
      "     -------------------------------------- - 2.4/2.4 GB 31.1 MB/s eta 0:00:04\n",
      "     -------------------------------------- - 2.4/2.4 GB 31.1 MB/s eta 0:00:03\n",
      "     -------------------------------------- - 2.4/2.4 GB 30.9 MB/s eta 0:00:03\n",
      "     -------------------------------------- - 2.4/2.4 GB 30.9 MB/s eta 0:00:03\n",
      "     -------------------------------------- - 2.4/2.4 GB 30.8 MB/s eta 0:00:03\n",
      "     ---------------------------------------  2.4/2.4 GB 32.1 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.4 GB 33.0 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.4 GB 33.2 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.4 GB 33.1 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.4 GB 33.1 MB/s eta 0:00:02\n",
      "     ---------------------------------------  2.4/2.4 GB 33.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.4 GB 33.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.4 GB 33.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.4 GB 33.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.4 GB 33.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.4 GB 33.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.4 GB 33.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.4 GB 33.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.4 GB 33.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.4 GB 33.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.4 GB 33.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.4 GB 33.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.4 GB 33.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.4 GB 33.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.4 GB 33.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.4 GB 33.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.4 GB 33.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.4/2.4 GB 25.4 MB/s eta 0:00:00\n",
      "Collecting torchvision==0.20.1\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp312-cp312-win_amd64.whl (6.1 MB)\n",
      "     ---------------------------------------- 0.0/6.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 6.1/6.1 MB 31.0 MB/s eta 0:00:00\n",
      "Collecting torchaudio==2.5.1\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp312-cp312-win_amd64.whl (4.1 MB)\n",
      "     ---------------------------------------- 0.0/4.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 4.1/4.1 MB 31.0 MB/s eta 0:00:00\n",
      "Collecting filelock (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting networkx (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting setuptools (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/setuptools-70.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting sympy==1.13.1 (from torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "     ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "     -------------------------------------- - 6.0/6.2 MB 30.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 6.2/6.2 MB 29.2 MB/s eta 0:00:00\n",
      "Collecting numpy (from torchvision==0.20.1)\n",
      "  Downloading https://download.pytorch.org/whl/numpy-2.1.2-cp312-cp312-win_amd64.whl.metadata (59 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.20.1)\n",
      "  Downloading https://download.pytorch.org/whl/pillow-11.0.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "     ------------------------------------- 536.2/536.2 kB 17.1 MB/s eta 0:00:00\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.5.1)\n",
      "  Downloading https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp312-cp312-win_amd64.whl (17 kB)\n",
      "Downloading https://download.pytorch.org/whl/pillow-11.0.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.6/2.6 MB 16.4 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Downloading https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Downloading https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 22.9 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/numpy-2.1.2-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 5.0/12.6 MB 25.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.7/12.6 MB 26.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 26.2 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/setuptools-70.2.0-py3-none-any.whl (930 kB)\n",
      "   ---------------------------------------- 0.0/930.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 930.8/930.8 kB 41.8 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, setuptools, pillow, numpy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch, torchvision, torchaudio\n",
      "Successfully installed MarkupSafe-2.1.5 filelock-3.13.1 fsspec-2024.6.1 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 numpy-2.1.2 pillow-11.0.0 setuptools-70.2.0 sympy-1.13.1 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121 typing-extensions-4.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\College\\George Brown\\industry project\\Concrete Dataset Sample\\segment-anything-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'segment-anything-2' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///D:/College/George%20Brown/industry%20project/Concrete%20Dataset%20Sample/segment-anything-2\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Getting requirements to build editable: started\n",
      "  Getting requirements to build editable: finished with status 'done'\n",
      "  Preparing editable metadata (pyproject.toml): started\n",
      "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: torch>=2.5.1 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from SAM-2==1.0) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision>=0.20.1 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from SAM-2==1.0) (0.20.1+cu121)\n",
      "Requirement already satisfied: numpy>=1.24.4 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from SAM-2==1.0) (2.1.2)\n",
      "Collecting tqdm>=4.66.1 (from SAM-2==1.0)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting hydra-core>=1.3.2 (from SAM-2==1.0)\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting iopath>=0.1.10 (from SAM-2==1.0)\n",
      "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: pillow>=9.4.0 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from SAM-2==1.0) (11.0.0)\n",
      "Collecting omegaconf<2.4,>=2.2 (from hydra-core>=1.3.2->SAM-2==1.0)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.3.2->SAM-2==1.0)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: packaging in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from hydra-core>=1.3.2->SAM-2==1.0) (24.2)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from iopath>=0.1.10->SAM-2==1.0) (4.12.2)\n",
      "Collecting portalocker (from iopath>=0.1.10->SAM-2==1.0)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from torch>=2.5.1->SAM-2==1.0) (3.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from torch>=2.5.1->SAM-2==1.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from torch>=2.5.1->SAM-2==1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from torch>=2.5.1->SAM-2==1.0) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from torch>=2.5.1->SAM-2==1.0) (70.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from torch>=2.5.1->SAM-2==1.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from sympy==1.13.1->torch>=2.5.1->SAM-2==1.0) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from tqdm>=4.66.1->SAM-2==1.0) (0.4.6)\n",
      "Collecting PyYAML>=5.1.0 (from omegaconf<2.4,>=2.2->hydra-core>=1.3.2->SAM-2==1.0)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from jinja2->torch>=2.5.1->SAM-2==1.0) (2.1.5)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from portalocker->iopath>=0.1.10->SAM-2==1.0) (310)\n",
      "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Building wheels for collected packages: SAM-2, antlr4-python3-runtime, iopath\n",
      "  Building editable for SAM-2 (pyproject.toml): started\n",
      "  Building editable for SAM-2 (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for SAM-2: filename=sam_2-1.0-0.editable-py3-none-any.whl size=13964 sha256=df04063c05a37ea4e4e1c9aa7aae130a8998c4b1386d2e07aa1fdd865186bebf\n",
      "  Stored in directory: C:\\Users\\saeed\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-oxcip200\\wheels\\aa\\2d\\44\\1b4e2a9a64c550aa6fdd9570c9f0ac3d2ae7789bb2bb878f35\n",
      "  Building wheel for antlr4-python3-runtime (pyproject.toml): started\n",
      "  Building wheel for antlr4-python3-runtime (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144616 sha256=a0557664bc2f5ab6cdb07633835b2290acd700b9302af7603b4cf1687e6b4d41\n",
      "  Stored in directory: c:\\users\\saeed\\appdata\\local\\pip\\cache\\wheels\\1f\\be\\48\\13754633f1d08d1fbfc60d5e80ae1e5d7329500477685286cd\n",
      "  Building wheel for iopath (pyproject.toml): started\n",
      "  Building wheel for iopath (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31609 sha256=0cf1921e9083ef343a113a08875ea0d51150326c70d13b7164eb879e832f43bc\n",
      "  Stored in directory: c:\\users\\saeed\\appdata\\local\\pip\\cache\\wheels\\7c\\96\\04\\4f5f31ff812f684f69f40cb1634357812220aac58d4698048c\n",
      "Successfully built SAM-2 antlr4-python3-runtime iopath\n",
      "Installing collected packages: antlr4-python3-runtime, tqdm, PyYAML, portalocker, omegaconf, iopath, hydra-core, SAM-2\n",
      "Successfully installed PyYAML-6.0.2 SAM-2-1.0 antlr4-python3-runtime-4.9.3 hydra-core-1.3.2 iopath-0.1.10 omegaconf-2.3.0 portalocker-3.1.1 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/segment-anything-2.git\n",
    "%cd segment-anything-2\n",
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\saeed\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (21.2.3)\n",
      "Collecting pip\n",
      "  Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.2.3\n",
      "    Uninstalling pip-21.2.3:\n",
      "      Successfully uninstalled pip-21.2.3\n",
      "Successfully installed pip-25.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python==4.10.0.84\n",
      "  Using cached opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting pycocotools==2.0.8\n",
      "  Downloading pycocotools-2.0.8-cp312-cp312-win_amd64.whl.metadata (1.1 kB)\n",
      "Collecting matplotlib==3.9.2\n",
      "  Using cached matplotlib-3.9.2-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting supervision==0.25.1\n",
      "  Using cached supervision-0.25.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting scikit-learn==1.5.2\n",
      "  Downloading scikit_learn-1.5.2-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pillow==11.0.0 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (11.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from opencv-python==4.10.0.84) (2.1.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib==3.9.2)\n",
      "  Downloading contourpy-1.3.1-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib==3.9.2)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib==3.9.2)\n",
      "  Downloading fonttools-4.57.0-cp312-cp312-win_amd64.whl.metadata (104 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib==3.9.2)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from matplotlib==3.9.2) (24.2)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib==3.9.2)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from matplotlib==3.9.2) (2.9.0.post0)\n",
      "Collecting defusedxml<0.8.0,>=0.7.1 (from supervision==0.25.1)\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from supervision==0.25.1) (6.0.2)\n",
      "Collecting requests>=2.26.0 (from supervision==0.25.1)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting scipy<2.0.0,>=1.10.0 (from supervision==0.25.1)\n",
      "  Downloading scipy-1.15.2-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.3 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from supervision==0.25.1) (4.67.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn==1.5.2)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn==1.5.2)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib==3.9.2) (1.17.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.26.0->supervision==0.25.1)\n",
      "  Downloading charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.26.0->supervision==0.25.1)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.26.0->supervision==0.25.1)\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.26.0->supervision==0.25.1)\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from tqdm>=4.62.3->supervision==0.25.1) (0.4.6)\n",
      "Using cached opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "Downloading pycocotools-2.0.8-cp312-cp312-win_amd64.whl (83 kB)\n",
      "Using cached matplotlib-3.9.2-cp312-cp312-win_amd64.whl (7.8 MB)\n",
      "Using cached supervision-0.25.1-py3-none-any.whl (181 kB)\n",
      "Downloading scikit_learn-1.5.2-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 5.8/11.0 MB 32.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 27.4 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.1-cp312-cp312-win_amd64.whl (220 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Downloading fonttools-4.57.0-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 24.5 MB/s eta 0:00:00\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading kiwisolver-1.4.8-cp312-cp312-win_amd64.whl (71 kB)\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading scipy-1.15.2-cp312-cp312-win_amd64.whl (40.9 MB)\n",
      "   ---------------------------------------- 0.0/40.9 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 6.3/40.9 MB 29.7 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 12.6/40.9 MB 30.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 18.6/40.9 MB 30.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 24.4/40.9 MB 30.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 28.0/40.9 MB 27.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 34.3/40.9 MB 28.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 39.3/40.9 MB 27.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 40.9/40.9 MB 26.6 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: urllib3, threadpoolctl, scipy, pyparsing, opencv-python, kiwisolver, joblib, idna, fonttools, defusedxml, cycler, contourpy, charset-normalizer, certifi, scikit-learn, requests, matplotlib, supervision, pycocotools\n",
      "Successfully installed certifi-2025.1.31 charset-normalizer-3.4.1 contourpy-1.3.1 cycler-0.12.1 defusedxml-0.7.1 fonttools-4.57.0 idna-3.10 joblib-1.4.2 kiwisolver-1.4.8 matplotlib-3.9.2 opencv-python-4.10.0.84 pycocotools-2.0.8 pyparsing-3.2.3 requests-2.32.3 scikit-learn-1.5.2 scipy-1.15.2 supervision-0.25.1 threadpoolctl-3.6.0 urllib3-2.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python==4.10.0.84 pycocotools==2.0.8 matplotlib==3.9.2 supervision==0.25.1 scikit-learn==1.5.2 pillow==11.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Using cached wget-3.2.zip (10 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (pyproject.toml): started\n",
      "  Building wheel for wget (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9711 sha256=6488c29feb9521e51b4f6a7392a526b8233f7624feb8bf73584018339950c4f3\n",
      "  Stored in directory: c:\\users\\saeed\\appdata\\local\\pip\\cache\\wheels\\01\\46\\3b\\e29ffbe4ebe614ff224bad40fc6a5773a67a163251585a13a9\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping segment-anything-2 as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///D:/College/George%20Brown/industry%20project/Concrete%20Dataset%20Sample/segment-anything-2\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Getting requirements to build editable: started\n",
      "  Getting requirements to build editable: finished with status 'done'\n",
      "  Preparing editable metadata (pyproject.toml): started\n",
      "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: torch>=2.5.1 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from SAM-2==1.0) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision>=0.20.1 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from SAM-2==1.0) (0.20.1+cu121)\n",
      "Requirement already satisfied: numpy>=1.24.4 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from SAM-2==1.0) (2.1.2)\n",
      "Requirement already satisfied: tqdm>=4.66.1 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from SAM-2==1.0) (4.67.1)\n",
      "Requirement already satisfied: hydra-core>=1.3.2 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from SAM-2==1.0) (1.3.2)\n",
      "Requirement already satisfied: iopath>=0.1.10 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from SAM-2==1.0) (0.1.10)\n",
      "Requirement already satisfied: pillow>=9.4.0 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from SAM-2==1.0) (11.0.0)\n",
      "Requirement already satisfied: omegaconf<2.4,>=2.2 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from hydra-core>=1.3.2->SAM-2==1.0) (2.3.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from hydra-core>=1.3.2->SAM-2==1.0) (4.9.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from hydra-core>=1.3.2->SAM-2==1.0) (24.2)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from iopath>=0.1.10->SAM-2==1.0) (4.12.2)\n",
      "Requirement already satisfied: portalocker in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from iopath>=0.1.10->SAM-2==1.0) (3.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from torch>=2.5.1->SAM-2==1.0) (3.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from torch>=2.5.1->SAM-2==1.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from torch>=2.5.1->SAM-2==1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from torch>=2.5.1->SAM-2==1.0) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from torch>=2.5.1->SAM-2==1.0) (70.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from torch>=2.5.1->SAM-2==1.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from sympy==1.13.1->torch>=2.5.1->SAM-2==1.0) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from tqdm>=4.66.1->SAM-2==1.0) (0.4.6)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from omegaconf<2.4,>=2.2->hydra-core>=1.3.2->SAM-2==1.0) (6.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from jinja2->torch>=2.5.1->SAM-2==1.0) (2.1.5)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from portalocker->iopath>=0.1.10->SAM-2==1.0) (310)\n",
      "Building wheels for collected packages: SAM-2\n",
      "  Building editable for SAM-2 (pyproject.toml): started\n",
      "  Building editable for SAM-2 (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for SAM-2: filename=sam_2-1.0-0.editable-py3-none-any.whl size=13964 sha256=bcc191bde88005e93392c533141587ccc13ebee26bd3f1c94e2ce0eecd564081\n",
      "  Stored in directory: C:\\Users\\saeed\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-n_hife4l\\wheels\\aa\\2d\\44\\1b4e2a9a64c550aa6fdd9570c9f0ac3d2ae7789bb2bb878f35\n",
      "Successfully built SAM-2\n",
      "Installing collected packages: SAM-2\n",
      "  Attempting uninstall: SAM-2\n",
      "    Found existing installation: SAM-2 1.0\n",
      "    Uninstalling SAM-2-1.0:\n",
      "      Successfully uninstalled SAM-2-1.0\n",
      "Successfully installed SAM-2-1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y segment-anything-2\n",
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAM-2                   1.0          D:\\College\\George Brown\\industry project\\Concrete Dataset Sample\\segment-anything-2\n"
     ]
    }
   ],
   "source": [
    "!pip list | findstr SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: D:\\College\\George Brown\\industry project\\Concrete Dataset Sample\\segment-anything-2\n",
      "Found existing installation: SAM-2 1.0\n",
      "Uninstalling SAM-2-1.0:\n",
      "  Successfully uninstalled SAM-2-1.0\n",
      "Obtaining file:///D:/College/George%20Brown/industry%20project/Concrete%20Dataset%20Sample/segment-anything-2\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Getting requirements to build editable: started\n",
      "  Getting requirements to build editable: finished with status 'done'\n",
      "  Preparing editable metadata (pyproject.toml): started\n",
      "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: torch>=2.5.1 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from SAM-2==1.0) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision>=0.20.1 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from SAM-2==1.0) (0.20.1+cu121)\n",
      "Requirement already satisfied: numpy>=1.24.4 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from SAM-2==1.0) (2.1.2)\n",
      "Requirement already satisfied: tqdm>=4.66.1 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from SAM-2==1.0) (4.67.1)\n",
      "Requirement already satisfied: hydra-core>=1.3.2 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from SAM-2==1.0) (1.3.2)\n",
      "Requirement already satisfied: iopath>=0.1.10 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from SAM-2==1.0) (0.1.10)\n",
      "Requirement already satisfied: pillow>=9.4.0 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from SAM-2==1.0) (11.0.0)\n",
      "Requirement already satisfied: omegaconf<2.4,>=2.2 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from hydra-core>=1.3.2->SAM-2==1.0) (2.3.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from hydra-core>=1.3.2->SAM-2==1.0) (4.9.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from hydra-core>=1.3.2->SAM-2==1.0) (24.2)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from iopath>=0.1.10->SAM-2==1.0) (4.12.2)\n",
      "Requirement already satisfied: portalocker in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from iopath>=0.1.10->SAM-2==1.0) (3.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from torch>=2.5.1->SAM-2==1.0) (3.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from torch>=2.5.1->SAM-2==1.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from torch>=2.5.1->SAM-2==1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from torch>=2.5.1->SAM-2==1.0) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from torch>=2.5.1->SAM-2==1.0) (70.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from torch>=2.5.1->SAM-2==1.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from sympy==1.13.1->torch>=2.5.1->SAM-2==1.0) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from tqdm>=4.66.1->SAM-2==1.0) (0.4.6)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from omegaconf<2.4,>=2.2->hydra-core>=1.3.2->SAM-2==1.0) (6.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from jinja2->torch>=2.5.1->SAM-2==1.0) (2.1.5)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\saeed\\sam2_env\\lib\\site-packages (from portalocker->iopath>=0.1.10->SAM-2==1.0) (310)\n",
      "Building wheels for collected packages: SAM-2\n",
      "  Building editable for SAM-2 (pyproject.toml): started\n",
      "  Building editable for SAM-2 (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for SAM-2: filename=sam_2-1.0-0.editable-py3-none-any.whl size=13964 sha256=70e4494166a33d22cd69d9f683f70bdbd77dceba1a1976c1a7c20a442dd1d3dd\n",
      "  Stored in directory: C:\\Users\\saeed\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-ropyqtv6\\wheels\\aa\\2d\\44\\1b4e2a9a64c550aa6fdd9570c9f0ac3d2ae7789bb2bb878f35\n",
      "Successfully built SAM-2\n",
      "Installing collected packages: SAM-2\n",
      "Successfully installed SAM-2-1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "sam2_dir = \"D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2\"\n",
    "os.chdir(sam2_dir)\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "\n",
    "# Uninstall any existing SAM-2 package\n",
    "!pip uninstall -y SAM-2\n",
    "\n",
    "# Install SAM2\n",
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: SAM-2\n",
      "Version: 1.0\n",
      "Summary: SAM 2: Segment Anything in Images and Videos\n",
      "Home-page: https://github.com/facebookresearch/sam2\n",
      "Author: Meta AI\n",
      "Author-email: segment-anything@meta.com\n",
      "License: Apache 2.0\n",
      "Location: C:\\Users\\saeed\\sam2_env\\Lib\\site-packages\n",
      "Editable project location: D:\\College\\George Brown\\industry project\\Concrete Dataset Sample\\segment-anything-2\n",
      "Requires: hydra-core, iopath, numpy, pillow, torch, torchvision, tqdm\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show SAM-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAM2 module imported successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from sam2.build_sam import build_sam2\n",
    "    print(\"SAM2 module imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"ImportError: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML exists: True\n",
      "\n",
      "YAML content (first 10 lines or less):\n",
      "Line 1: configs/sam2/sam2_hiera_b+.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "yaml_path = \"D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2/sam2_hiera_b+.yaml\"\n",
    "print(\"YAML exists:\", os.path.exists(yaml_path))\n",
    "\n",
    "if os.path.exists(yaml_path):\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        content = f.readlines()\n",
    "        print(\"\\nYAML content (first 10 lines or less):\")\n",
    "        for i, line in enumerate(content[:10], 1):\n",
    "            print(f\"Line {i}: {line.strip()}\")\n",
    "else:\n",
    "    print(\"YAML file missing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint keys:\n",
      "  image_encoder.neck.convs.0.conv.bias\n",
      "  image_encoder.neck.convs.0.conv.weight\n",
      "  image_encoder.neck.convs.1.conv.bias\n",
      "  image_encoder.neck.convs.1.conv.weight\n",
      "  image_encoder.neck.convs.2.conv.bias\n",
      "  image_encoder.neck.convs.2.conv.weight\n",
      "  image_encoder.neck.convs.3.conv.bias\n",
      "  image_encoder.neck.convs.3.conv.weight\n",
      "  image_encoder.trunk.blocks.0.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.0.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.0.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.0.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.0.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.0.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.0.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.0.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.0.norm1.bias\n",
      "  image_encoder.trunk.blocks.0.norm1.weight\n",
      "  image_encoder.trunk.blocks.0.norm2.bias\n",
      "  image_encoder.trunk.blocks.0.norm2.weight\n",
      "  image_encoder.trunk.blocks.1.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.1.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.1.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.1.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.1.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.1.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.1.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.1.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.1.norm1.bias\n",
      "  image_encoder.trunk.blocks.1.norm1.weight\n",
      "  image_encoder.trunk.blocks.1.norm2.bias\n",
      "  image_encoder.trunk.blocks.1.norm2.weight\n",
      "  image_encoder.trunk.blocks.10.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.10.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.10.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.10.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.10.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.10.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.10.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.10.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.10.norm1.bias\n",
      "  image_encoder.trunk.blocks.10.norm1.weight\n",
      "  image_encoder.trunk.blocks.10.norm2.bias\n",
      "  image_encoder.trunk.blocks.10.norm2.weight\n",
      "  image_encoder.trunk.blocks.11.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.11.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.11.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.11.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.11.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.11.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.11.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.11.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.11.norm1.bias\n",
      "  image_encoder.trunk.blocks.11.norm1.weight\n",
      "  image_encoder.trunk.blocks.11.norm2.bias\n",
      "  image_encoder.trunk.blocks.11.norm2.weight\n",
      "  image_encoder.trunk.blocks.12.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.12.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.12.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.12.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.12.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.12.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.12.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.12.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.12.norm1.bias\n",
      "  image_encoder.trunk.blocks.12.norm1.weight\n",
      "  image_encoder.trunk.blocks.12.norm2.bias\n",
      "  image_encoder.trunk.blocks.12.norm2.weight\n",
      "  image_encoder.trunk.blocks.13.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.13.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.13.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.13.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.13.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.13.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.13.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.13.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.13.norm1.bias\n",
      "  image_encoder.trunk.blocks.13.norm1.weight\n",
      "  image_encoder.trunk.blocks.13.norm2.bias\n",
      "  image_encoder.trunk.blocks.13.norm2.weight\n",
      "  image_encoder.trunk.blocks.14.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.14.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.14.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.14.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.14.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.14.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.14.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.14.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.14.norm1.bias\n",
      "  image_encoder.trunk.blocks.14.norm1.weight\n",
      "  image_encoder.trunk.blocks.14.norm2.bias\n",
      "  image_encoder.trunk.blocks.14.norm2.weight\n",
      "  image_encoder.trunk.blocks.15.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.15.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.15.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.15.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.15.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.15.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.15.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.15.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.15.norm1.bias\n",
      "  image_encoder.trunk.blocks.15.norm1.weight\n",
      "  image_encoder.trunk.blocks.15.norm2.bias\n",
      "  image_encoder.trunk.blocks.15.norm2.weight\n",
      "  image_encoder.trunk.blocks.16.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.16.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.16.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.16.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.16.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.16.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.16.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.16.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.16.norm1.bias\n",
      "  image_encoder.trunk.blocks.16.norm1.weight\n",
      "  image_encoder.trunk.blocks.16.norm2.bias\n",
      "  image_encoder.trunk.blocks.16.norm2.weight\n",
      "  image_encoder.trunk.blocks.17.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.17.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.17.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.17.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.17.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.17.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.17.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.17.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.17.norm1.bias\n",
      "  image_encoder.trunk.blocks.17.norm1.weight\n",
      "  image_encoder.trunk.blocks.17.norm2.bias\n",
      "  image_encoder.trunk.blocks.17.norm2.weight\n",
      "  image_encoder.trunk.blocks.18.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.18.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.18.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.18.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.18.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.18.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.18.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.18.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.18.norm1.bias\n",
      "  image_encoder.trunk.blocks.18.norm1.weight\n",
      "  image_encoder.trunk.blocks.18.norm2.bias\n",
      "  image_encoder.trunk.blocks.18.norm2.weight\n",
      "  image_encoder.trunk.blocks.19.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.19.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.19.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.19.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.19.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.19.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.19.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.19.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.19.norm1.bias\n",
      "  image_encoder.trunk.blocks.19.norm1.weight\n",
      "  image_encoder.trunk.blocks.19.norm2.bias\n",
      "  image_encoder.trunk.blocks.19.norm2.weight\n",
      "  image_encoder.trunk.blocks.2.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.2.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.2.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.2.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.2.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.2.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.2.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.2.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.2.norm1.bias\n",
      "  image_encoder.trunk.blocks.2.norm1.weight\n",
      "  image_encoder.trunk.blocks.2.norm2.bias\n",
      "  image_encoder.trunk.blocks.2.norm2.weight\n",
      "  image_encoder.trunk.blocks.2.proj.bias\n",
      "  image_encoder.trunk.blocks.2.proj.weight\n",
      "  image_encoder.trunk.blocks.20.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.20.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.20.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.20.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.20.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.20.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.20.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.20.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.20.norm1.bias\n",
      "  image_encoder.trunk.blocks.20.norm1.weight\n",
      "  image_encoder.trunk.blocks.20.norm2.bias\n",
      "  image_encoder.trunk.blocks.20.norm2.weight\n",
      "  image_encoder.trunk.blocks.21.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.21.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.21.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.21.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.21.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.21.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.21.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.21.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.21.norm1.bias\n",
      "  image_encoder.trunk.blocks.21.norm1.weight\n",
      "  image_encoder.trunk.blocks.21.norm2.bias\n",
      "  image_encoder.trunk.blocks.21.norm2.weight\n",
      "  image_encoder.trunk.blocks.21.proj.bias\n",
      "  image_encoder.trunk.blocks.21.proj.weight\n",
      "  image_encoder.trunk.blocks.22.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.22.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.22.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.22.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.22.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.22.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.22.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.22.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.22.norm1.bias\n",
      "  image_encoder.trunk.blocks.22.norm1.weight\n",
      "  image_encoder.trunk.blocks.22.norm2.bias\n",
      "  image_encoder.trunk.blocks.22.norm2.weight\n",
      "  image_encoder.trunk.blocks.23.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.23.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.23.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.23.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.23.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.23.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.23.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.23.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.23.norm1.bias\n",
      "  image_encoder.trunk.blocks.23.norm1.weight\n",
      "  image_encoder.trunk.blocks.23.norm2.bias\n",
      "  image_encoder.trunk.blocks.23.norm2.weight\n",
      "  image_encoder.trunk.blocks.3.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.3.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.3.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.3.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.3.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.3.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.3.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.3.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.3.norm1.bias\n",
      "  image_encoder.trunk.blocks.3.norm1.weight\n",
      "  image_encoder.trunk.blocks.3.norm2.bias\n",
      "  image_encoder.trunk.blocks.3.norm2.weight\n",
      "  image_encoder.trunk.blocks.4.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.4.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.4.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.4.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.4.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.4.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.4.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.4.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.4.norm1.bias\n",
      "  image_encoder.trunk.blocks.4.norm1.weight\n",
      "  image_encoder.trunk.blocks.4.norm2.bias\n",
      "  image_encoder.trunk.blocks.4.norm2.weight\n",
      "  image_encoder.trunk.blocks.5.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.5.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.5.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.5.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.5.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.5.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.5.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.5.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.5.norm1.bias\n",
      "  image_encoder.trunk.blocks.5.norm1.weight\n",
      "  image_encoder.trunk.blocks.5.norm2.bias\n",
      "  image_encoder.trunk.blocks.5.norm2.weight\n",
      "  image_encoder.trunk.blocks.5.proj.bias\n",
      "  image_encoder.trunk.blocks.5.proj.weight\n",
      "  image_encoder.trunk.blocks.6.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.6.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.6.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.6.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.6.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.6.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.6.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.6.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.6.norm1.bias\n",
      "  image_encoder.trunk.blocks.6.norm1.weight\n",
      "  image_encoder.trunk.blocks.6.norm2.bias\n",
      "  image_encoder.trunk.blocks.6.norm2.weight\n",
      "  image_encoder.trunk.blocks.7.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.7.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.7.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.7.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.7.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.7.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.7.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.7.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.7.norm1.bias\n",
      "  image_encoder.trunk.blocks.7.norm1.weight\n",
      "  image_encoder.trunk.blocks.7.norm2.bias\n",
      "  image_encoder.trunk.blocks.7.norm2.weight\n",
      "  image_encoder.trunk.blocks.8.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.8.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.8.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.8.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.8.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.8.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.8.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.8.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.8.norm1.bias\n",
      "  image_encoder.trunk.blocks.8.norm1.weight\n",
      "  image_encoder.trunk.blocks.8.norm2.bias\n",
      "  image_encoder.trunk.blocks.8.norm2.weight\n",
      "  image_encoder.trunk.blocks.9.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.9.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.9.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.9.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.9.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.9.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.9.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.9.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.9.norm1.bias\n",
      "  image_encoder.trunk.blocks.9.norm1.weight\n",
      "  image_encoder.trunk.blocks.9.norm2.bias\n",
      "  image_encoder.trunk.blocks.9.norm2.weight\n",
      "  image_encoder.trunk.patch_embed.proj.bias\n",
      "  image_encoder.trunk.patch_embed.proj.weight\n",
      "  image_encoder.trunk.pos_embed\n",
      "  image_encoder.trunk.pos_embed_window\n",
      "  mask_downsample.bias\n",
      "  mask_downsample.weight\n",
      "  maskmem_tpos_enc\n",
      "  memory_attention.layers.0.cross_attn_image.k_proj.bias\n",
      "  memory_attention.layers.0.cross_attn_image.k_proj.weight\n",
      "  memory_attention.layers.0.cross_attn_image.out_proj.bias\n",
      "  memory_attention.layers.0.cross_attn_image.out_proj.weight\n",
      "  memory_attention.layers.0.cross_attn_image.q_proj.bias\n",
      "  memory_attention.layers.0.cross_attn_image.q_proj.weight\n",
      "  memory_attention.layers.0.cross_attn_image.v_proj.bias\n",
      "  memory_attention.layers.0.cross_attn_image.v_proj.weight\n",
      "  memory_attention.layers.0.linear1.bias\n",
      "  memory_attention.layers.0.linear1.weight\n",
      "  memory_attention.layers.0.linear2.bias\n",
      "  memory_attention.layers.0.linear2.weight\n",
      "  memory_attention.layers.0.norm1.bias\n",
      "  memory_attention.layers.0.norm1.weight\n",
      "  memory_attention.layers.0.norm2.bias\n",
      "  memory_attention.layers.0.norm2.weight\n",
      "  memory_attention.layers.0.norm3.bias\n",
      "  memory_attention.layers.0.norm3.weight\n",
      "  memory_attention.layers.0.self_attn.k_proj.bias\n",
      "  memory_attention.layers.0.self_attn.k_proj.weight\n",
      "  memory_attention.layers.0.self_attn.out_proj.bias\n",
      "  memory_attention.layers.0.self_attn.out_proj.weight\n",
      "  memory_attention.layers.0.self_attn.q_proj.bias\n",
      "  memory_attention.layers.0.self_attn.q_proj.weight\n",
      "  memory_attention.layers.0.self_attn.v_proj.bias\n",
      "  memory_attention.layers.0.self_attn.v_proj.weight\n",
      "  memory_attention.layers.1.cross_attn_image.k_proj.bias\n",
      "  memory_attention.layers.1.cross_attn_image.k_proj.weight\n",
      "  memory_attention.layers.1.cross_attn_image.out_proj.bias\n",
      "  memory_attention.layers.1.cross_attn_image.out_proj.weight\n",
      "  memory_attention.layers.1.cross_attn_image.q_proj.bias\n",
      "  memory_attention.layers.1.cross_attn_image.q_proj.weight\n",
      "  memory_attention.layers.1.cross_attn_image.v_proj.bias\n",
      "  memory_attention.layers.1.cross_attn_image.v_proj.weight\n",
      "  memory_attention.layers.1.linear1.bias\n",
      "  memory_attention.layers.1.linear1.weight\n",
      "  memory_attention.layers.1.linear2.bias\n",
      "  memory_attention.layers.1.linear2.weight\n",
      "  memory_attention.layers.1.norm1.bias\n",
      "  memory_attention.layers.1.norm1.weight\n",
      "  memory_attention.layers.1.norm2.bias\n",
      "  memory_attention.layers.1.norm2.weight\n",
      "  memory_attention.layers.1.norm3.bias\n",
      "  memory_attention.layers.1.norm3.weight\n",
      "  memory_attention.layers.1.self_attn.k_proj.bias\n",
      "  memory_attention.layers.1.self_attn.k_proj.weight\n",
      "  memory_attention.layers.1.self_attn.out_proj.bias\n",
      "  memory_attention.layers.1.self_attn.out_proj.weight\n",
      "  memory_attention.layers.1.self_attn.q_proj.bias\n",
      "  memory_attention.layers.1.self_attn.q_proj.weight\n",
      "  memory_attention.layers.1.self_attn.v_proj.bias\n",
      "  memory_attention.layers.1.self_attn.v_proj.weight\n",
      "  memory_attention.layers.2.cross_attn_image.k_proj.bias\n",
      "  memory_attention.layers.2.cross_attn_image.k_proj.weight\n",
      "  memory_attention.layers.2.cross_attn_image.out_proj.bias\n",
      "  memory_attention.layers.2.cross_attn_image.out_proj.weight\n",
      "  memory_attention.layers.2.cross_attn_image.q_proj.bias\n",
      "  memory_attention.layers.2.cross_attn_image.q_proj.weight\n",
      "  memory_attention.layers.2.cross_attn_image.v_proj.bias\n",
      "  memory_attention.layers.2.cross_attn_image.v_proj.weight\n",
      "  memory_attention.layers.2.linear1.bias\n",
      "  memory_attention.layers.2.linear1.weight\n",
      "  memory_attention.layers.2.linear2.bias\n",
      "  memory_attention.layers.2.linear2.weight\n",
      "  memory_attention.layers.2.norm1.bias\n",
      "  memory_attention.layers.2.norm1.weight\n",
      "  memory_attention.layers.2.norm2.bias\n",
      "  memory_attention.layers.2.norm2.weight\n",
      "  memory_attention.layers.2.norm3.bias\n",
      "  memory_attention.layers.2.norm3.weight\n",
      "  memory_attention.layers.2.self_attn.k_proj.bias\n",
      "  memory_attention.layers.2.self_attn.k_proj.weight\n",
      "  memory_attention.layers.2.self_attn.out_proj.bias\n",
      "  memory_attention.layers.2.self_attn.out_proj.weight\n",
      "  memory_attention.layers.2.self_attn.q_proj.bias\n",
      "  memory_attention.layers.2.self_attn.q_proj.weight\n",
      "  memory_attention.layers.2.self_attn.v_proj.bias\n",
      "  memory_attention.layers.2.self_attn.v_proj.weight\n",
      "  memory_attention.layers.3.cross_attn_image.k_proj.bias\n",
      "  memory_attention.layers.3.cross_attn_image.k_proj.weight\n",
      "  memory_attention.layers.3.cross_attn_image.out_proj.bias\n",
      "  memory_attention.layers.3.cross_attn_image.out_proj.weight\n",
      "  memory_attention.layers.3.cross_attn_image.q_proj.bias\n",
      "  memory_attention.layers.3.cross_attn_image.q_proj.weight\n",
      "  memory_attention.layers.3.cross_attn_image.v_proj.bias\n",
      "  memory_attention.layers.3.cross_attn_image.v_proj.weight\n",
      "  memory_attention.layers.3.linear1.bias\n",
      "  memory_attention.layers.3.linear1.weight\n",
      "  memory_attention.layers.3.linear2.bias\n",
      "  memory_attention.layers.3.linear2.weight\n",
      "  memory_attention.layers.3.norm1.bias\n",
      "  memory_attention.layers.3.norm1.weight\n",
      "  memory_attention.layers.3.norm2.bias\n",
      "  memory_attention.layers.3.norm2.weight\n",
      "  memory_attention.layers.3.norm3.bias\n",
      "  memory_attention.layers.3.norm3.weight\n",
      "  memory_attention.layers.3.self_attn.k_proj.bias\n",
      "  memory_attention.layers.3.self_attn.k_proj.weight\n",
      "  memory_attention.layers.3.self_attn.out_proj.bias\n",
      "  memory_attention.layers.3.self_attn.out_proj.weight\n",
      "  memory_attention.layers.3.self_attn.q_proj.bias\n",
      "  memory_attention.layers.3.self_attn.q_proj.weight\n",
      "  memory_attention.layers.3.self_attn.v_proj.bias\n",
      "  memory_attention.layers.3.self_attn.v_proj.weight\n",
      "  memory_attention.norm.bias\n",
      "  memory_attention.norm.weight\n",
      "  memory_encoder.fuser.layers.0.dwconv.bias\n",
      "  memory_encoder.fuser.layers.0.dwconv.weight\n",
      "  memory_encoder.fuser.layers.0.gamma\n",
      "  memory_encoder.fuser.layers.0.norm.bias\n",
      "  memory_encoder.fuser.layers.0.norm.weight\n",
      "  memory_encoder.fuser.layers.0.pwconv1.bias\n",
      "  memory_encoder.fuser.layers.0.pwconv1.weight\n",
      "  memory_encoder.fuser.layers.0.pwconv2.bias\n",
      "  memory_encoder.fuser.layers.0.pwconv2.weight\n",
      "  memory_encoder.fuser.layers.1.dwconv.bias\n",
      "  memory_encoder.fuser.layers.1.dwconv.weight\n",
      "  memory_encoder.fuser.layers.1.gamma\n",
      "  memory_encoder.fuser.layers.1.norm.bias\n",
      "  memory_encoder.fuser.layers.1.norm.weight\n",
      "  memory_encoder.fuser.layers.1.pwconv1.bias\n",
      "  memory_encoder.fuser.layers.1.pwconv1.weight\n",
      "  memory_encoder.fuser.layers.1.pwconv2.bias\n",
      "  memory_encoder.fuser.layers.1.pwconv2.weight\n",
      "  memory_encoder.mask_downsampler.encoder.0.bias\n",
      "  memory_encoder.mask_downsampler.encoder.0.weight\n",
      "  memory_encoder.mask_downsampler.encoder.1.bias\n",
      "  memory_encoder.mask_downsampler.encoder.1.weight\n",
      "  memory_encoder.mask_downsampler.encoder.10.bias\n",
      "  memory_encoder.mask_downsampler.encoder.10.weight\n",
      "  memory_encoder.mask_downsampler.encoder.12.bias\n",
      "  memory_encoder.mask_downsampler.encoder.12.weight\n",
      "  memory_encoder.mask_downsampler.encoder.3.bias\n",
      "  memory_encoder.mask_downsampler.encoder.3.weight\n",
      "  memory_encoder.mask_downsampler.encoder.4.bias\n",
      "  memory_encoder.mask_downsampler.encoder.4.weight\n",
      "  memory_encoder.mask_downsampler.encoder.6.bias\n",
      "  memory_encoder.mask_downsampler.encoder.6.weight\n",
      "  memory_encoder.mask_downsampler.encoder.7.bias\n",
      "  memory_encoder.mask_downsampler.encoder.7.weight\n",
      "  memory_encoder.mask_downsampler.encoder.9.bias\n",
      "  memory_encoder.mask_downsampler.encoder.9.weight\n",
      "  memory_encoder.out_proj.bias\n",
      "  memory_encoder.out_proj.weight\n",
      "  memory_encoder.pix_feat_proj.bias\n",
      "  memory_encoder.pix_feat_proj.weight\n",
      "  no_mem_embed\n",
      "  no_mem_pos_enc\n",
      "  no_obj_ptr\n",
      "  obj_ptr_proj.layers.0.bias\n",
      "  obj_ptr_proj.layers.0.weight\n",
      "  obj_ptr_proj.layers.1.bias\n",
      "  obj_ptr_proj.layers.1.weight\n",
      "  obj_ptr_proj.layers.2.bias\n",
      "  obj_ptr_proj.layers.2.weight\n",
      "  sam_mask_decoder.conv_s0.bias\n",
      "  sam_mask_decoder.conv_s0.weight\n",
      "  sam_mask_decoder.conv_s1.bias\n",
      "  sam_mask_decoder.conv_s1.weight\n",
      "  sam_mask_decoder.iou_prediction_head.layers.0.bias\n",
      "  sam_mask_decoder.iou_prediction_head.layers.0.weight\n",
      "  sam_mask_decoder.iou_prediction_head.layers.1.bias\n",
      "  sam_mask_decoder.iou_prediction_head.layers.1.weight\n",
      "  sam_mask_decoder.iou_prediction_head.layers.2.bias\n",
      "  sam_mask_decoder.iou_prediction_head.layers.2.weight\n",
      "  sam_mask_decoder.iou_token.weight\n",
      "  sam_mask_decoder.mask_tokens.weight\n",
      "  sam_mask_decoder.obj_score_token.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.0.layers.0.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.0.layers.0.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.0.layers.1.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.0.layers.1.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.0.layers.2.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.0.layers.2.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.1.layers.0.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.1.layers.0.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.1.layers.1.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.1.layers.1.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.1.layers.2.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.1.layers.2.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.2.layers.0.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.2.layers.0.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.2.layers.1.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.2.layers.1.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.2.layers.2.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.2.layers.2.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.3.layers.0.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.3.layers.0.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.3.layers.1.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.3.layers.1.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.3.layers.2.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.3.layers.2.weight\n",
      "  sam_mask_decoder.output_upscaling.0.bias\n",
      "  sam_mask_decoder.output_upscaling.0.weight\n",
      "  sam_mask_decoder.output_upscaling.1.bias\n",
      "  sam_mask_decoder.output_upscaling.1.weight\n",
      "  sam_mask_decoder.output_upscaling.3.bias\n",
      "  sam_mask_decoder.output_upscaling.3.weight\n",
      "  sam_mask_decoder.pred_obj_score_head.layers.0.bias\n",
      "  sam_mask_decoder.pred_obj_score_head.layers.0.weight\n",
      "  sam_mask_decoder.pred_obj_score_head.layers.1.bias\n",
      "  sam_mask_decoder.pred_obj_score_head.layers.1.weight\n",
      "  sam_mask_decoder.pred_obj_score_head.layers.2.bias\n",
      "  sam_mask_decoder.pred_obj_score_head.layers.2.weight\n",
      "  sam_mask_decoder.transformer.final_attn_token_to_image.k_proj.bias\n",
      "  sam_mask_decoder.transformer.final_attn_token_to_image.k_proj.weight\n",
      "  sam_mask_decoder.transformer.final_attn_token_to_image.out_proj.bias\n",
      "  sam_mask_decoder.transformer.final_attn_token_to_image.out_proj.weight\n",
      "  sam_mask_decoder.transformer.final_attn_token_to_image.q_proj.bias\n",
      "  sam_mask_decoder.transformer.final_attn_token_to_image.q_proj.weight\n",
      "  sam_mask_decoder.transformer.final_attn_token_to_image.v_proj.bias\n",
      "  sam_mask_decoder.transformer.final_attn_token_to_image.v_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.mlp.layers.0.bias\n",
      "  sam_mask_decoder.transformer.layers.0.mlp.layers.0.weight\n",
      "  sam_mask_decoder.transformer.layers.0.mlp.layers.1.bias\n",
      "  sam_mask_decoder.transformer.layers.0.mlp.layers.1.weight\n",
      "  sam_mask_decoder.transformer.layers.0.norm1.bias\n",
      "  sam_mask_decoder.transformer.layers.0.norm1.weight\n",
      "  sam_mask_decoder.transformer.layers.0.norm2.bias\n",
      "  sam_mask_decoder.transformer.layers.0.norm2.weight\n",
      "  sam_mask_decoder.transformer.layers.0.norm3.bias\n",
      "  sam_mask_decoder.transformer.layers.0.norm3.weight\n",
      "  sam_mask_decoder.transformer.layers.0.norm4.bias\n",
      "  sam_mask_decoder.transformer.layers.0.norm4.weight\n",
      "  sam_mask_decoder.transformer.layers.0.self_attn.k_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.self_attn.k_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.self_attn.out_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.self_attn.out_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.self_attn.q_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.self_attn.q_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.self_attn.v_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.self_attn.v_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.mlp.layers.0.bias\n",
      "  sam_mask_decoder.transformer.layers.1.mlp.layers.0.weight\n",
      "  sam_mask_decoder.transformer.layers.1.mlp.layers.1.bias\n",
      "  sam_mask_decoder.transformer.layers.1.mlp.layers.1.weight\n",
      "  sam_mask_decoder.transformer.layers.1.norm1.bias\n",
      "  sam_mask_decoder.transformer.layers.1.norm1.weight\n",
      "  sam_mask_decoder.transformer.layers.1.norm2.bias\n",
      "  sam_mask_decoder.transformer.layers.1.norm2.weight\n",
      "  sam_mask_decoder.transformer.layers.1.norm3.bias\n",
      "  sam_mask_decoder.transformer.layers.1.norm3.weight\n",
      "  sam_mask_decoder.transformer.layers.1.norm4.bias\n",
      "  sam_mask_decoder.transformer.layers.1.norm4.weight\n",
      "  sam_mask_decoder.transformer.layers.1.self_attn.k_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.self_attn.k_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.self_attn.out_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.self_attn.out_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.self_attn.q_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.self_attn.q_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.self_attn.v_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.self_attn.v_proj.weight\n",
      "  sam_mask_decoder.transformer.norm_final_attn.bias\n",
      "  sam_mask_decoder.transformer.norm_final_attn.weight\n",
      "  sam_prompt_encoder.mask_downscaling.0.bias\n",
      "  sam_prompt_encoder.mask_downscaling.0.weight\n",
      "  sam_prompt_encoder.mask_downscaling.1.bias\n",
      "  sam_prompt_encoder.mask_downscaling.1.weight\n",
      "  sam_prompt_encoder.mask_downscaling.3.bias\n",
      "  sam_prompt_encoder.mask_downscaling.3.weight\n",
      "  sam_prompt_encoder.mask_downscaling.4.bias\n",
      "  sam_prompt_encoder.mask_downscaling.4.weight\n",
      "  sam_prompt_encoder.mask_downscaling.6.bias\n",
      "  sam_prompt_encoder.mask_downscaling.6.weight\n",
      "  sam_prompt_encoder.no_mask_embed.weight\n",
      "  sam_prompt_encoder.not_a_point_embed.weight\n",
      "  sam_prompt_encoder.pe_layer.positional_encoding_gaussian_matrix\n",
      "  sam_prompt_encoder.point_embeddings.0.weight\n",
      "  sam_prompt_encoder.point_embeddings.1.weight\n",
      "  sam_prompt_encoder.point_embeddings.2.weight\n",
      "  sam_prompt_encoder.point_embeddings.3.weight\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "checkpoint_path = \"D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2/checkpoints/sam2_hiera_base_plus.pt\"\n",
    "try:\n",
    "    sd = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True)[\"model\"]\n",
    "    print(\"Checkpoint keys:\")\n",
    "    for key in sorted(sd.keys()):\n",
    "        print(f\"  {key}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading checkpoint: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint keys (all):\n",
      "  image_encoder.neck.convs.0.conv.bias\n",
      "  image_encoder.neck.convs.0.conv.weight\n",
      "  image_encoder.neck.convs.1.conv.bias\n",
      "  image_encoder.neck.convs.1.conv.weight\n",
      "  image_encoder.neck.convs.2.conv.bias\n",
      "  image_encoder.neck.convs.2.conv.weight\n",
      "  image_encoder.neck.convs.3.conv.bias\n",
      "  image_encoder.neck.convs.3.conv.weight\n",
      "  image_encoder.trunk.blocks.0.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.0.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.0.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.0.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.0.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.0.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.0.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.0.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.0.norm1.bias\n",
      "  image_encoder.trunk.blocks.0.norm1.weight\n",
      "  image_encoder.trunk.blocks.0.norm2.bias\n",
      "  image_encoder.trunk.blocks.0.norm2.weight\n",
      "  image_encoder.trunk.blocks.1.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.1.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.1.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.1.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.1.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.1.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.1.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.1.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.1.norm1.bias\n",
      "  image_encoder.trunk.blocks.1.norm1.weight\n",
      "  image_encoder.trunk.blocks.1.norm2.bias\n",
      "  image_encoder.trunk.blocks.1.norm2.weight\n",
      "  image_encoder.trunk.blocks.10.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.10.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.10.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.10.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.10.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.10.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.10.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.10.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.10.norm1.bias\n",
      "  image_encoder.trunk.blocks.10.norm1.weight\n",
      "  image_encoder.trunk.blocks.10.norm2.bias\n",
      "  image_encoder.trunk.blocks.10.norm2.weight\n",
      "  image_encoder.trunk.blocks.11.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.11.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.11.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.11.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.11.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.11.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.11.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.11.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.11.norm1.bias\n",
      "  image_encoder.trunk.blocks.11.norm1.weight\n",
      "  image_encoder.trunk.blocks.11.norm2.bias\n",
      "  image_encoder.trunk.blocks.11.norm2.weight\n",
      "  image_encoder.trunk.blocks.12.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.12.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.12.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.12.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.12.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.12.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.12.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.12.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.12.norm1.bias\n",
      "  image_encoder.trunk.blocks.12.norm1.weight\n",
      "  image_encoder.trunk.blocks.12.norm2.bias\n",
      "  image_encoder.trunk.blocks.12.norm2.weight\n",
      "  image_encoder.trunk.blocks.13.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.13.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.13.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.13.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.13.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.13.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.13.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.13.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.13.norm1.bias\n",
      "  image_encoder.trunk.blocks.13.norm1.weight\n",
      "  image_encoder.trunk.blocks.13.norm2.bias\n",
      "  image_encoder.trunk.blocks.13.norm2.weight\n",
      "  image_encoder.trunk.blocks.14.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.14.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.14.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.14.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.14.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.14.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.14.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.14.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.14.norm1.bias\n",
      "  image_encoder.trunk.blocks.14.norm1.weight\n",
      "  image_encoder.trunk.blocks.14.norm2.bias\n",
      "  image_encoder.trunk.blocks.14.norm2.weight\n",
      "  image_encoder.trunk.blocks.15.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.15.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.15.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.15.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.15.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.15.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.15.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.15.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.15.norm1.bias\n",
      "  image_encoder.trunk.blocks.15.norm1.weight\n",
      "  image_encoder.trunk.blocks.15.norm2.bias\n",
      "  image_encoder.trunk.blocks.15.norm2.weight\n",
      "  image_encoder.trunk.blocks.16.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.16.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.16.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.16.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.16.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.16.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.16.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.16.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.16.norm1.bias\n",
      "  image_encoder.trunk.blocks.16.norm1.weight\n",
      "  image_encoder.trunk.blocks.16.norm2.bias\n",
      "  image_encoder.trunk.blocks.16.norm2.weight\n",
      "  image_encoder.trunk.blocks.17.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.17.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.17.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.17.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.17.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.17.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.17.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.17.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.17.norm1.bias\n",
      "  image_encoder.trunk.blocks.17.norm1.weight\n",
      "  image_encoder.trunk.blocks.17.norm2.bias\n",
      "  image_encoder.trunk.blocks.17.norm2.weight\n",
      "  image_encoder.trunk.blocks.18.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.18.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.18.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.18.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.18.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.18.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.18.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.18.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.18.norm1.bias\n",
      "  image_encoder.trunk.blocks.18.norm1.weight\n",
      "  image_encoder.trunk.blocks.18.norm2.bias\n",
      "  image_encoder.trunk.blocks.18.norm2.weight\n",
      "  image_encoder.trunk.blocks.19.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.19.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.19.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.19.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.19.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.19.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.19.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.19.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.19.norm1.bias\n",
      "  image_encoder.trunk.blocks.19.norm1.weight\n",
      "  image_encoder.trunk.blocks.19.norm2.bias\n",
      "  image_encoder.trunk.blocks.19.norm2.weight\n",
      "  image_encoder.trunk.blocks.2.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.2.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.2.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.2.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.2.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.2.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.2.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.2.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.2.norm1.bias\n",
      "  image_encoder.trunk.blocks.2.norm1.weight\n",
      "  image_encoder.trunk.blocks.2.norm2.bias\n",
      "  image_encoder.trunk.blocks.2.norm2.weight\n",
      "  image_encoder.trunk.blocks.2.proj.bias\n",
      "  image_encoder.trunk.blocks.2.proj.weight\n",
      "  image_encoder.trunk.blocks.20.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.20.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.20.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.20.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.20.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.20.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.20.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.20.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.20.norm1.bias\n",
      "  image_encoder.trunk.blocks.20.norm1.weight\n",
      "  image_encoder.trunk.blocks.20.norm2.bias\n",
      "  image_encoder.trunk.blocks.20.norm2.weight\n",
      "  image_encoder.trunk.blocks.21.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.21.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.21.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.21.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.21.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.21.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.21.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.21.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.21.norm1.bias\n",
      "  image_encoder.trunk.blocks.21.norm1.weight\n",
      "  image_encoder.trunk.blocks.21.norm2.bias\n",
      "  image_encoder.trunk.blocks.21.norm2.weight\n",
      "  image_encoder.trunk.blocks.21.proj.bias\n",
      "  image_encoder.trunk.blocks.21.proj.weight\n",
      "  image_encoder.trunk.blocks.22.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.22.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.22.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.22.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.22.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.22.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.22.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.22.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.22.norm1.bias\n",
      "  image_encoder.trunk.blocks.22.norm1.weight\n",
      "  image_encoder.trunk.blocks.22.norm2.bias\n",
      "  image_encoder.trunk.blocks.22.norm2.weight\n",
      "  image_encoder.trunk.blocks.23.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.23.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.23.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.23.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.23.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.23.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.23.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.23.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.23.norm1.bias\n",
      "  image_encoder.trunk.blocks.23.norm1.weight\n",
      "  image_encoder.trunk.blocks.23.norm2.bias\n",
      "  image_encoder.trunk.blocks.23.norm2.weight\n",
      "  image_encoder.trunk.blocks.3.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.3.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.3.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.3.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.3.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.3.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.3.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.3.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.3.norm1.bias\n",
      "  image_encoder.trunk.blocks.3.norm1.weight\n",
      "  image_encoder.trunk.blocks.3.norm2.bias\n",
      "  image_encoder.trunk.blocks.3.norm2.weight\n",
      "  image_encoder.trunk.blocks.4.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.4.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.4.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.4.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.4.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.4.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.4.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.4.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.4.norm1.bias\n",
      "  image_encoder.trunk.blocks.4.norm1.weight\n",
      "  image_encoder.trunk.blocks.4.norm2.bias\n",
      "  image_encoder.trunk.blocks.4.norm2.weight\n",
      "  image_encoder.trunk.blocks.5.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.5.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.5.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.5.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.5.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.5.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.5.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.5.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.5.norm1.bias\n",
      "  image_encoder.trunk.blocks.5.norm1.weight\n",
      "  image_encoder.trunk.blocks.5.norm2.bias\n",
      "  image_encoder.trunk.blocks.5.norm2.weight\n",
      "  image_encoder.trunk.blocks.5.proj.bias\n",
      "  image_encoder.trunk.blocks.5.proj.weight\n",
      "  image_encoder.trunk.blocks.6.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.6.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.6.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.6.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.6.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.6.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.6.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.6.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.6.norm1.bias\n",
      "  image_encoder.trunk.blocks.6.norm1.weight\n",
      "  image_encoder.trunk.blocks.6.norm2.bias\n",
      "  image_encoder.trunk.blocks.6.norm2.weight\n",
      "  image_encoder.trunk.blocks.7.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.7.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.7.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.7.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.7.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.7.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.7.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.7.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.7.norm1.bias\n",
      "  image_encoder.trunk.blocks.7.norm1.weight\n",
      "  image_encoder.trunk.blocks.7.norm2.bias\n",
      "  image_encoder.trunk.blocks.7.norm2.weight\n",
      "  image_encoder.trunk.blocks.8.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.8.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.8.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.8.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.8.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.8.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.8.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.8.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.8.norm1.bias\n",
      "  image_encoder.trunk.blocks.8.norm1.weight\n",
      "  image_encoder.trunk.blocks.8.norm2.bias\n",
      "  image_encoder.trunk.blocks.8.norm2.weight\n",
      "  image_encoder.trunk.blocks.9.attn.proj.bias\n",
      "  image_encoder.trunk.blocks.9.attn.proj.weight\n",
      "  image_encoder.trunk.blocks.9.attn.qkv.bias\n",
      "  image_encoder.trunk.blocks.9.attn.qkv.weight\n",
      "  image_encoder.trunk.blocks.9.mlp.layers.0.bias\n",
      "  image_encoder.trunk.blocks.9.mlp.layers.0.weight\n",
      "  image_encoder.trunk.blocks.9.mlp.layers.1.bias\n",
      "  image_encoder.trunk.blocks.9.mlp.layers.1.weight\n",
      "  image_encoder.trunk.blocks.9.norm1.bias\n",
      "  image_encoder.trunk.blocks.9.norm1.weight\n",
      "  image_encoder.trunk.blocks.9.norm2.bias\n",
      "  image_encoder.trunk.blocks.9.norm2.weight\n",
      "  image_encoder.trunk.patch_embed.proj.bias\n",
      "  image_encoder.trunk.patch_embed.proj.weight\n",
      "  image_encoder.trunk.pos_embed\n",
      "  image_encoder.trunk.pos_embed_window\n",
      "  mask_downsample.bias\n",
      "  mask_downsample.weight\n",
      "  maskmem_tpos_enc\n",
      "  memory_attention.layers.0.cross_attn_image.k_proj.bias\n",
      "  memory_attention.layers.0.cross_attn_image.k_proj.weight\n",
      "  memory_attention.layers.0.cross_attn_image.out_proj.bias\n",
      "  memory_attention.layers.0.cross_attn_image.out_proj.weight\n",
      "  memory_attention.layers.0.cross_attn_image.q_proj.bias\n",
      "  memory_attention.layers.0.cross_attn_image.q_proj.weight\n",
      "  memory_attention.layers.0.cross_attn_image.v_proj.bias\n",
      "  memory_attention.layers.0.cross_attn_image.v_proj.weight\n",
      "  memory_attention.layers.0.linear1.bias\n",
      "  memory_attention.layers.0.linear1.weight\n",
      "  memory_attention.layers.0.linear2.bias\n",
      "  memory_attention.layers.0.linear2.weight\n",
      "  memory_attention.layers.0.norm1.bias\n",
      "  memory_attention.layers.0.norm1.weight\n",
      "  memory_attention.layers.0.norm2.bias\n",
      "  memory_attention.layers.0.norm2.weight\n",
      "  memory_attention.layers.0.norm3.bias\n",
      "  memory_attention.layers.0.norm3.weight\n",
      "  memory_attention.layers.0.self_attn.k_proj.bias\n",
      "  memory_attention.layers.0.self_attn.k_proj.weight\n",
      "  memory_attention.layers.0.self_attn.out_proj.bias\n",
      "  memory_attention.layers.0.self_attn.out_proj.weight\n",
      "  memory_attention.layers.0.self_attn.q_proj.bias\n",
      "  memory_attention.layers.0.self_attn.q_proj.weight\n",
      "  memory_attention.layers.0.self_attn.v_proj.bias\n",
      "  memory_attention.layers.0.self_attn.v_proj.weight\n",
      "  memory_attention.layers.1.cross_attn_image.k_proj.bias\n",
      "  memory_attention.layers.1.cross_attn_image.k_proj.weight\n",
      "  memory_attention.layers.1.cross_attn_image.out_proj.bias\n",
      "  memory_attention.layers.1.cross_attn_image.out_proj.weight\n",
      "  memory_attention.layers.1.cross_attn_image.q_proj.bias\n",
      "  memory_attention.layers.1.cross_attn_image.q_proj.weight\n",
      "  memory_attention.layers.1.cross_attn_image.v_proj.bias\n",
      "  memory_attention.layers.1.cross_attn_image.v_proj.weight\n",
      "  memory_attention.layers.1.linear1.bias\n",
      "  memory_attention.layers.1.linear1.weight\n",
      "  memory_attention.layers.1.linear2.bias\n",
      "  memory_attention.layers.1.linear2.weight\n",
      "  memory_attention.layers.1.norm1.bias\n",
      "  memory_attention.layers.1.norm1.weight\n",
      "  memory_attention.layers.1.norm2.bias\n",
      "  memory_attention.layers.1.norm2.weight\n",
      "  memory_attention.layers.1.norm3.bias\n",
      "  memory_attention.layers.1.norm3.weight\n",
      "  memory_attention.layers.1.self_attn.k_proj.bias\n",
      "  memory_attention.layers.1.self_attn.k_proj.weight\n",
      "  memory_attention.layers.1.self_attn.out_proj.bias\n",
      "  memory_attention.layers.1.self_attn.out_proj.weight\n",
      "  memory_attention.layers.1.self_attn.q_proj.bias\n",
      "  memory_attention.layers.1.self_attn.q_proj.weight\n",
      "  memory_attention.layers.1.self_attn.v_proj.bias\n",
      "  memory_attention.layers.1.self_attn.v_proj.weight\n",
      "  memory_attention.layers.2.cross_attn_image.k_proj.bias\n",
      "  memory_attention.layers.2.cross_attn_image.k_proj.weight\n",
      "  memory_attention.layers.2.cross_attn_image.out_proj.bias\n",
      "  memory_attention.layers.2.cross_attn_image.out_proj.weight\n",
      "  memory_attention.layers.2.cross_attn_image.q_proj.bias\n",
      "  memory_attention.layers.2.cross_attn_image.q_proj.weight\n",
      "  memory_attention.layers.2.cross_attn_image.v_proj.bias\n",
      "  memory_attention.layers.2.cross_attn_image.v_proj.weight\n",
      "  memory_attention.layers.2.linear1.bias\n",
      "  memory_attention.layers.2.linear1.weight\n",
      "  memory_attention.layers.2.linear2.bias\n",
      "  memory_attention.layers.2.linear2.weight\n",
      "  memory_attention.layers.2.norm1.bias\n",
      "  memory_attention.layers.2.norm1.weight\n",
      "  memory_attention.layers.2.norm2.bias\n",
      "  memory_attention.layers.2.norm2.weight\n",
      "  memory_attention.layers.2.norm3.bias\n",
      "  memory_attention.layers.2.norm3.weight\n",
      "  memory_attention.layers.2.self_attn.k_proj.bias\n",
      "  memory_attention.layers.2.self_attn.k_proj.weight\n",
      "  memory_attention.layers.2.self_attn.out_proj.bias\n",
      "  memory_attention.layers.2.self_attn.out_proj.weight\n",
      "  memory_attention.layers.2.self_attn.q_proj.bias\n",
      "  memory_attention.layers.2.self_attn.q_proj.weight\n",
      "  memory_attention.layers.2.self_attn.v_proj.bias\n",
      "  memory_attention.layers.2.self_attn.v_proj.weight\n",
      "  memory_attention.layers.3.cross_attn_image.k_proj.bias\n",
      "  memory_attention.layers.3.cross_attn_image.k_proj.weight\n",
      "  memory_attention.layers.3.cross_attn_image.out_proj.bias\n",
      "  memory_attention.layers.3.cross_attn_image.out_proj.weight\n",
      "  memory_attention.layers.3.cross_attn_image.q_proj.bias\n",
      "  memory_attention.layers.3.cross_attn_image.q_proj.weight\n",
      "  memory_attention.layers.3.cross_attn_image.v_proj.bias\n",
      "  memory_attention.layers.3.cross_attn_image.v_proj.weight\n",
      "  memory_attention.layers.3.linear1.bias\n",
      "  memory_attention.layers.3.linear1.weight\n",
      "  memory_attention.layers.3.linear2.bias\n",
      "  memory_attention.layers.3.linear2.weight\n",
      "  memory_attention.layers.3.norm1.bias\n",
      "  memory_attention.layers.3.norm1.weight\n",
      "  memory_attention.layers.3.norm2.bias\n",
      "  memory_attention.layers.3.norm2.weight\n",
      "  memory_attention.layers.3.norm3.bias\n",
      "  memory_attention.layers.3.norm3.weight\n",
      "  memory_attention.layers.3.self_attn.k_proj.bias\n",
      "  memory_attention.layers.3.self_attn.k_proj.weight\n",
      "  memory_attention.layers.3.self_attn.out_proj.bias\n",
      "  memory_attention.layers.3.self_attn.out_proj.weight\n",
      "  memory_attention.layers.3.self_attn.q_proj.bias\n",
      "  memory_attention.layers.3.self_attn.q_proj.weight\n",
      "  memory_attention.layers.3.self_attn.v_proj.bias\n",
      "  memory_attention.layers.3.self_attn.v_proj.weight\n",
      "  memory_attention.norm.bias\n",
      "  memory_attention.norm.weight\n",
      "  memory_encoder.fuser.layers.0.dwconv.bias\n",
      "  memory_encoder.fuser.layers.0.dwconv.weight\n",
      "  memory_encoder.fuser.layers.0.gamma\n",
      "  memory_encoder.fuser.layers.0.norm.bias\n",
      "  memory_encoder.fuser.layers.0.norm.weight\n",
      "  memory_encoder.fuser.layers.0.pwconv1.bias\n",
      "  memory_encoder.fuser.layers.0.pwconv1.weight\n",
      "  memory_encoder.fuser.layers.0.pwconv2.bias\n",
      "  memory_encoder.fuser.layers.0.pwconv2.weight\n",
      "  memory_encoder.fuser.layers.1.dwconv.bias\n",
      "  memory_encoder.fuser.layers.1.dwconv.weight\n",
      "  memory_encoder.fuser.layers.1.gamma\n",
      "  memory_encoder.fuser.layers.1.norm.bias\n",
      "  memory_encoder.fuser.layers.1.norm.weight\n",
      "  memory_encoder.fuser.layers.1.pwconv1.bias\n",
      "  memory_encoder.fuser.layers.1.pwconv1.weight\n",
      "  memory_encoder.fuser.layers.1.pwconv2.bias\n",
      "  memory_encoder.fuser.layers.1.pwconv2.weight\n",
      "  memory_encoder.mask_downsampler.encoder.0.bias\n",
      "  memory_encoder.mask_downsampler.encoder.0.weight\n",
      "  memory_encoder.mask_downsampler.encoder.1.bias\n",
      "  memory_encoder.mask_downsampler.encoder.1.weight\n",
      "  memory_encoder.mask_downsampler.encoder.10.bias\n",
      "  memory_encoder.mask_downsampler.encoder.10.weight\n",
      "  memory_encoder.mask_downsampler.encoder.12.bias\n",
      "  memory_encoder.mask_downsampler.encoder.12.weight\n",
      "  memory_encoder.mask_downsampler.encoder.3.bias\n",
      "  memory_encoder.mask_downsampler.encoder.3.weight\n",
      "  memory_encoder.mask_downsampler.encoder.4.bias\n",
      "  memory_encoder.mask_downsampler.encoder.4.weight\n",
      "  memory_encoder.mask_downsampler.encoder.6.bias\n",
      "  memory_encoder.mask_downsampler.encoder.6.weight\n",
      "  memory_encoder.mask_downsampler.encoder.7.bias\n",
      "  memory_encoder.mask_downsampler.encoder.7.weight\n",
      "  memory_encoder.mask_downsampler.encoder.9.bias\n",
      "  memory_encoder.mask_downsampler.encoder.9.weight\n",
      "  memory_encoder.out_proj.bias\n",
      "  memory_encoder.out_proj.weight\n",
      "  memory_encoder.pix_feat_proj.bias\n",
      "  memory_encoder.pix_feat_proj.weight\n",
      "  no_mem_embed\n",
      "  no_mem_pos_enc\n",
      "  no_obj_ptr\n",
      "  obj_ptr_proj.layers.0.bias\n",
      "  obj_ptr_proj.layers.0.weight\n",
      "  obj_ptr_proj.layers.1.bias\n",
      "  obj_ptr_proj.layers.1.weight\n",
      "  obj_ptr_proj.layers.2.bias\n",
      "  obj_ptr_proj.layers.2.weight\n",
      "  sam_mask_decoder.conv_s0.bias\n",
      "  sam_mask_decoder.conv_s0.weight\n",
      "  sam_mask_decoder.conv_s1.bias\n",
      "  sam_mask_decoder.conv_s1.weight\n",
      "  sam_mask_decoder.iou_prediction_head.layers.0.bias\n",
      "  sam_mask_decoder.iou_prediction_head.layers.0.weight\n",
      "  sam_mask_decoder.iou_prediction_head.layers.1.bias\n",
      "  sam_mask_decoder.iou_prediction_head.layers.1.weight\n",
      "  sam_mask_decoder.iou_prediction_head.layers.2.bias\n",
      "  sam_mask_decoder.iou_prediction_head.layers.2.weight\n",
      "  sam_mask_decoder.iou_token.weight\n",
      "  sam_mask_decoder.mask_tokens.weight\n",
      "  sam_mask_decoder.obj_score_token.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.0.layers.0.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.0.layers.0.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.0.layers.1.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.0.layers.1.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.0.layers.2.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.0.layers.2.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.1.layers.0.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.1.layers.0.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.1.layers.1.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.1.layers.1.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.1.layers.2.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.1.layers.2.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.2.layers.0.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.2.layers.0.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.2.layers.1.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.2.layers.1.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.2.layers.2.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.2.layers.2.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.3.layers.0.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.3.layers.0.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.3.layers.1.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.3.layers.1.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.3.layers.2.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.3.layers.2.weight\n",
      "  sam_mask_decoder.output_upscaling.0.bias\n",
      "  sam_mask_decoder.output_upscaling.0.weight\n",
      "  sam_mask_decoder.output_upscaling.1.bias\n",
      "  sam_mask_decoder.output_upscaling.1.weight\n",
      "  sam_mask_decoder.output_upscaling.3.bias\n",
      "  sam_mask_decoder.output_upscaling.3.weight\n",
      "  sam_mask_decoder.pred_obj_score_head.layers.0.bias\n",
      "  sam_mask_decoder.pred_obj_score_head.layers.0.weight\n",
      "  sam_mask_decoder.pred_obj_score_head.layers.1.bias\n",
      "  sam_mask_decoder.pred_obj_score_head.layers.1.weight\n",
      "  sam_mask_decoder.pred_obj_score_head.layers.2.bias\n",
      "  sam_mask_decoder.pred_obj_score_head.layers.2.weight\n",
      "  sam_mask_decoder.transformer.final_attn_token_to_image.k_proj.bias\n",
      "  sam_mask_decoder.transformer.final_attn_token_to_image.k_proj.weight\n",
      "  sam_mask_decoder.transformer.final_attn_token_to_image.out_proj.bias\n",
      "  sam_mask_decoder.transformer.final_attn_token_to_image.out_proj.weight\n",
      "  sam_mask_decoder.transformer.final_attn_token_to_image.q_proj.bias\n",
      "  sam_mask_decoder.transformer.final_attn_token_to_image.q_proj.weight\n",
      "  sam_mask_decoder.transformer.final_attn_token_to_image.v_proj.bias\n",
      "  sam_mask_decoder.transformer.final_attn_token_to_image.v_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.mlp.layers.0.bias\n",
      "  sam_mask_decoder.transformer.layers.0.mlp.layers.0.weight\n",
      "  sam_mask_decoder.transformer.layers.0.mlp.layers.1.bias\n",
      "  sam_mask_decoder.transformer.layers.0.mlp.layers.1.weight\n",
      "  sam_mask_decoder.transformer.layers.0.norm1.bias\n",
      "  sam_mask_decoder.transformer.layers.0.norm1.weight\n",
      "  sam_mask_decoder.transformer.layers.0.norm2.bias\n",
      "  sam_mask_decoder.transformer.layers.0.norm2.weight\n",
      "  sam_mask_decoder.transformer.layers.0.norm3.bias\n",
      "  sam_mask_decoder.transformer.layers.0.norm3.weight\n",
      "  sam_mask_decoder.transformer.layers.0.norm4.bias\n",
      "  sam_mask_decoder.transformer.layers.0.norm4.weight\n",
      "  sam_mask_decoder.transformer.layers.0.self_attn.k_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.self_attn.k_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.self_attn.out_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.self_attn.out_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.self_attn.q_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.self_attn.q_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.self_attn.v_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.self_attn.v_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.mlp.layers.0.bias\n",
      "  sam_mask_decoder.transformer.layers.1.mlp.layers.0.weight\n",
      "  sam_mask_decoder.transformer.layers.1.mlp.layers.1.bias\n",
      "  sam_mask_decoder.transformer.layers.1.mlp.layers.1.weight\n",
      "  sam_mask_decoder.transformer.layers.1.norm1.bias\n",
      "  sam_mask_decoder.transformer.layers.1.norm1.weight\n",
      "  sam_mask_decoder.transformer.layers.1.norm2.bias\n",
      "  sam_mask_decoder.transformer.layers.1.norm2.weight\n",
      "  sam_mask_decoder.transformer.layers.1.norm3.bias\n",
      "  sam_mask_decoder.transformer.layers.1.norm3.weight\n",
      "  sam_mask_decoder.transformer.layers.1.norm4.bias\n",
      "  sam_mask_decoder.transformer.layers.1.norm4.weight\n",
      "  sam_mask_decoder.transformer.layers.1.self_attn.k_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.self_attn.k_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.self_attn.out_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.self_attn.out_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.self_attn.q_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.self_attn.q_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.self_attn.v_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.self_attn.v_proj.weight\n",
      "  sam_mask_decoder.transformer.norm_final_attn.bias\n",
      "  sam_mask_decoder.transformer.norm_final_attn.weight\n",
      "  sam_prompt_encoder.mask_downscaling.0.bias\n",
      "  sam_prompt_encoder.mask_downscaling.0.weight\n",
      "  sam_prompt_encoder.mask_downscaling.1.bias\n",
      "  sam_prompt_encoder.mask_downscaling.1.weight\n",
      "  sam_prompt_encoder.mask_downscaling.3.bias\n",
      "  sam_prompt_encoder.mask_downscaling.3.weight\n",
      "  sam_prompt_encoder.mask_downscaling.4.bias\n",
      "  sam_prompt_encoder.mask_downscaling.4.weight\n",
      "  sam_prompt_encoder.mask_downscaling.6.bias\n",
      "  sam_prompt_encoder.mask_downscaling.6.weight\n",
      "  sam_prompt_encoder.no_mask_embed.weight\n",
      "  sam_prompt_encoder.not_a_point_embed.weight\n",
      "  sam_prompt_encoder.pe_layer.positional_encoding_gaussian_matrix\n",
      "  sam_prompt_encoder.point_embeddings.0.weight\n",
      "  sam_prompt_encoder.point_embeddings.1.weight\n",
      "  sam_prompt_encoder.point_embeddings.2.weight\n",
      "  sam_prompt_encoder.point_embeddings.3.weight\n",
      "\n",
      "Keys containing 'mask_decoder', 'prompt_encoder', or 'obj_ptr':\n",
      "  no_obj_ptr\n",
      "  obj_ptr_proj.layers.0.bias\n",
      "  obj_ptr_proj.layers.0.weight\n",
      "  obj_ptr_proj.layers.1.bias\n",
      "  obj_ptr_proj.layers.1.weight\n",
      "  obj_ptr_proj.layers.2.bias\n",
      "  obj_ptr_proj.layers.2.weight\n",
      "  sam_mask_decoder.conv_s0.bias\n",
      "  sam_mask_decoder.conv_s0.weight\n",
      "  sam_mask_decoder.conv_s1.bias\n",
      "  sam_mask_decoder.conv_s1.weight\n",
      "  sam_mask_decoder.iou_prediction_head.layers.0.bias\n",
      "  sam_mask_decoder.iou_prediction_head.layers.0.weight\n",
      "  sam_mask_decoder.iou_prediction_head.layers.1.bias\n",
      "  sam_mask_decoder.iou_prediction_head.layers.1.weight\n",
      "  sam_mask_decoder.iou_prediction_head.layers.2.bias\n",
      "  sam_mask_decoder.iou_prediction_head.layers.2.weight\n",
      "  sam_mask_decoder.iou_token.weight\n",
      "  sam_mask_decoder.mask_tokens.weight\n",
      "  sam_mask_decoder.obj_score_token.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.0.layers.0.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.0.layers.0.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.0.layers.1.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.0.layers.1.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.0.layers.2.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.0.layers.2.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.1.layers.0.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.1.layers.0.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.1.layers.1.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.1.layers.1.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.1.layers.2.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.1.layers.2.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.2.layers.0.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.2.layers.0.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.2.layers.1.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.2.layers.1.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.2.layers.2.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.2.layers.2.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.3.layers.0.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.3.layers.0.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.3.layers.1.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.3.layers.1.weight\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.3.layers.2.bias\n",
      "  sam_mask_decoder.output_hypernetworks_mlps.3.layers.2.weight\n",
      "  sam_mask_decoder.output_upscaling.0.bias\n",
      "  sam_mask_decoder.output_upscaling.0.weight\n",
      "  sam_mask_decoder.output_upscaling.1.bias\n",
      "  sam_mask_decoder.output_upscaling.1.weight\n",
      "  sam_mask_decoder.output_upscaling.3.bias\n",
      "  sam_mask_decoder.output_upscaling.3.weight\n",
      "  sam_mask_decoder.pred_obj_score_head.layers.0.bias\n",
      "  sam_mask_decoder.pred_obj_score_head.layers.0.weight\n",
      "  sam_mask_decoder.pred_obj_score_head.layers.1.bias\n",
      "  sam_mask_decoder.pred_obj_score_head.layers.1.weight\n",
      "  sam_mask_decoder.pred_obj_score_head.layers.2.bias\n",
      "  sam_mask_decoder.pred_obj_score_head.layers.2.weight\n",
      "  sam_mask_decoder.transformer.final_attn_token_to_image.k_proj.bias\n",
      "  sam_mask_decoder.transformer.final_attn_token_to_image.k_proj.weight\n",
      "  sam_mask_decoder.transformer.final_attn_token_to_image.out_proj.bias\n",
      "  sam_mask_decoder.transformer.final_attn_token_to_image.out_proj.weight\n",
      "  sam_mask_decoder.transformer.final_attn_token_to_image.q_proj.bias\n",
      "  sam_mask_decoder.transformer.final_attn_token_to_image.q_proj.weight\n",
      "  sam_mask_decoder.transformer.final_attn_token_to_image.v_proj.bias\n",
      "  sam_mask_decoder.transformer.final_attn_token_to_image.v_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.mlp.layers.0.bias\n",
      "  sam_mask_decoder.transformer.layers.0.mlp.layers.0.weight\n",
      "  sam_mask_decoder.transformer.layers.0.mlp.layers.1.bias\n",
      "  sam_mask_decoder.transformer.layers.0.mlp.layers.1.weight\n",
      "  sam_mask_decoder.transformer.layers.0.norm1.bias\n",
      "  sam_mask_decoder.transformer.layers.0.norm1.weight\n",
      "  sam_mask_decoder.transformer.layers.0.norm2.bias\n",
      "  sam_mask_decoder.transformer.layers.0.norm2.weight\n",
      "  sam_mask_decoder.transformer.layers.0.norm3.bias\n",
      "  sam_mask_decoder.transformer.layers.0.norm3.weight\n",
      "  sam_mask_decoder.transformer.layers.0.norm4.bias\n",
      "  sam_mask_decoder.transformer.layers.0.norm4.weight\n",
      "  sam_mask_decoder.transformer.layers.0.self_attn.k_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.self_attn.k_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.self_attn.out_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.self_attn.out_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.self_attn.q_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.self_attn.q_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.0.self_attn.v_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.0.self_attn.v_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.mlp.layers.0.bias\n",
      "  sam_mask_decoder.transformer.layers.1.mlp.layers.0.weight\n",
      "  sam_mask_decoder.transformer.layers.1.mlp.layers.1.bias\n",
      "  sam_mask_decoder.transformer.layers.1.mlp.layers.1.weight\n",
      "  sam_mask_decoder.transformer.layers.1.norm1.bias\n",
      "  sam_mask_decoder.transformer.layers.1.norm1.weight\n",
      "  sam_mask_decoder.transformer.layers.1.norm2.bias\n",
      "  sam_mask_decoder.transformer.layers.1.norm2.weight\n",
      "  sam_mask_decoder.transformer.layers.1.norm3.bias\n",
      "  sam_mask_decoder.transformer.layers.1.norm3.weight\n",
      "  sam_mask_decoder.transformer.layers.1.norm4.bias\n",
      "  sam_mask_decoder.transformer.layers.1.norm4.weight\n",
      "  sam_mask_decoder.transformer.layers.1.self_attn.k_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.self_attn.k_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.self_attn.out_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.self_attn.out_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.self_attn.q_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.self_attn.q_proj.weight\n",
      "  sam_mask_decoder.transformer.layers.1.self_attn.v_proj.bias\n",
      "  sam_mask_decoder.transformer.layers.1.self_attn.v_proj.weight\n",
      "  sam_mask_decoder.transformer.norm_final_attn.bias\n",
      "  sam_mask_decoder.transformer.norm_final_attn.weight\n",
      "  sam_prompt_encoder.mask_downscaling.0.bias\n",
      "  sam_prompt_encoder.mask_downscaling.0.weight\n",
      "  sam_prompt_encoder.mask_downscaling.1.bias\n",
      "  sam_prompt_encoder.mask_downscaling.1.weight\n",
      "  sam_prompt_encoder.mask_downscaling.3.bias\n",
      "  sam_prompt_encoder.mask_downscaling.3.weight\n",
      "  sam_prompt_encoder.mask_downscaling.4.bias\n",
      "  sam_prompt_encoder.mask_downscaling.4.weight\n",
      "  sam_prompt_encoder.mask_downscaling.6.bias\n",
      "  sam_prompt_encoder.mask_downscaling.6.weight\n",
      "  sam_prompt_encoder.no_mask_embed.weight\n",
      "  sam_prompt_encoder.not_a_point_embed.weight\n",
      "  sam_prompt_encoder.pe_layer.positional_encoding_gaussian_matrix\n",
      "  sam_prompt_encoder.point_embeddings.0.weight\n",
      "  sam_prompt_encoder.point_embeddings.1.weight\n",
      "  sam_prompt_encoder.point_embeddings.2.weight\n",
      "  sam_prompt_encoder.point_embeddings.3.weight\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "checkpoint_path = \"D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2/checkpoints/sam2_hiera_base_plus.pt\"\n",
    "try:\n",
    "    sd = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True)[\"model\"]\n",
    "    print(\"Checkpoint keys (all):\")\n",
    "    keys = sorted(sd.keys())\n",
    "    for key in keys:\n",
    "        print(f\"  {key}\")\n",
    "    # Highlight specific keys\n",
    "    print(\"\\nKeys containing 'mask_decoder', 'prompt_encoder', or 'obj_ptr':\")\n",
    "    for key in keys:\n",
    "        if any(term in key for term in ['mask_decoder', 'prompt_encoder', 'obj_ptr', 'no_obj']):\n",
    "            print(f\"  {key}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading checkpoint: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in sam2.modeling:\n",
      "  backbones\n",
      "  memory_attention.py\n",
      "  memory_encoder.py\n",
      "  position_encoding.py\n",
      "  sam\n",
      "  sam2_base.py\n",
      "  sam2_utils.py\n",
      "  __init__.py\n",
      "  __pycache__\n",
      "No PromptEncoder in sam2.modeling.sam2_utils\n",
      "Error importing sam2.modeling.prompt_encoder: No module named 'sam2.modeling.prompt_encoder'\n",
      "Error importing sam2.modeling.sam_prompt_encoder: No module named 'sam2.modeling.sam_prompt_encoder'\n",
      "No PromptEncoder in sam2.modeling\n",
      "\n",
      "Contents of sam2.modeling.sam2_utils:\n",
      "['DropPath', 'F', 'LayerNorm2d', 'MLP', 'Tuple', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'copy', 'get_1d_sine_pe', 'get_activation_fn', 'get_clones', 'get_next_point', 'mask_to_box', 'nn', 'np', 'sample_box_points', 'sample_one_point_from_error_center', 'sample_random_points_from_errors', 'select_closest_cond_frames', 'torch']\n"
     ]
    }
   ],
   "source": [
    "import sam2\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to SAM2 repository\n",
    "sam2_path = \"D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2/\"\n",
    "\n",
    "# List files in sam2.modeling\n",
    "modeling_path = os.path.join(sam2_path, \"sam2\", \"modeling\")\n",
    "print(\"Files in sam2.modeling:\")\n",
    "for f in os.listdir(modeling_path):\n",
    "    print(f\"  {f}\")\n",
    "\n",
    "# Try importing possible PromptEncoder locations\n",
    "possible_modules = [\n",
    "    \"sam2.modeling.sam2_utils\",\n",
    "    \"sam2.modeling.prompt_encoder\",\n",
    "    \"sam2.modeling.sam_prompt_encoder\",\n",
    "    \"sam2.modeling\",\n",
    "]\n",
    "\n",
    "for module in possible_modules:\n",
    "    try:\n",
    "        mod = __import__(module, fromlist=[\"PromptEncoder\"])\n",
    "        if hasattr(mod, \"PromptEncoder\"):\n",
    "            print(f\"Found PromptEncoder in {module}\")\n",
    "        else:\n",
    "            print(f\"No PromptEncoder in {module}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"Error importing {module}: {e}\")\n",
    "\n",
    "# Check sam2_utils specifically\n",
    "try:\n",
    "    from sam2.modeling import sam2_utils\n",
    "    print(\"\\nContents of sam2.modeling.sam2_utils:\")\n",
    "    print(dir(sam2_utils))\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing sam2.modeling.sam2_utils: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for PromptEncoder class...\n",
      "Possible encoder class in D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2/sam2\\modeling\\memory_encoder.py, line 138: class MemoryEncoder(nn.Module):\n",
      "Possible encoder class in D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2/sam2\\modeling\\backbones\\image_encoder.py, line 14: class ImageEncoder(nn.Module):\n",
      "Found PromptEncoder in D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2/sam2\\modeling\\sam\\prompt_encoder.py, line 17: class PromptEncoder(nn.Module):\n",
      "Possible encoder class in D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2/sam2\\modeling\\sam\\prompt_encoder.py, line 17: class PromptEncoder(nn.Module):\n",
      "Search complete.\n",
      "\n",
      "Files in sam2.modeling.sam:\n",
      "  mask_decoder.py\n",
      "  prompt_encoder.py\n",
      "  transformer.py\n",
      "  __init__.py\n",
      "  __pycache__\n",
      "\n",
      "Checking sam2_base.py for PromptEncoder...\n",
      "PromptEncoder reference in sam2_base.py, line 14: from sam2.modeling.sam.prompt_encoder import PromptEncoder\n",
      "PromptEncoder reference in sam2_base.py, line 212: # build PromptEncoder and MaskDecoder from SAM\n",
      "PromptEncoder reference in sam2_base.py, line 214: self.sam_prompt_encoder = PromptEncoder(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to SAM2 repository\n",
    "sam2_path = \"D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2/\"\n",
    "\n",
    "# Function to search for PromptEncoder in Python files\n",
    "def search_prompt_encoder(root_dir):\n",
    "    print(\"Searching for PromptEncoder class...\")\n",
    "    for root, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".py\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        lines = f.readlines()\n",
    "                        for i, line in enumerate(lines):\n",
    "                            if 'class PromptEncoder' in line:\n",
    "                                print(f\"Found PromptEncoder in {file_path}, line {i+1}: {line.strip()}\")\n",
    "                            # Also check for similar classes\n",
    "                            if 'class ' in line and 'Encoder' in line:\n",
    "                                print(f\"Possible encoder class in {file_path}, line {i+1}: {line.strip()}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file_path}: {e}\")\n",
    "    print(\"Search complete.\")\n",
    "\n",
    "# Run search\n",
    "search_prompt_encoder(sam2_path)\n",
    "\n",
    "# Check sam2.modeling.sam contents\n",
    "sam_path = os.path.join(sam2_path, \"sam2\", \"modeling\", \"sam\")\n",
    "print(\"\\nFiles in sam2.modeling.sam:\")\n",
    "if os.path.exists(sam_path):\n",
    "    for f in os.listdir(sam_path):\n",
    "        print(f\"  {f}\")\n",
    "else:\n",
    "    print(\"  sam directory not found\")\n",
    "\n",
    "# Check sam2_base.py specifically\n",
    "sam2_base_path = os.path.join(sam2_path, \"sam2\", \"modeling\", \"sam2_base.py\")\n",
    "if os.path.exists(sam2_base_path):\n",
    "    print(\"\\nChecking sam2_base.py for PromptEncoder...\")\n",
    "    with open(sam2_base_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            if 'PromptEncoder' in line:\n",
    "                print(f\"PromptEncoder reference in sam2_base.py, line {i+1}: {line.strip()}\")\n",
    "            if 'class ' in line and 'Encoder' in line:\n",
    "                print(f\"Encoder class in sam2_base.py, line {i+1}: {line.strip()}\")\n",
    "else:\n",
    "    print(\"sam2_base.py not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 112: class MLP(nn.Module):\n",
      "class MLP(nn.Module):\n",
      "    def __init__(\n",
      "        self,\n",
      "        input_dim: int,\n",
      "        hidden_dim: int,\n",
      "        output_dim: int,\n",
      "        num_layers: int,\n",
      "        activation: nn.Module = nn.ReLU,\n",
      "        sigmoid_output: bool = False,\n",
      "    ) -> None:\n",
      "        super().__init__()\n",
      "        self.num_layers = num_layers\n",
      "        h = [hidden_dim] * (num_layers - 1)\n",
      "        self.layers = nn.ModuleList(\n",
      "            nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim])\n",
      "        )\n",
      "        self.sigmoid_output = sigmoid_output\n",
      "        self.act = activation()\n",
      "\n",
      "    def forward(self, x):\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2/sam2/modeling/sam2_utils.py\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'class MLP' in line:\n",
    "            print(f\"Line {i+1}: {line.strip()}\")\n",
    "            print(\"\".join(lines[i:i+20]))  # Print next 20 lines for context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 15: class MaskDecoder(nn.Module):\n",
      "class MaskDecoder(nn.Module):\n",
      "    def __init__(\n",
      "        self,\n",
      "        *,\n",
      "        transformer_dim: int,\n",
      "        transformer: nn.Module,\n",
      "        num_multimask_outputs: int = 3,\n",
      "        activation: Type[nn.Module] = nn.GELU,\n",
      "        iou_head_depth: int = 3,\n",
      "        iou_head_hidden_dim: int = 256,\n",
      "        use_high_res_features: bool = False,\n",
      "        iou_prediction_use_sigmoid=False,\n",
      "        dynamic_multimask_via_stability=False,\n",
      "        dynamic_multimask_stability_delta=0.05,\n",
      "        dynamic_multimask_stability_thresh=0.98,\n",
      "        pred_obj_scores: bool = False,\n",
      "        pred_obj_scores_mlp: bool = False,\n",
      "        use_multimask_token_for_obj_ptr: bool = False,\n",
      "    ) -> None:\n",
      "        \"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2/sam2/modeling/sam/mask_decoder.py\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'class MaskDecoder' in line:\n",
    "            print(f\"Line {i+1}: {line.strip()}\")\n",
    "            print(\"\".join(lines[i:i+20]))  # Print next 20 lines for context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 22: class SAM2Base(torch.nn.Module):\n",
      "class SAM2Base(torch.nn.Module):\n",
      "    def __init__(\n",
      "        self,\n",
      "        image_encoder,\n",
      "        memory_attention,\n",
      "        memory_encoder,\n",
      "        num_maskmem=7,  # default 1 input frame + 6 previous frames\n",
      "        image_size=512,\n",
      "        backbone_stride=16,  # stride of the image backbone output\n",
      "        sigmoid_scale_for_mem_enc=1.0,  # scale factor for mask sigmoid prob\n",
      "        sigmoid_bias_for_mem_enc=0.0,  # bias factor for mask sigmoid prob\n",
      "        # During evaluation, whether to binarize the sigmoid mask logits on interacted frames with clicks\n",
      "        binarize_mask_from_pts_for_mem_enc=False,\n",
      "        use_mask_input_as_output_without_sam=False,  # on frames with mask input, whether to directly output the input mask without using a SAM prompt encoder + mask decoder\n",
      "        # The maximum number of conditioning frames to participate in the memory attention (-1 means no limit; if there are more conditioning frames than this limit,\n",
      "        # we only cross-attend to the temporally closest `max_cond_frames_in_attn` conditioning frames in the encoder when tracking each frame). This gives the model\n",
      "        # a temporal locality when handling a large number of annotated frames (since closer frames should be more important) and also avoids GPU OOM.\n",
      "        max_cond_frames_in_attn=-1,\n",
      "        # on the first frame, whether to directly add the no-memory embedding to the image feature\n",
      "        # (instead of using the transformer encoder)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2/sam2/modeling/sam2_base.py\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'class SAM2Base' in line:\n",
    "            print(f\"Line {i+1}: {line.strip()}\")\n",
    "            print(\"\".join(lines[i:i+20]))  # Print next 20 lines for context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 23: def __init__(\n",
      "Line 23: def __init__(\n",
      "Line 24: self,\n",
      "Line 25: image_encoder,\n",
      "Line 26: memory_attention,\n",
      "Line 27: memory_encoder,\n",
      "Line 28: num_maskmem=7,  # default 1 input frame + 6 previous frames\n",
      "Line 29: image_size=512,\n",
      "Line 30: backbone_stride=16,  # stride of the image backbone output\n",
      "Line 31: sigmoid_scale_for_mem_enc=1.0,  # scale factor for mask sigmoid prob\n",
      "Line 32: sigmoid_bias_for_mem_enc=0.0,  # bias factor for mask sigmoid prob\n",
      "Line 33: # During evaluation, whether to binarize the sigmoid mask logits on interacted frames with clicks\n",
      "Line 34: binarize_mask_from_pts_for_mem_enc=False,\n",
      "Line 35: use_mask_input_as_output_without_sam=False,  # on frames with mask input, whether to directly output the input mask without using a SAM prompt encoder + mask decoder\n",
      "Line 36: # The maximum number of conditioning frames to participate in the memory attention (-1 means no limit; if there are more conditioning frames than this limit,\n",
      "Line 37: # we only cross-attend to the temporally closest `max_cond_frames_in_attn` conditioning frames in the encoder when tracking each frame). This gives the model\n",
      "Line 38: # a temporal locality when handling a large number of annotated frames (since closer frames should be more important) and also avoids GPU OOM.\n",
      "Line 39: max_cond_frames_in_attn=-1,\n",
      "Line 40: # on the first frame, whether to directly add the no-memory embedding to the image feature\n",
      "Line 41: # (instead of using the transformer encoder)\n",
      "Line 42: directly_add_no_mem_embed=False,\n",
      "Line 43: # whether to use high-resolution feature maps in the SAM mask decoder\n",
      "Line 44: use_high_res_features_in_sam=False,\n",
      "Line 45: # whether to output multiple (3) masks for the first click on initial conditioning frames\n",
      "Line 46: multimask_output_in_sam=False,\n",
      "Line 47: # the minimum and maximum number of clicks to use multimask_output_in_sam (only relevant when `multimask_output_in_sam=True`;\n",
      "Line 48: # default is 1 for both, meaning that only the first click gives multimask output; also note that a box counts as two points)\n",
      "Line 49: multimask_min_pt_num=1,\n",
      "Line 50: multimask_max_pt_num=1,\n",
      "Line 51: # whether to also use multimask output for tracking (not just for the first click on initial conditioning frames; only relevant when `multimask_output_in_sam=True`)\n",
      "Line 52: multimask_output_for_tracking=False,\n",
      "Line 53: # Whether to use multimask tokens for obj ptr; Only relevant when both\n",
      "Line 54: # use_obj_ptrs_in_encoder=True and multimask_output_for_tracking=True\n",
      "Line 55: use_multimask_token_for_obj_ptr: bool = False,\n",
      "Line 56: # whether to use sigmoid to restrict ious prediction to [0-1]\n",
      "Line 57: iou_prediction_use_sigmoid=False,\n",
      "Line 58: # The memory bank's temporal stride during evaluation (i.e. the `r` parameter in XMem and Cutie; XMem and Cutie use r=5).\n",
      "Line 59: # For r>1, the (self.num_maskmem - 1) non-conditioning memory frames consist of\n",
      "Line 60: # (self.num_maskmem - 2) nearest frames from every r-th frames, plus the last frame.\n",
      "Line 61: memory_temporal_stride_for_eval=1,\n",
      "Line 62: # whether to apply non-overlapping constraints on the object masks in the memory encoder during evaluation (to avoid/alleviate superposing masks)\n",
      "Line 63: non_overlap_masks_for_mem_enc=False,\n",
      "Line 64: # whether to cross-attend to object pointers from other frames (based on SAM output tokens) in the encoder\n",
      "Line 65: use_obj_ptrs_in_encoder=False,\n",
      "Line 66: # the maximum number of object pointers from other frames in encoder cross attention (only relevant when `use_obj_ptrs_in_encoder=True`)\n",
      "Line 67: max_obj_ptrs_in_encoder=16,\n",
      "Line 68: # whether to add temporal positional encoding to the object pointers in the encoder (only relevant when `use_obj_ptrs_in_encoder=True`)\n",
      "Line 69: add_tpos_enc_to_obj_ptrs=True,\n",
      "Line 70: # whether to add an extra linear projection layer for the temporal positional encoding in the object pointers to avoid potential interference\n",
      "Line 71: # with spatial positional encoding (only relevant when both `use_obj_ptrs_in_encoder=True` and `add_tpos_enc_to_obj_ptrs=True`)\n",
      "Line 72: proj_tpos_enc_in_obj_ptrs=False,\n",
      "Line 73: # whether to use signed distance (instead of unsigned absolute distance) in the temporal positional encoding in the object pointers\n",
      "Line 74: # (only relevant when both `use_obj_ptrs_in_encoder=True` and `add_tpos_enc_to_obj_ptrs=True`)\n",
      "Line 75: use_signed_tpos_enc_to_obj_ptrs=False,\n",
      "Line 76: # whether to only attend to object pointers in the past (before the current frame) in the encoder during evaluation\n",
      "Line 77: # (only relevant when `use_obj_ptrs_in_encoder=True`; this might avoid pointer information too far in the future to distract the initial tracking)\n",
      "Line 78: only_obj_ptrs_in_the_past_for_eval=False,\n",
      "Line 79: # Whether to predict if there is an object in the frame\n",
      "Line 80: pred_obj_scores: bool = False,\n",
      "Line 81: # Whether to use an MLP to predict object scores\n",
      "Line 82: pred_obj_scores_mlp: bool = False,\n",
      "Line 83: # Only relevant if pred_obj_scores=True and use_obj_ptrs_in_encoder=True;\n",
      "Line 84: # Whether to have a fixed no obj pointer when there is no object present\n",
      "Line 85: # or to use it as an additive embedding with obj_ptr produced by decoder\n",
      "Line 86: fixed_no_obj_ptr: bool = False,\n",
      "Line 87: # Soft no object, i.e. mix in no_obj_ptr softly,\n",
      "Line 88: # hope to make recovery easier if there is a mistake and mitigate accumulation of errors\n",
      "Line 89: soft_no_obj_ptr: bool = False,\n",
      "Line 90: use_mlp_for_obj_ptr_proj: bool = False,\n",
      "Line 91: # add no obj embedding to spatial frames\n",
      "Line 92: no_obj_embed_spatial: bool = False,\n",
      "Line 93: # extra arguments used to construct the SAM mask decoder; if not None, it should be a dict of kwargs to be passed into `MaskDecoder` class.\n",
      "Line 94: sam_mask_decoder_extra_args=None,\n",
      "Line 95: compile_image_encoder: bool = False,\n",
      "Line 96: ):\n",
      "\n",
      "Searching for prompt_encoder and mask_decoder initialization:\n",
      "Line 13: from sam2.modeling.sam.mask_decoder import MaskDecoder\n",
      "Line 14: from sam2.modeling.sam.prompt_encoder import PromptEncoder\n",
      "Line 54: # use_obj_ptrs_in_encoder=True and multimask_output_for_tracking=True\n",
      "Line 55: use_multimask_token_for_obj_ptr: bool = False,\n",
      "Line 65: use_obj_ptrs_in_encoder=False,\n",
      "Line 66: # the maximum number of object pointers from other frames in encoder cross attention (only relevant when `use_obj_ptrs_in_encoder=True`)\n",
      "Line 67: max_obj_ptrs_in_encoder=16,\n",
      "Line 68: # whether to add temporal positional encoding to the object pointers in the encoder (only relevant when `use_obj_ptrs_in_encoder=True`)\n",
      "Line 69: add_tpos_enc_to_obj_ptrs=True,\n",
      "Line 71: # with spatial positional encoding (only relevant when both `use_obj_ptrs_in_encoder=True` and `add_tpos_enc_to_obj_ptrs=True`)\n",
      "Line 72: proj_tpos_enc_in_obj_ptrs=False,\n",
      "Line 74: # (only relevant when both `use_obj_ptrs_in_encoder=True` and `add_tpos_enc_to_obj_ptrs=True`)\n",
      "Line 75: use_signed_tpos_enc_to_obj_ptrs=False,\n",
      "Line 77: # (only relevant when `use_obj_ptrs_in_encoder=True`; this might avoid pointer information too far in the future to distract the initial tracking)\n",
      "Line 78: only_obj_ptrs_in_the_past_for_eval=False,\n",
      "Line 83: # Only relevant if pred_obj_scores=True and use_obj_ptrs_in_encoder=True;\n",
      "Line 85: # or to use it as an additive embedding with obj_ptr produced by decoder\n",
      "Line 86: fixed_no_obj_ptr: bool = False,\n",
      "Line 87: # Soft no object, i.e. mix in no_obj_ptr softly,\n",
      "Line 89: soft_no_obj_ptr: bool = False,\n",
      "Line 90: use_mlp_for_obj_ptr_proj: bool = False,\n",
      "Line 94: sam_mask_decoder_extra_args=None,\n",
      "Line 104: self.use_obj_ptrs_in_encoder = use_obj_ptrs_in_encoder\n",
      "Line 105: self.max_obj_ptrs_in_encoder = max_obj_ptrs_in_encoder\n",
      "Line 106: if use_obj_ptrs_in_encoder:\n",
      "Line 111: self.add_tpos_enc_to_obj_ptrs = add_tpos_enc_to_obj_ptrs\n",
      "Line 112: if proj_tpos_enc_in_obj_ptrs:\n",
      "Line 113: assert add_tpos_enc_to_obj_ptrs  # these options need to be used together\n",
      "Line 114: self.proj_tpos_enc_in_obj_ptrs = proj_tpos_enc_in_obj_ptrs\n",
      "Line 115: self.use_signed_tpos_enc_to_obj_ptrs = use_signed_tpos_enc_to_obj_ptrs\n",
      "Line 116: self.only_obj_ptrs_in_the_past_for_eval = only_obj_ptrs_in_the_past_for_eval\n",
      "Line 157: self.use_multimask_token_for_obj_ptr = use_multimask_token_for_obj_ptr\n",
      "Line 164: self.sam_mask_decoder_extra_args = sam_mask_decoder_extra_args\n",
      "Line 167: self.fixed_no_obj_ptr = fixed_no_obj_ptr\n",
      "Line 168: self.soft_no_obj_ptr = soft_no_obj_ptr\n",
      "Line 169: if self.fixed_no_obj_ptr:\n",
      "Line 171: assert self.use_obj_ptrs_in_encoder\n",
      "Line 172: if self.pred_obj_scores and self.use_obj_ptrs_in_encoder:\n",
      "Line 173: self.no_obj_ptr = torch.nn.Parameter(torch.zeros(1, self.hidden_dim))\n",
      "Line 174: trunc_normal_(self.no_obj_ptr, std=0.02)\n",
      "Line 175: self.use_mlp_for_obj_ptr_proj = use_mlp_for_obj_ptr_proj\n",
      "Line 214: self.sam_prompt_encoder = PromptEncoder(\n",
      "Line 223: self.sam_mask_decoder = MaskDecoder(\n",
      "Line 238: use_multimask_token_for_obj_ptr=self.use_multimask_token_for_obj_ptr,\n",
      "Line 239: **(self.sam_mask_decoder_extra_args or {}),\n",
      "Line 241: if self.use_obj_ptrs_in_encoder:\n",
      "Line 243: self.obj_ptr_proj = torch.nn.Linear(self.hidden_dim, self.hidden_dim)\n",
      "Line 244: if self.use_mlp_for_obj_ptr_proj:\n",
      "Line 245: self.obj_ptr_proj = MLP(\n",
      "Line 249: self.obj_ptr_proj = torch.nn.Identity()\n",
      "Line 250: if self.proj_tpos_enc_in_obj_ptrs:\n",
      "Line 253: self.obj_ptr_tpos_proj = torch.nn.Linear(self.hidden_dim, self.mem_dim)\n",
      "Line 255: self.obj_ptr_tpos_proj = torch.nn.Identity()\n",
      "Line 301: - obj_ptr: [B, C] shape, the object pointer vector for the output mask, extracted\n",
      "Line 325: if mask_inputs.shape[-2:] != self.sam_prompt_encoder.mask_input_size:\n",
      "Line 328: size=self.sam_prompt_encoder.mask_input_size,\n",
      "Line 340: sparse_embeddings, dense_embeddings = self.sam_prompt_encoder(\n",
      "Line 350: ) = self.sam_mask_decoder(\n",
      "Line 352: image_pe=self.sam_prompt_encoder.get_dense_pe(),\n",
      "Line 393: obj_ptr = self.obj_ptr_proj(sam_output_token)\n",
      "Line 396: if self.soft_no_obj_ptr:\n",
      "Line 401: if self.fixed_no_obj_ptr:\n",
      "Line 402: obj_ptr = lambda_is_obj_appearing * obj_ptr\n",
      "Line 403: obj_ptr = obj_ptr + (1 - lambda_is_obj_appearing) * self.no_obj_ptr\n",
      "Line 411: obj_ptr,\n",
      "Line 433: if not self.use_obj_ptrs_in_encoder:\n",
      "Line 435: obj_ptr = torch.zeros(\n",
      "Line 440: _, _, _, _, _, obj_ptr, _ = self._forward_sam_heads(\n",
      "Line 453: if self.fixed_no_obj_ptr:\n",
      "Line 454: obj_ptr = lambda_is_obj_appearing * obj_ptr\n",
      "Line 455: obj_ptr = obj_ptr + (1 - lambda_is_obj_appearing) * self.no_obj_ptr\n",
      "Line 463: obj_ptr,\n",
      "Line 473: backbone_out[\"backbone_fpn\"][0] = self.sam_mask_decoder.conv_s0(\n",
      "Line 476: backbone_out[\"backbone_fpn\"][1] = self.sam_mask_decoder.conv_s1(\n",
      "Line 519: num_obj_ptr_tokens = 0\n",
      "Line 587: if self.use_obj_ptrs_in_encoder:\n",
      "Line 588: max_obj_ptrs_in_encoder = min(num_frames, self.max_obj_ptrs_in_encoder)\n",
      "Line 591: if not self.training and self.only_obj_ptrs_in_the_past_for_eval:\n",
      "Line 604: if self.use_signed_tpos_enc_to_obj_ptrs\n",
      "Line 607: out[\"obj_ptr\"],\n",
      "Line 611: # Add up to (max_obj_ptrs_in_encoder - 1) non-conditioning frames before current frame\n",
      "Line 612: for t_diff in range(1, max_obj_ptrs_in_encoder):\n",
      "Line 620: pos_and_ptrs.append((t_diff, out[\"obj_ptr\"]))\n",
      "Line 625: obj_ptrs = torch.stack(ptrs_list, dim=0)\n",
      "Line 628: if self.add_tpos_enc_to_obj_ptrs:\n",
      "Line 629: t_diff_max = max_obj_ptrs_in_encoder - 1\n",
      "Line 630: tpos_dim = C if self.proj_tpos_enc_in_obj_ptrs else self.mem_dim\n",
      "Line 635: obj_pos = self.obj_ptr_tpos_proj(obj_pos)\n",
      "Line 638: obj_pos = obj_ptrs.new_zeros(len(pos_list), B, self.mem_dim)\n",
      "Line 641: obj_ptrs = obj_ptrs.reshape(\n",
      "Line 644: obj_ptrs = obj_ptrs.permute(0, 2, 1, 3).flatten(0, 1)\n",
      "Line 646: to_cat_memory.append(obj_ptrs)\n",
      "Line 648: num_obj_ptr_tokens = obj_ptrs.shape[0]\n",
      "Line 650: num_obj_ptr_tokens = 0\n",
      "Line 672: num_obj_ptr_tokens=num_obj_ptr_tokens,\n",
      "Line 855: obj_ptr,\n",
      "Line 861: current_out[\"obj_ptr\"] = obj_ptr\n"
     ]
    }
   ],
   "source": [
    "with open(\"D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2/sam2/modeling/sam2_base.py\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    found_init = False\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'def __init__(' in line and 'SAM2Base' in lines[i-1]:\n",
    "            found_init = True\n",
    "            print(f\"Line {i+1}: {line.strip()}\")\n",
    "        if found_init:\n",
    "            print(f\"Line {i+1}: {line.strip()}\")\n",
    "            if '):' in line:\n",
    "                break\n",
    "    print(\"\\nSearching for prompt_encoder and mask_decoder initialization:\")\n",
    "    for i, line in enumerate(lines):\n",
    "        if any(x in line for x in ['prompt_encoder', 'mask_decoder', 'obj_ptr']):\n",
    "            print(f\"Line {i+1}: {line.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for mask_decoder initialization:\n",
      "Line 80: pred_obj_scores: bool = False,\n",
      "Line 82: pred_obj_scores_mlp: bool = False,\n",
      "Line 83: # Only relevant if pred_obj_scores=True and use_obj_ptrs_in_encoder=True;\n",
      "Line 165: self.pred_obj_scores = pred_obj_scores\n",
      "Line 166: self.pred_obj_scores_mlp = pred_obj_scores_mlp\n",
      "Line 170: assert self.pred_obj_scores\n",
      "Line 172: if self.pred_obj_scores and self.use_obj_ptrs_in_encoder:\n",
      "Line 223: self.sam_mask_decoder = MaskDecoder(\n",
      "Line 236: pred_obj_scores=self.pred_obj_scores,\n",
      "Line 237: pred_obj_scores_mlp=self.pred_obj_scores_mlp,\n",
      "Line 359: if self.pred_obj_scores:\n",
      "Line 394: if self.pred_obj_scores:\n",
      "Line 452: if self.pred_obj_scores:\n"
     ]
    }
   ],
   "source": [
    "with open(\"D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2/sam2/modeling/sam2_base.py\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    print(\"Searching for mask_decoder initialization:\")\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'mask_decoder =' in line or 'MaskDecoder(' in line:\n",
    "            print(f\"Line {i+1}: {line.strip()}\")\n",
    "        if 'pred_obj_scores' in line:\n",
    "            print(f\"Line {i+1}: {line.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for MaskDecoder in: D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2/sam2/modeling\n",
      "Found MaskDecoder in D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2/sam2/modeling\\sam\\mask_decoder.py, Line 15: class MaskDecoder(nn.Module):\n",
      "Line 16: def __init__(\n",
      "Line 16: def __init__(\n",
      "Line 17: self,\n",
      "Line 18: *,\n",
      "Line 19: transformer_dim: int,\n",
      "Line 20: transformer: nn.Module,\n",
      "Line 21: num_multimask_outputs: int = 3,\n",
      "Line 22: activation: Type[nn.Module] = nn.GELU,\n",
      "Line 23: iou_head_depth: int = 3,\n",
      "Line 24: iou_head_hidden_dim: int = 256,\n",
      "Line 25: use_high_res_features: bool = False,\n",
      "Line 26: iou_prediction_use_sigmoid=False,\n",
      "Line 27: dynamic_multimask_via_stability=False,\n",
      "Line 28: dynamic_multimask_stability_delta=0.05,\n",
      "Line 29: dynamic_multimask_stability_thresh=0.98,\n",
      "Line 30: pred_obj_scores: bool = False,\n",
      "Line 31: pred_obj_scores_mlp: bool = False,\n",
      "Line 32: use_multimask_token_for_obj_ptr: bool = False,\n",
      "Line 33: ) -> None:\n",
      "Line 34: \"\"\"\n",
      "Line 35: Predicts masks given an image and prompt embeddings, using a\n",
      "Line 36: transformer architecture.\n",
      "Line 37: \n",
      "Line 38: Arguments:\n",
      "Line 39: transformer_dim (int): the channel dimension of the transformer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "modeling_dir = \"D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2/sam2/modeling\"\n",
    "print(\"Searching for MaskDecoder in:\", modeling_dir)\n",
    "\n",
    "for root, dirs, files in os.walk(modeling_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".py\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    lines = f.readlines()\n",
    "                    for i, line in enumerate(lines):\n",
    "                        if 'class MaskDecoder(' in line:\n",
    "                            print(f\"Found MaskDecoder in {file_path}, Line {i+1}: {line.strip()}\")\n",
    "                            # Print __init__\n",
    "                            found_init = False\n",
    "                            for j in range(i, len(lines)):\n",
    "                                if 'def __init__(' in lines[j]:\n",
    "                                    found_init = True\n",
    "                                    print(f\"Line {j+1}: {lines[j].strip()}\")\n",
    "                                if found_init:\n",
    "                                    print(f\"Line {j+1}: {lines[j].strip()}\")\n",
    "                                    if '):' in lines[j]:\n",
    "                                        found_init = False\n",
    "                                        break\n",
    "                                if any(x in lines[j] for x in ['conv_s0', 'conv_s1', 'pred_obj_score_head', 'obj_ptr_proj']):\n",
    "                                    print(f\"Line {j+1}: {lines[j].strip()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for MaskDecoder components:\n",
      "Line 15: class MaskDecoder(nn.Module):\n",
      "Line 78: self.conv_s0 = nn.Conv2d(\n",
      "Line 81: self.conv_s1 = nn.Conv2d(\n",
      "Line 100: self.pred_obj_score_head = nn.Linear(transformer_dim, 1)\n",
      "Line 102: self.pred_obj_score_head = MLP(transformer_dim, transformer_dim, 1, 3)\n",
      "Line 240: object_score_logits = self.pred_obj_score_head(hs[:, 0, :])\n"
     ]
    }
   ],
   "source": [
    "with open(\"D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2/sam2/modeling/sam/mask_decoder.py\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    print(\"Searching for MaskDecoder components:\")\n",
    "    for i, line in enumerate(lines):\n",
    "        if any(x in line for x in ['conv_s0', 'conv_s1', 'pred_obj_score_head', 'obj_ptr_proj']):\n",
    "            print(f\"Line {i+1}: {line.strip()}\")\n",
    "        if 'MaskDecoder(nn.Module):' in line:\n",
    "            print(f\"Line {i+1}: {line.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for obj_ptr_proj:\n",
      "Line 54: # use_obj_ptrs_in_encoder=True and multimask_output_for_tracking=True\n",
      "Line 65: use_obj_ptrs_in_encoder=False,\n",
      "Line 66: # the maximum number of object pointers from other frames in encoder cross attention (only relevant when `use_obj_ptrs_in_encoder=True`)\n",
      "Line 68: # whether to add temporal positional encoding to the object pointers in the encoder (only relevant when `use_obj_ptrs_in_encoder=True`)\n",
      "Line 71: # with spatial positional encoding (only relevant when both `use_obj_ptrs_in_encoder=True` and `add_tpos_enc_to_obj_ptrs=True`)\n",
      "Line 74: # (only relevant when both `use_obj_ptrs_in_encoder=True` and `add_tpos_enc_to_obj_ptrs=True`)\n",
      "Line 77: # (only relevant when `use_obj_ptrs_in_encoder=True`; this might avoid pointer information too far in the future to distract the initial tracking)\n",
      "Line 83: # Only relevant if pred_obj_scores=True and use_obj_ptrs_in_encoder=True;\n",
      "Line 90: use_mlp_for_obj_ptr_proj: bool = False,\n",
      "Line 104: self.use_obj_ptrs_in_encoder = use_obj_ptrs_in_encoder\n",
      "Line 106: if use_obj_ptrs_in_encoder:\n",
      "Line 171: assert self.use_obj_ptrs_in_encoder\n",
      "Line 172: if self.pred_obj_scores and self.use_obj_ptrs_in_encoder:\n",
      "Line 175: self.use_mlp_for_obj_ptr_proj = use_mlp_for_obj_ptr_proj\n",
      "Line 241: if self.use_obj_ptrs_in_encoder:\n",
      "Line 243: self.obj_ptr_proj = torch.nn.Linear(self.hidden_dim, self.hidden_dim)\n",
      "Line 244: if self.use_mlp_for_obj_ptr_proj:\n",
      "Line 245: self.obj_ptr_proj = MLP(\n",
      "Line 249: self.obj_ptr_proj = torch.nn.Identity()\n",
      "Line 393: obj_ptr = self.obj_ptr_proj(sam_output_token)\n",
      "Line 433: if not self.use_obj_ptrs_in_encoder:\n",
      "Line 587: if self.use_obj_ptrs_in_encoder:\n"
     ]
    }
   ],
   "source": [
    "with open(\"D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2/sam2/modeling/sam2_base.py\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    print(\"Searching for obj_ptr_proj:\")\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'obj_ptr_proj' in line:\n",
    "            print(f\"Line {i+1}: {line.strip()}\")\n",
    "        if 'use_obj_ptrs_in_encoder' in line:\n",
    "            print(f\"Line {i+1}: {line.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated sam2_hiera_b+.yaml\n",
      "\n",
      "YAML content (last 15 lines):\n",
      "Line 1: num_maskmem: 7\n",
      "Line 2: image_size: 1024\n",
      "Line 3: backbone_stride: 16\n",
      "Line 4: sigmoid_scale_for_mem_enc: 1.0\n",
      "Line 5: sigmoid_bias_for_mem_enc: 0.0\n",
      "Line 6: binarize_mask_from_pts_for_mem_enc: false\n",
      "Line 7: use_mask_input_as_output_without_sam: false\n",
      "Line 8: max_cond_frames_in_attn: -1\n",
      "Line 9: directly_add_no_mem_embed: false\n",
      "Line 10: use_high_res_features_in_sam: true\n",
      "Line 11: multimask_output_in_sam: true\n",
      "Line 12: pred_obj_scores: true\n",
      "Line 13: pred_obj_scores_mlp: true\n",
      "Line 14: use_obj_ptrs_in_encoder: true\n",
      "Line 15: use_mlp_for_obj_ptr_proj: true\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "yaml_path = \"D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2/sam2_hiera_b+.yaml\"\n",
    "yaml_content = \"\"\"# @package _global_\n",
    "\n",
    "# Model\n",
    "model:\n",
    "  _target_: sam2.modeling.sam2_base.SAM2Base\n",
    "  image_encoder:\n",
    "    _target_: sam2.modeling.backbones.image_encoder.ImageEncoder\n",
    "    scalp: 1\n",
    "    trunk:\n",
    "      _target_: sam2.modeling.backbones.hieradet.Hiera\n",
    "      embed_dim: 112\n",
    "      num_heads: 2\n",
    "    neck:\n",
    "      _target_: sam2.modeling.backbones.image_encoder.FpnNeck\n",
    "      position_encoding:\n",
    "        _target_: sam2.modeling.position_encoding.PositionEmbeddingSine\n",
    "        num_pos_feats: 256\n",
    "        normalize: true\n",
    "        scale: null\n",
    "        temperature: 10000\n",
    "      d_model: 256\n",
    "      backbone_channel_list: [896, 448, 224, 112]\n",
    "      fpn_top_down_levels: [2, 3]\n",
    "      fpn_interp_model: nearest\n",
    "  memory_attention:\n",
    "    _target_: sam2.modeling.memory_attention.MemoryAttention\n",
    "    d_model: 256\n",
    "    pos_enc_at_input: true\n",
    "    layer:\n",
    "      _target_: sam2.modeling.memory_attention.MemoryAttentionLayer\n",
    "      activation: relu\n",
    "      dim_feedforward: 2048\n",
    "      dropout: 0.1\n",
    "      pos_enc_at_attn: false\n",
    "      self_attention:\n",
    "        _target_: sam2.modeling.sam.transformer.RoPEAttention\n",
    "        rope_theta: 10000.0\n",
    "        feat_sizes: [64, 64]\n",
    "        embedding_dim: 256\n",
    "        num_heads: 1\n",
    "        downsample_rate: 1\n",
    "        dropout: 0.1\n",
    "      d_model: 256\n",
    "      pos_enc_at_cross_attn_keys: true\n",
    "      pos_enc_at_cross_attn_queries: false\n",
    "      cross_attention:\n",
    "        _target_: sam2.modeling.sam.transformer.RoPEAttention\n",
    "        rope_theta: 10000.0\n",
    "        feat_sizes: [64, 64]\n",
    "        rope_k_repeat: true\n",
    "        embedding_dim: 256\n",
    "        num_heads: 1\n",
    "        downsample_rate: 1\n",
    "        dropout: 0.1\n",
    "        kv_in_dim: 64\n",
    "    num_layers: 4\n",
    "  memory_encoder:\n",
    "    _target_: sam2.modeling.memory_encoder.MemoryEncoder\n",
    "    out_dim: 64\n",
    "    position_encoding:\n",
    "      _target_: sam2.modeling.position_encoding.PositionEmbeddingSine\n",
    "      num_pos_feats: 64\n",
    "      normalize: true\n",
    "      scale: null\n",
    "      temperature: 10000\n",
    "    mask_downsampler:\n",
    "      _target_: sam2.modeling.memory_encoder.MaskDownSampler\n",
    "      kernel_size: 3\n",
    "      stride: 2\n",
    "      padding: 1\n",
    "    fuser:\n",
    "      _target_: sam2.modeling.memory_encoder.Fuser\n",
    "      layer:\n",
    "        _target_: sam2.modeling.memory_encoder.CXBlock\n",
    "        dim: 256\n",
    "        kernel_size: 7\n",
    "        padding: 3\n",
    "        layer_scale_init_value: 1e-6\n",
    "        use_dwconv: true\n",
    "      num_layers: 2\n",
    "  num_maskmem: 7\n",
    "  image_size: 1024\n",
    "  backbone_stride: 16\n",
    "  sigmoid_scale_for_mem_enc: 1.0\n",
    "  sigmoid_bias_for_mem_enc: 0.0\n",
    "  binarize_mask_from_pts_for_mem_enc: false\n",
    "  use_mask_input_as_output_without_sam: false\n",
    "  max_cond_frames_in_attn: -1\n",
    "  directly_add_no_mem_embed: false\n",
    "  use_high_res_features_in_sam: true\n",
    "  multimask_output_in_sam: true\n",
    "  pred_obj_scores: true\n",
    "  pred_obj_scores_mlp: true\n",
    "  use_obj_ptrs_in_encoder: true\n",
    "  use_mlp_for_obj_ptr_proj: true\n",
    "\"\"\"\n",
    "\n",
    "# Write YAML file\n",
    "with open(yaml_path, 'w') as f:\n",
    "    f.write(yaml_content)\n",
    "print(\"Updated sam2_hiera_b+.yaml\")\n",
    "\n",
    "# Verify content\n",
    "with open(yaml_path, 'r') as f:\n",
    "    content = f.readlines()\n",
    "    print(\"\\nYAML content (last 15 lines):\")\n",
    "    for i, line in enumerate(content[-15:], 1):\n",
    "        print(f\"Line {i}: {line.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded successfully!\n",
      "Config resolved successfully!\n",
      "Model config: {'_target_': 'sam2.modeling.sam2_base.SAM2Base', 'image_encoder': {'_target_': 'sam2.modeling.backbones.image_encoder.ImageEncoder', 'scalp': 1, 'trunk': {'_target_': 'sam2.modeling.backbones.hieradet.Hiera', 'embed_dim': 112, 'num_heads': 2}, 'neck': {'_target_': 'sam2.modeling.backbones.image_encoder.FpnNeck', 'position_encoding': {'_target_': 'sam2.modeling.position_encoding.PositionEmbeddingSine', 'num_pos_feats': 256, 'normalize': True, 'scale': None, 'temperature': 10000}, 'd_model': 256, 'backbone_channel_list': [896, 448, 224, 112], 'fpn_top_down_levels': [2, 3], 'fpn_interp_model': 'nearest'}}, 'memory_attention': {'_target_': 'sam2.modeling.memory_attention.MemoryAttention', 'd_model': 256, 'pos_enc_at_input': True, 'layer': {'_target_': 'sam2.modeling.memory_attention.MemoryAttentionLayer', 'activation': 'relu', 'dim_feedforward': 2048, 'dropout': 0.1, 'pos_enc_at_attn': False, 'self_attention': {'_target_': 'sam2.modeling.sam.transformer.RoPEAttention', 'rope_theta': 10000.0, 'feat_sizes': [64, 64], 'embedding_dim': 256, 'num_heads': 1, 'downsample_rate': 1, 'dropout': 0.1}, 'd_model': 256, 'pos_enc_at_cross_attn_keys': True, 'pos_enc_at_cross_attn_queries': False, 'cross_attention': {'_target_': 'sam2.modeling.sam.transformer.RoPEAttention', 'rope_theta': 10000.0, 'feat_sizes': [64, 64], 'rope_k_repeat': True, 'embedding_dim': 256, 'num_heads': 1, 'downsample_rate': 1, 'dropout': 0.1, 'kv_in_dim': 64}}, 'num_layers': 4}, 'memory_encoder': {'_target_': 'sam2.modeling.memory_encoder.MemoryEncoder', 'out_dim': 64, 'position_encoding': {'_target_': 'sam2.modeling.position_encoding.PositionEmbeddingSine', 'num_pos_feats': 64, 'normalize': True, 'scale': None, 'temperature': 10000}, 'mask_downsampler': {'_target_': 'sam2.modeling.memory_encoder.MaskDownSampler', 'kernel_size': 3, 'stride': 2, 'padding': 1}, 'fuser': {'_target_': 'sam2.modeling.memory_encoder.Fuser', 'layer': {'_target_': 'sam2.modeling.memory_encoder.CXBlock', 'dim': 256, 'kernel_size': 7, 'padding': 3, 'layer_scale_init_value': 1e-06, 'use_dwconv': True}, 'num_layers': 2}}, 'num_maskmem': 7, 'image_size': 1024, 'backbone_stride': 16, 'sigmoid_scale_for_mem_enc': 1.0, 'sigmoid_bias_for_mem_enc': 0.0, 'binarize_mask_from_pts_for_mem_enc': False, 'use_mask_input_as_output_without_sam': False, 'max_cond_frames_in_attn': -1, 'directly_add_no_mem_embed': False, 'use_high_res_features_in_sam': True, 'multimask_output_in_sam': True, 'pred_obj_scores': True, 'pred_obj_scores_mlp': True, 'use_obj_ptrs_in_encoder': True, 'use_mlp_for_obj_ptr_proj': True}\n",
      "Model instantiated, type: <class 'sam2.modeling.sam2_base.SAM2Base'>\n",
      "Checkpoint loaded successfully!\n",
      "Missing keys: []\n",
      "Unexpected keys: []\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import hydra\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "# Clear Hydra instance\n",
    "GlobalHydra.instance().clear()\n",
    "\n",
    "# Load config\n",
    "model_cfg = \"D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2/sam2_hiera_b+.yaml\"\n",
    "\n",
    "try:\n",
    "    cfg = OmegaConf.load(model_cfg)\n",
    "    print(\"Config loaded successfully!\")\n",
    "    OmegaConf.resolve(cfg)\n",
    "    print(\"Config resolved successfully!\")\n",
    "\n",
    "    # Inspect model config\n",
    "    print(\"Model config:\", cfg.get('model', 'No model key found'))\n",
    "\n",
    "    # Instantiate model\n",
    "    model = instantiate(cfg.model, _recursive_=True)\n",
    "    print(\"Model instantiated, type:\", type(model))\n",
    "\n",
    "    # Try loading checkpoint\n",
    "    checkpoint_path = \"D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2/checkpoints/sam2_hiera_base_plus.pt\"\n",
    "    sd = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True)[\"model\"]\n",
    "\n",
    "    try:\n",
    "        missing_keys, unexpected_keys = model.load_state_dict(sd, strict=True)\n",
    "        print(\"Checkpoint loaded successfully!\")\n",
    "        print(\"Missing keys:\", missing_keys)\n",
    "        print(\"Unexpected keys:\", unexpected_keys)\n",
    "    except Exception as e:\n",
    "        # This catches errors specifically from load_state_dict\n",
    "        print(f\"Error during load_state_dict: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    # This catches errors from the outer try block (e.g., file not found, config issues)\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: GlobalHydra is not initialized, use @hydra.main() or call one of the hydra initialization methods first\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sam2.build_sam import build_sam2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint_path = \"D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2/checkpoints/sam2_hiera_base_plus.pt\"\n",
    "model_cfg = \"D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2/sam2_hiera_b+.yaml\"\n",
    "\n",
    "try:\n",
    "    sam2_model = build_sam2(model_cfg, checkpoint_path, device=device)\n",
    "    print(\"SAM2 model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hydra config loaded successfully!\n",
      "SAM2 model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from sam2.build_sam import build_sam2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint_path = \"D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2/checkpoints/sam2_hiera_base_plus.pt\"\n",
    "model_cfg = \"sam2_hiera_b+.yaml\"\n",
    "config_dir = \"segment-anything-2\"  # Relative path to directory containing YAML\n",
    "\n",
    "try:\n",
    "    # Initialize Hydra with relative config path\n",
    "    with initialize(config_path=config_dir, version_base=None):\n",
    "        # Load the configuration\n",
    "        cfg = compose(config_name=model_cfg)\n",
    "        print(\"Hydra config loaded successfully!\")\n",
    "        \n",
    "        # Build SAM2 model\n",
    "        sam2_model = build_sam2(model_cfg, checkpoint_path, device=device)\n",
    "        print(\"SAM2 model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output root: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\n",
      "Validation predictions: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\n",
      "Test predictions: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\n",
      "Checkpoints: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\checkpoints\n",
      "Hydra config loaded successfully!\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Evaluating prompt strategies on validation set...\n",
      "Image 7001-56_jpg.rf.6b41799830de60d5c1185f81475f4912.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-56_jpg.rf.6b41799830de60d5c1185f81475f4912.jpg: box, Data: [[518 227 637 574]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7001-56_jpg.rf.6b41799830de60d5c1185f81475f4912.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7001-56_jpg.rf.6b41799830de60d5c1185f81475f4912.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-56_jpg.rf.6b41799830de60d5c1185f81475f4912.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-56_jpg.rf.6b41799830de60d5c1185f81475f4912.jpg\n",
      "Image 7003-230_jpg.rf.f0382c71bf78cd9eb9913ae435116136.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-230_jpg.rf.f0382c71bf78cd9eb9913ae435116136.jpg: point, Data: [[463 516]\n",
      " [425 506]\n",
      " [475 498]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7003-230_jpg.rf.f0382c71bf78cd9eb9913ae435116136.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7003-230_jpg.rf.f0382c71bf78cd9eb9913ae435116136.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-230_jpg.rf.f0382c71bf78cd9eb9913ae435116136.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-230_jpg.rf.f0382c71bf78cd9eb9913ae435116136.jpg\n",
      "Image 1-s2_0-S0950061807002760-gr1_jpg.rf.0ca3b381b6e25bf6bcf0337f5df2e485.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 1-s2_0-S0950061807002760-gr1_jpg.rf.0ca3b381b6e25bf6bcf0337f5df2e485.jpg: point, Data: [[399 124]\n",
      " [361 375]\n",
      " [345 403]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 1-s2_0-S0950061807002760-gr1_jpg.rf.0ca3b381b6e25bf6bcf0337f5df2e485.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 1-s2_0-S0950061807002760-gr1_jpg.rf.0ca3b381b6e25bf6bcf0337f5df2e485.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_1-s2_0-S0950061807002760-gr1_jpg.rf.0ca3b381b6e25bf6bcf0337f5df2e485.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_1-s2_0-S0950061807002760-gr1_jpg.rf.0ca3b381b6e25bf6bcf0337f5df2e485.jpg\n",
      "Image 7002-63_jpg.rf.343ef5463eee93e68ef3f9a522c20223.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-63_jpg.rf.343ef5463eee93e68ef3f9a522c20223.jpg: box, Data: [[  5 210 633 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-63_jpg.rf.343ef5463eee93e68ef3f9a522c20223.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-63_jpg.rf.343ef5463eee93e68ef3f9a522c20223.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-63_jpg.rf.343ef5463eee93e68ef3f9a522c20223.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-63_jpg.rf.343ef5463eee93e68ef3f9a522c20223.jpg\n",
      "Image Pitted-concrete-driveway_webp.rf.1a295b83f95d88a3b98b5dd67ac4e67d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Pitted-concrete-driveway_webp.rf.1a295b83f95d88a3b98b5dd67ac4e67d.jpg: point, Data: [[300 319]\n",
      " [316 215]\n",
      " [334 214]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image Pitted-concrete-driveway_webp.rf.1a295b83f95d88a3b98b5dd67ac4e67d.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image Pitted-concrete-driveway_webp.rf.1a295b83f95d88a3b98b5dd67ac4e67d.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_Pitted-concrete-driveway_webp.rf.1a295b83f95d88a3b98b5dd67ac4e67d.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_Pitted-concrete-driveway_webp.rf.1a295b83f95d88a3b98b5dd67ac4e67d.jpg\n",
      "Image 7001-26_jpg.rf.28b5f963432adbc0264f760edf91a357.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-26_jpg.rf.28b5f963432adbc0264f760edf91a357.jpg: point, Data: [[352 524]\n",
      " [135 558]\n",
      " [ 86 570]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7001-26_jpg.rf.28b5f963432adbc0264f760edf91a357.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7001-26_jpg.rf.28b5f963432adbc0264f760edf91a357.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-26_jpg.rf.28b5f963432adbc0264f760edf91a357.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-26_jpg.rf.28b5f963432adbc0264f760edf91a357.jpg\n",
      "Image 7002-46_jpg.rf.5f20e9fb37b8bc54ee5024bcd1a49775.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-46_jpg.rf.5f20e9fb37b8bc54ee5024bcd1a49775.jpg: point, Data: [[506 147]\n",
      " [529 597]\n",
      " [521 320]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-46_jpg.rf.5f20e9fb37b8bc54ee5024bcd1a49775.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-46_jpg.rf.5f20e9fb37b8bc54ee5024bcd1a49775.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-46_jpg.rf.5f20e9fb37b8bc54ee5024bcd1a49775.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-46_jpg.rf.5f20e9fb37b8bc54ee5024bcd1a49775.jpg\n",
      "Image old-cement-wall-texture-background_493325-1497_avif.rf.fb8de7c96d23acbf5af5737c3f5aa987.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for old-cement-wall-texture-background_493325-1497_avif.rf.fb8de7c96d23acbf5af5737c3f5aa987.jpg: point, Data: [[389 166]\n",
      " [403  78]\n",
      " [411 228]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image old-cement-wall-texture-background_493325-1497_avif.rf.fb8de7c96d23acbf5af5737c3f5aa987.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image old-cement-wall-texture-background_493325-1497_avif.rf.fb8de7c96d23acbf5af5737c3f5aa987.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_old-cement-wall-texture-background_493325-1497_avif.rf.fb8de7c96d23acbf5af5737c3f5aa987.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_old-cement-wall-texture-background_493325-1497_avif.rf.fb8de7c96d23acbf5af5737c3f5aa987.jpg\n",
      "Image what-are-these-holes-v0-tf1h89m5q5hb1_webp.rf.64d3f4180a20b7b6483ee967216e1057.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for what-are-these-holes-v0-tf1h89m5q5hb1_webp.rf.64d3f4180a20b7b6483ee967216e1057.jpg: box, Data: [[119 345 310 388]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image what-are-these-holes-v0-tf1h89m5q5hb1_webp.rf.64d3f4180a20b7b6483ee967216e1057.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image what-are-these-holes-v0-tf1h89m5q5hb1_webp.rf.64d3f4180a20b7b6483ee967216e1057.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_what-are-these-holes-v0-tf1h89m5q5hb1_webp.rf.64d3f4180a20b7b6483ee967216e1057.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_what-are-these-holes-v0-tf1h89m5q5hb1_webp.rf.64d3f4180a20b7b6483ee967216e1057.jpg\n",
      "Image 7003-162_jpg.rf.358aa953aaf0bcf271e111470692f795.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-162_jpg.rf.358aa953aaf0bcf271e111470692f795.jpg: box, Data: [[220 324 367 556]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7003-162_jpg.rf.358aa953aaf0bcf271e111470692f795.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7003-162_jpg.rf.358aa953aaf0bcf271e111470692f795.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-162_jpg.rf.358aa953aaf0bcf271e111470692f795.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-162_jpg.rf.358aa953aaf0bcf271e111470692f795.jpg\n",
      "Image 7002-152_jpg.rf.f3f8c554902b1e06650d38336872d8d9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-152_jpg.rf.f3f8c554902b1e06650d38336872d8d9.jpg: point, Data: [[571  77]\n",
      " [556  62]\n",
      " [513  51]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-152_jpg.rf.f3f8c554902b1e06650d38336872d8d9.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-152_jpg.rf.f3f8c554902b1e06650d38336872d8d9.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-152_jpg.rf.f3f8c554902b1e06650d38336872d8d9.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-152_jpg.rf.f3f8c554902b1e06650d38336872d8d9.jpg\n",
      "Image 7002-41_jpg.rf.75c10941e7deb4f072ed787bbfddbf94.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-41_jpg.rf.75c10941e7deb4f072ed787bbfddbf94.jpg: point, Data: [[371 390]\n",
      " [347 366]\n",
      " [348 365]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-41_jpg.rf.75c10941e7deb4f072ed787bbfddbf94.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-41_jpg.rf.75c10941e7deb4f072ed787bbfddbf94.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-41_jpg.rf.75c10941e7deb4f072ed787bbfddbf94.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-41_jpg.rf.75c10941e7deb4f072ed787bbfddbf94.jpg\n",
      "Image 7002-122_jpg.rf.a96be481b527b028ec8f290b10ec45e4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-122_jpg.rf.a96be481b527b028ec8f290b10ec45e4.jpg: point, Data: [[428 352]\n",
      " [441 330]\n",
      " [521 530]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-122_jpg.rf.a96be481b527b028ec8f290b10ec45e4.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-122_jpg.rf.a96be481b527b028ec8f290b10ec45e4.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-122_jpg.rf.a96be481b527b028ec8f290b10ec45e4.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-122_jpg.rf.a96be481b527b028ec8f290b10ec45e4.jpg\n",
      "Image 7003-15_jpg.rf.c4321c5bb9ee70d1168f318550a937f9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-15_jpg.rf.c4321c5bb9ee70d1168f318550a937f9.jpg: point, Data: [[416  40]\n",
      " [607 381]\n",
      " [617 444]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7003-15_jpg.rf.c4321c5bb9ee70d1168f318550a937f9.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7003-15_jpg.rf.c4321c5bb9ee70d1168f318550a937f9.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-15_jpg.rf.c4321c5bb9ee70d1168f318550a937f9.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-15_jpg.rf.c4321c5bb9ee70d1168f318550a937f9.jpg\n",
      "Image 10021_jpg.rf.27dc3e7ae6ef37ccc8cee732dc97cb7e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10021_jpg.rf.27dc3e7ae6ef37ccc8cee732dc97cb7e.jpg: point, Data: [[422 308]\n",
      " [250 124]\n",
      " [390 339]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 10021_jpg.rf.27dc3e7ae6ef37ccc8cee732dc97cb7e.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 10021_jpg.rf.27dc3e7ae6ef37ccc8cee732dc97cb7e.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10021_jpg.rf.27dc3e7ae6ef37ccc8cee732dc97cb7e.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10021_jpg.rf.27dc3e7ae6ef37ccc8cee732dc97cb7e.jpg\n",
      "Image 7002-162_jpg.rf.ba7d3dc9ba6b9c2352f856ea5f2938af.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-162_jpg.rf.ba7d3dc9ba6b9c2352f856ea5f2938af.jpg: point, Data: [[ 30 520]\n",
      " [133 517]\n",
      " [126 459]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-162_jpg.rf.ba7d3dc9ba6b9c2352f856ea5f2938af.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-162_jpg.rf.ba7d3dc9ba6b9c2352f856ea5f2938af.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-162_jpg.rf.ba7d3dc9ba6b9c2352f856ea5f2938af.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-162_jpg.rf.ba7d3dc9ba6b9c2352f856ea5f2938af.jpg\n",
      "Image 7001-139_jpg.rf.519f4029434cfd9b4e61292b7a6bed74.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-139_jpg.rf.519f4029434cfd9b4e61292b7a6bed74.jpg: box, Data: [[279   0 422 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7001-139_jpg.rf.519f4029434cfd9b4e61292b7a6bed74.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7001-139_jpg.rf.519f4029434cfd9b4e61292b7a6bed74.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-139_jpg.rf.519f4029434cfd9b4e61292b7a6bed74.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-139_jpg.rf.519f4029434cfd9b4e61292b7a6bed74.jpg\n",
      "Image 7003-18_jpg.rf.f654fe70de618f0b5b3c0ad1b46e3e79.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-18_jpg.rf.f654fe70de618f0b5b3c0ad1b46e3e79.jpg: box, Data: [[  0 188 636 564]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7003-18_jpg.rf.f654fe70de618f0b5b3c0ad1b46e3e79.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7003-18_jpg.rf.f654fe70de618f0b5b3c0ad1b46e3e79.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-18_jpg.rf.f654fe70de618f0b5b3c0ad1b46e3e79.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-18_jpg.rf.f654fe70de618f0b5b3c0ad1b46e3e79.jpg\n",
      "Image 7003-16_jpg.rf.8a8f6680174644066b5ed459e40d8ef9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-16_jpg.rf.8a8f6680174644066b5ed459e40d8ef9.jpg: point, Data: [[167 410]\n",
      " [392 634]\n",
      " [361 435]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7003-16_jpg.rf.8a8f6680174644066b5ed459e40d8ef9.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7003-16_jpg.rf.8a8f6680174644066b5ed459e40d8ef9.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-16_jpg.rf.8a8f6680174644066b5ed459e40d8ef9.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-16_jpg.rf.8a8f6680174644066b5ed459e40d8ef9.jpg\n",
      "Image concrete-efflorescence-white-substance-oozing-steps-known-as-caused-salts-leeching-damp-can-be-sticky-first-259924023_webp.rf.d8521b43430773837879944ead17e4cc.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-efflorescence-white-substance-oozing-steps-known-as-caused-salts-leeching-damp-can-be-sticky-first-259924023_webp.rf.d8521b43430773837879944ead17e4cc.jpg: box, Data: [[  0 163 638 393]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image concrete-efflorescence-white-substance-oozing-steps-known-as-caused-salts-leeching-damp-can-be-sticky-first-259924023_webp.rf.d8521b43430773837879944ead17e4cc.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image concrete-efflorescence-white-substance-oozing-steps-known-as-caused-salts-leeching-damp-can-be-sticky-first-259924023_webp.rf.d8521b43430773837879944ead17e4cc.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_concrete-efflorescence-white-substance-oozing-steps-known-as-caused-salts-leeching-damp-can-be-sticky-first-259924023_webp.rf.d8521b43430773837879944ead17e4cc.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_concrete-efflorescence-white-substance-oozing-steps-known-as-caused-salts-leeching-damp-can-be-sticky-first-259924023_webp.rf.d8521b43430773837879944ead17e4cc.jpg\n",
      "Image 10045_jpg.rf.04d4188de1c967f33755e9329cc000a0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10045_jpg.rf.04d4188de1c967f33755e9329cc000a0.jpg: box, Data: [[  0 336 623 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 10045_jpg.rf.04d4188de1c967f33755e9329cc000a0.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 10045_jpg.rf.04d4188de1c967f33755e9329cc000a0.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10045_jpg.rf.04d4188de1c967f33755e9329cc000a0.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10045_jpg.rf.04d4188de1c967f33755e9329cc000a0.jpg\n",
      "Image 71332b074718b5893f67f6fb2aa6f73b54ffc0e3_jpeg.rf.faed23e522e4dff600e627d1b992bb5b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 71332b074718b5893f67f6fb2aa6f73b54ffc0e3_jpeg.rf.faed23e522e4dff600e627d1b992bb5b.jpg: box, Data: [[173 112 461 518]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 71332b074718b5893f67f6fb2aa6f73b54ffc0e3_jpeg.rf.faed23e522e4dff600e627d1b992bb5b.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 71332b074718b5893f67f6fb2aa6f73b54ffc0e3_jpeg.rf.faed23e522e4dff600e627d1b992bb5b.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_71332b074718b5893f67f6fb2aa6f73b54ffc0e3_jpeg.rf.faed23e522e4dff600e627d1b992bb5b.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_71332b074718b5893f67f6fb2aa6f73b54ffc0e3_jpeg.rf.faed23e522e4dff600e627d1b992bb5b.jpg\n",
      "Image 7002-194_jpg.rf.cfded4786c5b0c09665b527d636136a8.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-194_jpg.rf.cfded4786c5b0c09665b527d636136a8.jpg: point, Data: [[630 115]\n",
      " [374 581]\n",
      " [115  93]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-194_jpg.rf.cfded4786c5b0c09665b527d636136a8.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-194_jpg.rf.cfded4786c5b0c09665b527d636136a8.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-194_jpg.rf.cfded4786c5b0c09665b527d636136a8.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-194_jpg.rf.cfded4786c5b0c09665b527d636136a8.jpg\n",
      "Image PO1_jpg.rf.a9389a85ee53bd69f5e42f96085cf242.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for PO1_jpg.rf.a9389a85ee53bd69f5e42f96085cf242.jpg: point, Data: [[450 547]\n",
      " [476 530]\n",
      " [490 591]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image PO1_jpg.rf.a9389a85ee53bd69f5e42f96085cf242.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image PO1_jpg.rf.a9389a85ee53bd69f5e42f96085cf242.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_PO1_jpg.rf.a9389a85ee53bd69f5e42f96085cf242.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_PO1_jpg.rf.a9389a85ee53bd69f5e42f96085cf242.jpg\n",
      "Image Trachyt_Herz_Jesu_Kirche_Aachen_jpg.rf.5952b07919513f7843dd666322c8ff17.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Trachyt_Herz_Jesu_Kirche_Aachen_jpg.rf.5952b07919513f7843dd666322c8ff17.jpg: point, Data: [[357 505]\n",
      " [313 491]\n",
      " [ 73  90]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image Trachyt_Herz_Jesu_Kirche_Aachen_jpg.rf.5952b07919513f7843dd666322c8ff17.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image Trachyt_Herz_Jesu_Kirche_Aachen_jpg.rf.5952b07919513f7843dd666322c8ff17.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_Trachyt_Herz_Jesu_Kirche_Aachen_jpg.rf.5952b07919513f7843dd666322c8ff17.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_Trachyt_Herz_Jesu_Kirche_Aachen_jpg.rf.5952b07919513f7843dd666322c8ff17.jpg\n",
      "Image images-3-_jpg.rf.378d487bfdbe9774997c218795c1b52e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-3-_jpg.rf.378d487bfdbe9774997c218795c1b52e.jpg: box, Data: [[ 69 262 422 559]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image images-3-_jpg.rf.378d487bfdbe9774997c218795c1b52e.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image images-3-_jpg.rf.378d487bfdbe9774997c218795c1b52e.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_images-3-_jpg.rf.378d487bfdbe9774997c218795c1b52e.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_images-3-_jpg.rf.378d487bfdbe9774997c218795c1b52e.jpg\n",
      "Image 10146_jpg.rf.1448fe577ad0ad4e3791683c77aca7ef.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10146_jpg.rf.1448fe577ad0ad4e3791683c77aca7ef.jpg: point, Data: [[463  74]\n",
      " [617  48]\n",
      " [310 253]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 10146_jpg.rf.1448fe577ad0ad4e3791683c77aca7ef.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 10146_jpg.rf.1448fe577ad0ad4e3791683c77aca7ef.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10146_jpg.rf.1448fe577ad0ad4e3791683c77aca7ef.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10146_jpg.rf.1448fe577ad0ad4e3791683c77aca7ef.jpg\n",
      "Image 7002-124_jpg.rf.1a3eb255fc816ff8d13f0693d6f9032a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-124_jpg.rf.1a3eb255fc816ff8d13f0693d6f9032a.jpg: point, Data: [[153 130]\n",
      " [159  51]\n",
      " [229  75]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-124_jpg.rf.1a3eb255fc816ff8d13f0693d6f9032a.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-124_jpg.rf.1a3eb255fc816ff8d13f0693d6f9032a.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-124_jpg.rf.1a3eb255fc816ff8d13f0693d6f9032a.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-124_jpg.rf.1a3eb255fc816ff8d13f0693d6f9032a.jpg\n",
      "Image 10062_jpg.rf.85bdbf0d7ca1d7aa6cdad897ed028261.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10062_jpg.rf.85bdbf0d7ca1d7aa6cdad897ed028261.jpg: box, Data: [[  0 168 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 10062_jpg.rf.85bdbf0d7ca1d7aa6cdad897ed028261.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 10062_jpg.rf.85bdbf0d7ca1d7aa6cdad897ed028261.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10062_jpg.rf.85bdbf0d7ca1d7aa6cdad897ed028261.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10062_jpg.rf.85bdbf0d7ca1d7aa6cdad897ed028261.jpg\n",
      "Image 10063_jpg.rf.e51ff14750880a4070ebd4dc8242fa8b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10063_jpg.rf.e51ff14750880a4070ebd4dc8242fa8b.jpg: box, Data: [[  1   0 639 492]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 10063_jpg.rf.e51ff14750880a4070ebd4dc8242fa8b.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 10063_jpg.rf.e51ff14750880a4070ebd4dc8242fa8b.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10063_jpg.rf.e51ff14750880a4070ebd4dc8242fa8b.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10063_jpg.rf.e51ff14750880a4070ebd4dc8242fa8b.jpg\n",
      "Image 10064_jpg.rf.03a7e7512851767ca801357dccd6a49b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10064_jpg.rf.03a7e7512851767ca801357dccd6a49b.jpg: point, Data: [[265 153]\n",
      " [104 227]\n",
      " [601 235]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 10064_jpg.rf.03a7e7512851767ca801357dccd6a49b.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 10064_jpg.rf.03a7e7512851767ca801357dccd6a49b.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10064_jpg.rf.03a7e7512851767ca801357dccd6a49b.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10064_jpg.rf.03a7e7512851767ca801357dccd6a49b.jpg\n",
      "Image 7001-79_jpg.rf.2adf95ef63cda5606796a107c91f76b7.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-79_jpg.rf.2adf95ef63cda5606796a107c91f76b7.jpg: point, Data: [[ 16 469]\n",
      " [ 36 525]\n",
      " [  3 470]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7001-79_jpg.rf.2adf95ef63cda5606796a107c91f76b7.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7001-79_jpg.rf.2adf95ef63cda5606796a107c91f76b7.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-79_jpg.rf.2adf95ef63cda5606796a107c91f76b7.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-79_jpg.rf.2adf95ef63cda5606796a107c91f76b7.jpg\n",
      "Image 7002-33_jpg.rf.72590d9eddfb351afb7cb0cbcb99aa69.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-33_jpg.rf.72590d9eddfb351afb7cb0cbcb99aa69.jpg: box, Data: [[  1 471 370 590]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-33_jpg.rf.72590d9eddfb351afb7cb0cbcb99aa69.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-33_jpg.rf.72590d9eddfb351afb7cb0cbcb99aa69.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-33_jpg.rf.72590d9eddfb351afb7cb0cbcb99aa69.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-33_jpg.rf.72590d9eddfb351afb7cb0cbcb99aa69.jpg\n",
      "Image 7001-27_jpg.rf.35b211a2ba92caecdadfd1ebb15d9d7d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-27_jpg.rf.35b211a2ba92caecdadfd1ebb15d9d7d.jpg: point, Data: [[106 528]\n",
      " [182 491]\n",
      " [329 438]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7001-27_jpg.rf.35b211a2ba92caecdadfd1ebb15d9d7d.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7001-27_jpg.rf.35b211a2ba92caecdadfd1ebb15d9d7d.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-27_jpg.rf.35b211a2ba92caecdadfd1ebb15d9d7d.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-27_jpg.rf.35b211a2ba92caecdadfd1ebb15d9d7d.jpg\n",
      "Image images-6-_jpg.rf.6f05d42ec0df0d276fc1792e925a1eaf.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-6-_jpg.rf.6f05d42ec0df0d276fc1792e925a1eaf.jpg: point, Data: [[278 315]\n",
      " [532 615]\n",
      " [334 431]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image images-6-_jpg.rf.6f05d42ec0df0d276fc1792e925a1eaf.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image images-6-_jpg.rf.6f05d42ec0df0d276fc1792e925a1eaf.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_images-6-_jpg.rf.6f05d42ec0df0d276fc1792e925a1eaf.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_images-6-_jpg.rf.6f05d42ec0df0d276fc1792e925a1eaf.jpg\n",
      "Image 10031_jpg.rf.34a5f65dd73f98a3d7389f37da869166.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10031_jpg.rf.34a5f65dd73f98a3d7389f37da869166.jpg: box, Data: [[140   0 522 622]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 10031_jpg.rf.34a5f65dd73f98a3d7389f37da869166.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 10031_jpg.rf.34a5f65dd73f98a3d7389f37da869166.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10031_jpg.rf.34a5f65dd73f98a3d7389f37da869166.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10031_jpg.rf.34a5f65dd73f98a3d7389f37da869166.jpg\n",
      "Image 7002-81_jpg.rf.fb3f28f676754c0b197db91090d31267.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-81_jpg.rf.fb3f28f676754c0b197db91090d31267.jpg: box, Data: [[246   0 378 613]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-81_jpg.rf.fb3f28f676754c0b197db91090d31267.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-81_jpg.rf.fb3f28f676754c0b197db91090d31267.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-81_jpg.rf.fb3f28f676754c0b197db91090d31267.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-81_jpg.rf.fb3f28f676754c0b197db91090d31267.jpg\n",
      "Image 10066_jpg.rf.4f2c688fef6906e66119534c3af68586.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10066_jpg.rf.4f2c688fef6906e66119534c3af68586.jpg: point, Data: [[169 527]\n",
      " [158 477]\n",
      " [145 470]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 10066_jpg.rf.4f2c688fef6906e66119534c3af68586.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 10066_jpg.rf.4f2c688fef6906e66119534c3af68586.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10066_jpg.rf.4f2c688fef6906e66119534c3af68586.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10066_jpg.rf.4f2c688fef6906e66119534c3af68586.jpg\n",
      "Image anyone-know-what-these-craters-come-from-v0-u0dlp1u6el2b1_webp.rf.e1b25b99f77f4328a70f558d5cd083bf.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for anyone-know-what-these-craters-come-from-v0-u0dlp1u6el2b1_webp.rf.e1b25b99f77f4328a70f558d5cd083bf.jpg: point, Data: [[279 307]\n",
      " [302 345]\n",
      " [389 343]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image anyone-know-what-these-craters-come-from-v0-u0dlp1u6el2b1_webp.rf.e1b25b99f77f4328a70f558d5cd083bf.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image anyone-know-what-these-craters-come-from-v0-u0dlp1u6el2b1_webp.rf.e1b25b99f77f4328a70f558d5cd083bf.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_anyone-know-what-these-craters-come-from-v0-u0dlp1u6el2b1_webp.rf.e1b25b99f77f4328a70f558d5cd083bf.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_anyone-know-what-these-craters-come-from-v0-u0dlp1u6el2b1_webp.rf.e1b25b99f77f4328a70f558d5cd083bf.jpg\n",
      "Image 7001-56_jpg.rf.6b41799830de60d5c1185f81475f4912.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-56_jpg.rf.6b41799830de60d5c1185f81475f4912.jpg: point, Data: [[605 252]\n",
      " [623 286]\n",
      " [574 310]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7001-56_jpg.rf.6b41799830de60d5c1185f81475f4912.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7001-56_jpg.rf.6b41799830de60d5c1185f81475f4912.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-56_jpg.rf.6b41799830de60d5c1185f81475f4912.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-56_jpg.rf.6b41799830de60d5c1185f81475f4912.jpg\n",
      "Image 7003-230_jpg.rf.f0382c71bf78cd9eb9913ae435116136.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-230_jpg.rf.f0382c71bf78cd9eb9913ae435116136.jpg: box, Data: [[ 62 116 477 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7003-230_jpg.rf.f0382c71bf78cd9eb9913ae435116136.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7003-230_jpg.rf.f0382c71bf78cd9eb9913ae435116136.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-230_jpg.rf.f0382c71bf78cd9eb9913ae435116136.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-230_jpg.rf.f0382c71bf78cd9eb9913ae435116136.jpg\n",
      "Image 1-s2_0-S0950061807002760-gr1_jpg.rf.0ca3b381b6e25bf6bcf0337f5df2e485.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 1-s2_0-S0950061807002760-gr1_jpg.rf.0ca3b381b6e25bf6bcf0337f5df2e485.jpg: box, Data: [[165  80 464 477]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 1-s2_0-S0950061807002760-gr1_jpg.rf.0ca3b381b6e25bf6bcf0337f5df2e485.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 1-s2_0-S0950061807002760-gr1_jpg.rf.0ca3b381b6e25bf6bcf0337f5df2e485.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_1-s2_0-S0950061807002760-gr1_jpg.rf.0ca3b381b6e25bf6bcf0337f5df2e485.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_1-s2_0-S0950061807002760-gr1_jpg.rf.0ca3b381b6e25bf6bcf0337f5df2e485.jpg\n",
      "Image 7002-63_jpg.rf.343ef5463eee93e68ef3f9a522c20223.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-63_jpg.rf.343ef5463eee93e68ef3f9a522c20223.jpg: point, Data: [[389 251]\n",
      " [404 426]\n",
      " [463 424]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-63_jpg.rf.343ef5463eee93e68ef3f9a522c20223.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-63_jpg.rf.343ef5463eee93e68ef3f9a522c20223.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-63_jpg.rf.343ef5463eee93e68ef3f9a522c20223.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-63_jpg.rf.343ef5463eee93e68ef3f9a522c20223.jpg\n",
      "Image Pitted-concrete-driveway_webp.rf.1a295b83f95d88a3b98b5dd67ac4e67d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Pitted-concrete-driveway_webp.rf.1a295b83f95d88a3b98b5dd67ac4e67d.jpg: box, Data: [[280 200 401 472]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image Pitted-concrete-driveway_webp.rf.1a295b83f95d88a3b98b5dd67ac4e67d.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image Pitted-concrete-driveway_webp.rf.1a295b83f95d88a3b98b5dd67ac4e67d.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_Pitted-concrete-driveway_webp.rf.1a295b83f95d88a3b98b5dd67ac4e67d.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_Pitted-concrete-driveway_webp.rf.1a295b83f95d88a3b98b5dd67ac4e67d.jpg\n",
      "Image 7001-26_jpg.rf.28b5f963432adbc0264f760edf91a357.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-26_jpg.rf.28b5f963432adbc0264f760edf91a357.jpg: point, Data: [[517 526]\n",
      " [ 81 564]\n",
      " [221 497]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7001-26_jpg.rf.28b5f963432adbc0264f760edf91a357.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7001-26_jpg.rf.28b5f963432adbc0264f760edf91a357.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-26_jpg.rf.28b5f963432adbc0264f760edf91a357.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-26_jpg.rf.28b5f963432adbc0264f760edf91a357.jpg\n",
      "Image 7002-46_jpg.rf.5f20e9fb37b8bc54ee5024bcd1a49775.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-46_jpg.rf.5f20e9fb37b8bc54ee5024bcd1a49775.jpg: box, Data: [[463   0 639 623]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-46_jpg.rf.5f20e9fb37b8bc54ee5024bcd1a49775.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-46_jpg.rf.5f20e9fb37b8bc54ee5024bcd1a49775.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-46_jpg.rf.5f20e9fb37b8bc54ee5024bcd1a49775.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-46_jpg.rf.5f20e9fb37b8bc54ee5024bcd1a49775.jpg\n",
      "Image old-cement-wall-texture-background_493325-1497_avif.rf.fb8de7c96d23acbf5af5737c3f5aa987.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for old-cement-wall-texture-background_493325-1497_avif.rf.fb8de7c96d23acbf5af5737c3f5aa987.jpg: point, Data: [[444 249]\n",
      " [415 151]\n",
      " [348 212]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image old-cement-wall-texture-background_493325-1497_avif.rf.fb8de7c96d23acbf5af5737c3f5aa987.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image old-cement-wall-texture-background_493325-1497_avif.rf.fb8de7c96d23acbf5af5737c3f5aa987.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_old-cement-wall-texture-background_493325-1497_avif.rf.fb8de7c96d23acbf5af5737c3f5aa987.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_old-cement-wall-texture-background_493325-1497_avif.rf.fb8de7c96d23acbf5af5737c3f5aa987.jpg\n",
      "Image what-are-these-holes-v0-tf1h89m5q5hb1_webp.rf.64d3f4180a20b7b6483ee967216e1057.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for what-are-these-holes-v0-tf1h89m5q5hb1_webp.rf.64d3f4180a20b7b6483ee967216e1057.jpg: point, Data: [[279 382]\n",
      " [238 368]\n",
      " [277 355]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image what-are-these-holes-v0-tf1h89m5q5hb1_webp.rf.64d3f4180a20b7b6483ee967216e1057.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image what-are-these-holes-v0-tf1h89m5q5hb1_webp.rf.64d3f4180a20b7b6483ee967216e1057.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_what-are-these-holes-v0-tf1h89m5q5hb1_webp.rf.64d3f4180a20b7b6483ee967216e1057.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_what-are-these-holes-v0-tf1h89m5q5hb1_webp.rf.64d3f4180a20b7b6483ee967216e1057.jpg\n",
      "Image 7003-162_jpg.rf.358aa953aaf0bcf271e111470692f795.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-162_jpg.rf.358aa953aaf0bcf271e111470692f795.jpg: point, Data: [[225 394]\n",
      " [355 524]\n",
      " [284 325]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7003-162_jpg.rf.358aa953aaf0bcf271e111470692f795.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7003-162_jpg.rf.358aa953aaf0bcf271e111470692f795.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-162_jpg.rf.358aa953aaf0bcf271e111470692f795.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-162_jpg.rf.358aa953aaf0bcf271e111470692f795.jpg\n",
      "Image 7002-152_jpg.rf.f3f8c554902b1e06650d38336872d8d9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-152_jpg.rf.f3f8c554902b1e06650d38336872d8d9.jpg: box, Data: [[500   0 588 160]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-152_jpg.rf.f3f8c554902b1e06650d38336872d8d9.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-152_jpg.rf.f3f8c554902b1e06650d38336872d8d9.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-152_jpg.rf.f3f8c554902b1e06650d38336872d8d9.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-152_jpg.rf.f3f8c554902b1e06650d38336872d8d9.jpg\n",
      "Image 7002-41_jpg.rf.75c10941e7deb4f072ed787bbfddbf94.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-41_jpg.rf.75c10941e7deb4f072ed787bbfddbf94.jpg: box, Data: [[274 356 378 491]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-41_jpg.rf.75c10941e7deb4f072ed787bbfddbf94.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-41_jpg.rf.75c10941e7deb4f072ed787bbfddbf94.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-41_jpg.rf.75c10941e7deb4f072ed787bbfddbf94.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-41_jpg.rf.75c10941e7deb4f072ed787bbfddbf94.jpg\n",
      "Image 7002-122_jpg.rf.a96be481b527b028ec8f290b10ec45e4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-122_jpg.rf.a96be481b527b028ec8f290b10ec45e4.jpg: point, Data: [[472 474]\n",
      " [379 142]\n",
      " [484 478]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-122_jpg.rf.a96be481b527b028ec8f290b10ec45e4.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-122_jpg.rf.a96be481b527b028ec8f290b10ec45e4.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-122_jpg.rf.a96be481b527b028ec8f290b10ec45e4.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-122_jpg.rf.a96be481b527b028ec8f290b10ec45e4.jpg\n",
      "Image 7003-15_jpg.rf.c4321c5bb9ee70d1168f318550a937f9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-15_jpg.rf.c4321c5bb9ee70d1168f318550a937f9.jpg: box, Data: [[  1   5 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7003-15_jpg.rf.c4321c5bb9ee70d1168f318550a937f9.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7003-15_jpg.rf.c4321c5bb9ee70d1168f318550a937f9.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-15_jpg.rf.c4321c5bb9ee70d1168f318550a937f9.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-15_jpg.rf.c4321c5bb9ee70d1168f318550a937f9.jpg\n",
      "Image 10021_jpg.rf.27dc3e7ae6ef37ccc8cee732dc97cb7e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10021_jpg.rf.27dc3e7ae6ef37ccc8cee732dc97cb7e.jpg: box, Data: [[209   0 484 374]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 10021_jpg.rf.27dc3e7ae6ef37ccc8cee732dc97cb7e.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 10021_jpg.rf.27dc3e7ae6ef37ccc8cee732dc97cb7e.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10021_jpg.rf.27dc3e7ae6ef37ccc8cee732dc97cb7e.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10021_jpg.rf.27dc3e7ae6ef37ccc8cee732dc97cb7e.jpg\n",
      "Image 7002-162_jpg.rf.ba7d3dc9ba6b9c2352f856ea5f2938af.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-162_jpg.rf.ba7d3dc9ba6b9c2352f856ea5f2938af.jpg: point, Data: [[ 78 522]\n",
      " [126 492]\n",
      " [110 515]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-162_jpg.rf.ba7d3dc9ba6b9c2352f856ea5f2938af.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-162_jpg.rf.ba7d3dc9ba6b9c2352f856ea5f2938af.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-162_jpg.rf.ba7d3dc9ba6b9c2352f856ea5f2938af.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-162_jpg.rf.ba7d3dc9ba6b9c2352f856ea5f2938af.jpg\n",
      "Image 7001-139_jpg.rf.519f4029434cfd9b4e61292b7a6bed74.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-139_jpg.rf.519f4029434cfd9b4e61292b7a6bed74.jpg: point, Data: [[417  92]\n",
      " [297 629]\n",
      " [393  40]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7001-139_jpg.rf.519f4029434cfd9b4e61292b7a6bed74.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7001-139_jpg.rf.519f4029434cfd9b4e61292b7a6bed74.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-139_jpg.rf.519f4029434cfd9b4e61292b7a6bed74.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-139_jpg.rf.519f4029434cfd9b4e61292b7a6bed74.jpg\n",
      "Image 7003-18_jpg.rf.f654fe70de618f0b5b3c0ad1b46e3e79.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-18_jpg.rf.f654fe70de618f0b5b3c0ad1b46e3e79.jpg: box, Data: [[  0 188 636 564]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7003-18_jpg.rf.f654fe70de618f0b5b3c0ad1b46e3e79.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7003-18_jpg.rf.f654fe70de618f0b5b3c0ad1b46e3e79.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-18_jpg.rf.f654fe70de618f0b5b3c0ad1b46e3e79.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-18_jpg.rf.f654fe70de618f0b5b3c0ad1b46e3e79.jpg\n",
      "Image 7003-16_jpg.rf.8a8f6680174644066b5ed459e40d8ef9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-16_jpg.rf.8a8f6680174644066b5ed459e40d8ef9.jpg: box, Data: [[  1   0 637 635]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7003-16_jpg.rf.8a8f6680174644066b5ed459e40d8ef9.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7003-16_jpg.rf.8a8f6680174644066b5ed459e40d8ef9.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-16_jpg.rf.8a8f6680174644066b5ed459e40d8ef9.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-16_jpg.rf.8a8f6680174644066b5ed459e40d8ef9.jpg\n",
      "Image concrete-efflorescence-white-substance-oozing-steps-known-as-caused-salts-leeching-damp-can-be-sticky-first-259924023_webp.rf.d8521b43430773837879944ead17e4cc.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-efflorescence-white-substance-oozing-steps-known-as-caused-salts-leeching-damp-can-be-sticky-first-259924023_webp.rf.d8521b43430773837879944ead17e4cc.jpg: box, Data: [[  0 163 638 393]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image concrete-efflorescence-white-substance-oozing-steps-known-as-caused-salts-leeching-damp-can-be-sticky-first-259924023_webp.rf.d8521b43430773837879944ead17e4cc.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image concrete-efflorescence-white-substance-oozing-steps-known-as-caused-salts-leeching-damp-can-be-sticky-first-259924023_webp.rf.d8521b43430773837879944ead17e4cc.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_concrete-efflorescence-white-substance-oozing-steps-known-as-caused-salts-leeching-damp-can-be-sticky-first-259924023_webp.rf.d8521b43430773837879944ead17e4cc.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_concrete-efflorescence-white-substance-oozing-steps-known-as-caused-salts-leeching-damp-can-be-sticky-first-259924023_webp.rf.d8521b43430773837879944ead17e4cc.jpg\n",
      "Image 10045_jpg.rf.04d4188de1c967f33755e9329cc000a0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10045_jpg.rf.04d4188de1c967f33755e9329cc000a0.jpg: point, Data: [[397 482]\n",
      " [521 553]\n",
      " [336 352]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 10045_jpg.rf.04d4188de1c967f33755e9329cc000a0.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 10045_jpg.rf.04d4188de1c967f33755e9329cc000a0.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10045_jpg.rf.04d4188de1c967f33755e9329cc000a0.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10045_jpg.rf.04d4188de1c967f33755e9329cc000a0.jpg\n",
      "Image 71332b074718b5893f67f6fb2aa6f73b54ffc0e3_jpeg.rf.faed23e522e4dff600e627d1b992bb5b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 71332b074718b5893f67f6fb2aa6f73b54ffc0e3_jpeg.rf.faed23e522e4dff600e627d1b992bb5b.jpg: point, Data: [[353 276]\n",
      " [326 510]\n",
      " [228 489]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 71332b074718b5893f67f6fb2aa6f73b54ffc0e3_jpeg.rf.faed23e522e4dff600e627d1b992bb5b.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 71332b074718b5893f67f6fb2aa6f73b54ffc0e3_jpeg.rf.faed23e522e4dff600e627d1b992bb5b.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_71332b074718b5893f67f6fb2aa6f73b54ffc0e3_jpeg.rf.faed23e522e4dff600e627d1b992bb5b.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_71332b074718b5893f67f6fb2aa6f73b54ffc0e3_jpeg.rf.faed23e522e4dff600e627d1b992bb5b.jpg\n",
      "Image 7002-194_jpg.rf.cfded4786c5b0c09665b527d636136a8.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-194_jpg.rf.cfded4786c5b0c09665b527d636136a8.jpg: box, Data: [[ 31   0 632 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-194_jpg.rf.cfded4786c5b0c09665b527d636136a8.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-194_jpg.rf.cfded4786c5b0c09665b527d636136a8.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-194_jpg.rf.cfded4786c5b0c09665b527d636136a8.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-194_jpg.rf.cfded4786c5b0c09665b527d636136a8.jpg\n",
      "Image PO1_jpg.rf.a9389a85ee53bd69f5e42f96085cf242.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for PO1_jpg.rf.a9389a85ee53bd69f5e42f96085cf242.jpg: point, Data: [[431 534]\n",
      " [494 574]\n",
      " [456 513]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image PO1_jpg.rf.a9389a85ee53bd69f5e42f96085cf242.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image PO1_jpg.rf.a9389a85ee53bd69f5e42f96085cf242.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_PO1_jpg.rf.a9389a85ee53bd69f5e42f96085cf242.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_PO1_jpg.rf.a9389a85ee53bd69f5e42f96085cf242.jpg\n",
      "Image Trachyt_Herz_Jesu_Kirche_Aachen_jpg.rf.5952b07919513f7843dd666322c8ff17.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Trachyt_Herz_Jesu_Kirche_Aachen_jpg.rf.5952b07919513f7843dd666322c8ff17.jpg: point, Data: [[505 249]\n",
      " [ 73  88]\n",
      " [322 537]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image Trachyt_Herz_Jesu_Kirche_Aachen_jpg.rf.5952b07919513f7843dd666322c8ff17.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image Trachyt_Herz_Jesu_Kirche_Aachen_jpg.rf.5952b07919513f7843dd666322c8ff17.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_Trachyt_Herz_Jesu_Kirche_Aachen_jpg.rf.5952b07919513f7843dd666322c8ff17.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_Trachyt_Herz_Jesu_Kirche_Aachen_jpg.rf.5952b07919513f7843dd666322c8ff17.jpg\n",
      "Image images-3-_jpg.rf.378d487bfdbe9774997c218795c1b52e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-3-_jpg.rf.378d487bfdbe9774997c218795c1b52e.jpg: box, Data: [[ 69 262 422 559]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image images-3-_jpg.rf.378d487bfdbe9774997c218795c1b52e.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image images-3-_jpg.rf.378d487bfdbe9774997c218795c1b52e.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_images-3-_jpg.rf.378d487bfdbe9774997c218795c1b52e.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_images-3-_jpg.rf.378d487bfdbe9774997c218795c1b52e.jpg\n",
      "Image 10146_jpg.rf.1448fe577ad0ad4e3791683c77aca7ef.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10146_jpg.rf.1448fe577ad0ad4e3791683c77aca7ef.jpg: point, Data: [[292 278]\n",
      " [327  49]\n",
      " [334  81]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 10146_jpg.rf.1448fe577ad0ad4e3791683c77aca7ef.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 10146_jpg.rf.1448fe577ad0ad4e3791683c77aca7ef.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10146_jpg.rf.1448fe577ad0ad4e3791683c77aca7ef.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10146_jpg.rf.1448fe577ad0ad4e3791683c77aca7ef.jpg\n",
      "Image 7002-124_jpg.rf.1a3eb255fc816ff8d13f0693d6f9032a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-124_jpg.rf.1a3eb255fc816ff8d13f0693d6f9032a.jpg: box, Data: [[103   0 242 130]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-124_jpg.rf.1a3eb255fc816ff8d13f0693d6f9032a.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-124_jpg.rf.1a3eb255fc816ff8d13f0693d6f9032a.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-124_jpg.rf.1a3eb255fc816ff8d13f0693d6f9032a.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-124_jpg.rf.1a3eb255fc816ff8d13f0693d6f9032a.jpg\n",
      "Image 10062_jpg.rf.85bdbf0d7ca1d7aa6cdad897ed028261.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10062_jpg.rf.85bdbf0d7ca1d7aa6cdad897ed028261.jpg: box, Data: [[  0 168 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 10062_jpg.rf.85bdbf0d7ca1d7aa6cdad897ed028261.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 10062_jpg.rf.85bdbf0d7ca1d7aa6cdad897ed028261.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10062_jpg.rf.85bdbf0d7ca1d7aa6cdad897ed028261.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10062_jpg.rf.85bdbf0d7ca1d7aa6cdad897ed028261.jpg\n",
      "Image 10063_jpg.rf.e51ff14750880a4070ebd4dc8242fa8b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10063_jpg.rf.e51ff14750880a4070ebd4dc8242fa8b.jpg: box, Data: [[  1   0 639 492]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 10063_jpg.rf.e51ff14750880a4070ebd4dc8242fa8b.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 10063_jpg.rf.e51ff14750880a4070ebd4dc8242fa8b.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10063_jpg.rf.e51ff14750880a4070ebd4dc8242fa8b.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10063_jpg.rf.e51ff14750880a4070ebd4dc8242fa8b.jpg\n",
      "Image 10064_jpg.rf.03a7e7512851767ca801357dccd6a49b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10064_jpg.rf.03a7e7512851767ca801357dccd6a49b.jpg: box, Data: [[  0   0 639 287]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 10064_jpg.rf.03a7e7512851767ca801357dccd6a49b.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 10064_jpg.rf.03a7e7512851767ca801357dccd6a49b.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10064_jpg.rf.03a7e7512851767ca801357dccd6a49b.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10064_jpg.rf.03a7e7512851767ca801357dccd6a49b.jpg\n",
      "Image 7001-79_jpg.rf.2adf95ef63cda5606796a107c91f76b7.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-79_jpg.rf.2adf95ef63cda5606796a107c91f76b7.jpg: box, Data: [[  2 360 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7001-79_jpg.rf.2adf95ef63cda5606796a107c91f76b7.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7001-79_jpg.rf.2adf95ef63cda5606796a107c91f76b7.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-79_jpg.rf.2adf95ef63cda5606796a107c91f76b7.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-79_jpg.rf.2adf95ef63cda5606796a107c91f76b7.jpg\n",
      "Image 7002-33_jpg.rf.72590d9eddfb351afb7cb0cbcb99aa69.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-33_jpg.rf.72590d9eddfb351afb7cb0cbcb99aa69.jpg: box, Data: [[  1 471 370 590]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-33_jpg.rf.72590d9eddfb351afb7cb0cbcb99aa69.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-33_jpg.rf.72590d9eddfb351afb7cb0cbcb99aa69.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-33_jpg.rf.72590d9eddfb351afb7cb0cbcb99aa69.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-33_jpg.rf.72590d9eddfb351afb7cb0cbcb99aa69.jpg\n",
      "Image 7001-27_jpg.rf.35b211a2ba92caecdadfd1ebb15d9d7d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-27_jpg.rf.35b211a2ba92caecdadfd1ebb15d9d7d.jpg: box, Data: [[ 30 417 454 573]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7001-27_jpg.rf.35b211a2ba92caecdadfd1ebb15d9d7d.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7001-27_jpg.rf.35b211a2ba92caecdadfd1ebb15d9d7d.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-27_jpg.rf.35b211a2ba92caecdadfd1ebb15d9d7d.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-27_jpg.rf.35b211a2ba92caecdadfd1ebb15d9d7d.jpg\n",
      "Image images-6-_jpg.rf.6f05d42ec0df0d276fc1792e925a1eaf.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-6-_jpg.rf.6f05d42ec0df0d276fc1792e925a1eaf.jpg: point, Data: [[486 586]\n",
      " [512 572]\n",
      " [300 365]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image images-6-_jpg.rf.6f05d42ec0df0d276fc1792e925a1eaf.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image images-6-_jpg.rf.6f05d42ec0df0d276fc1792e925a1eaf.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_images-6-_jpg.rf.6f05d42ec0df0d276fc1792e925a1eaf.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_images-6-_jpg.rf.6f05d42ec0df0d276fc1792e925a1eaf.jpg\n",
      "Image 10031_jpg.rf.34a5f65dd73f98a3d7389f37da869166.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10031_jpg.rf.34a5f65dd73f98a3d7389f37da869166.jpg: point, Data: [[386 272]\n",
      " [318 157]\n",
      " [461  28]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 10031_jpg.rf.34a5f65dd73f98a3d7389f37da869166.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 10031_jpg.rf.34a5f65dd73f98a3d7389f37da869166.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10031_jpg.rf.34a5f65dd73f98a3d7389f37da869166.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10031_jpg.rf.34a5f65dd73f98a3d7389f37da869166.jpg\n",
      "Image 7002-81_jpg.rf.fb3f28f676754c0b197db91090d31267.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-81_jpg.rf.fb3f28f676754c0b197db91090d31267.jpg: point, Data: [[351  43]\n",
      " [359 230]\n",
      " [317  14]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-81_jpg.rf.fb3f28f676754c0b197db91090d31267.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-81_jpg.rf.fb3f28f676754c0b197db91090d31267.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-81_jpg.rf.fb3f28f676754c0b197db91090d31267.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-81_jpg.rf.fb3f28f676754c0b197db91090d31267.jpg\n",
      "Image 10066_jpg.rf.4f2c688fef6906e66119534c3af68586.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10066_jpg.rf.4f2c688fef6906e66119534c3af68586.jpg: box, Data: [[ 55 394 298 603]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 10066_jpg.rf.4f2c688fef6906e66119534c3af68586.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 10066_jpg.rf.4f2c688fef6906e66119534c3af68586.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10066_jpg.rf.4f2c688fef6906e66119534c3af68586.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10066_jpg.rf.4f2c688fef6906e66119534c3af68586.jpg\n",
      "Image anyone-know-what-these-craters-come-from-v0-u0dlp1u6el2b1_webp.rf.e1b25b99f77f4328a70f558d5cd083bf.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for anyone-know-what-these-craters-come-from-v0-u0dlp1u6el2b1_webp.rf.e1b25b99f77f4328a70f558d5cd083bf.jpg: point, Data: [[357 316]\n",
      " [326 286]\n",
      " [285 298]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image anyone-know-what-these-craters-come-from-v0-u0dlp1u6el2b1_webp.rf.e1b25b99f77f4328a70f558d5cd083bf.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image anyone-know-what-these-craters-come-from-v0-u0dlp1u6el2b1_webp.rf.e1b25b99f77f4328a70f558d5cd083bf.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_anyone-know-what-these-craters-come-from-v0-u0dlp1u6el2b1_webp.rf.e1b25b99f77f4328a70f558d5cd083bf.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_anyone-know-what-these-craters-come-from-v0-u0dlp1u6el2b1_webp.rf.e1b25b99f77f4328a70f558d5cd083bf.jpg\n",
      "Point Prompt Metrics: {'mean_iou': np.float64(0.374853165733325), 'mean_precision': np.float64(0.6875851666682564), 'mean_recall': np.float64(0.6466547028348307), 'mean_f1': np.float64(0.483358551191224)}\n",
      "Box Prompt Metrics: {'mean_iou': np.float64(0.3619462170243522), 'mean_precision': np.float64(0.6749389594179803), 'mean_recall': np.float64(0.6089694202885413), 'mean_f1': np.float64(0.4725019098118639)}\n",
      "Saving plot: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_point_metrics.png\n",
      "Successfully saved plot: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_point_metrics.png\n",
      "Saving plot: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_box_metrics.png\n",
      "Successfully saved plot: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_box_metrics.png\n",
      "Selected prompt strategy: point\n",
      "\n",
      "Testing data augmentation...\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Image 7001-56_jpg.rf.6b41799830de60d5c1185f81475f4912.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-56_jpg.rf.6b41799830de60d5c1185f81475f4912.jpg: box, Data: [[518 227 637 574]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7001-56_jpg.rf.6b41799830de60d5c1185f81475f4912.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7001-56_jpg.rf.6b41799830de60d5c1185f81475f4912.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-56_jpg.rf.6b41799830de60d5c1185f81475f4912.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-56_jpg.rf.6b41799830de60d5c1185f81475f4912.jpg\n",
      "Image 7003-230_jpg.rf.f0382c71bf78cd9eb9913ae435116136.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-230_jpg.rf.f0382c71bf78cd9eb9913ae435116136.jpg: point, Data: [[406 160]\n",
      " [427 136]\n",
      " [473 535]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7003-230_jpg.rf.f0382c71bf78cd9eb9913ae435116136.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7003-230_jpg.rf.f0382c71bf78cd9eb9913ae435116136.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-230_jpg.rf.f0382c71bf78cd9eb9913ae435116136.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-230_jpg.rf.f0382c71bf78cd9eb9913ae435116136.jpg\n",
      "Image 1-s2_0-S0950061807002760-gr1_jpg.rf.0ca3b381b6e25bf6bcf0337f5df2e485.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 1-s2_0-S0950061807002760-gr1_jpg.rf.0ca3b381b6e25bf6bcf0337f5df2e485.jpg: point, Data: [[460 183]\n",
      " [365 201]\n",
      " [249 372]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 1-s2_0-S0950061807002760-gr1_jpg.rf.0ca3b381b6e25bf6bcf0337f5df2e485.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 1-s2_0-S0950061807002760-gr1_jpg.rf.0ca3b381b6e25bf6bcf0337f5df2e485.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_1-s2_0-S0950061807002760-gr1_jpg.rf.0ca3b381b6e25bf6bcf0337f5df2e485.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_1-s2_0-S0950061807002760-gr1_jpg.rf.0ca3b381b6e25bf6bcf0337f5df2e485.jpg\n",
      "Image 7002-63_jpg.rf.343ef5463eee93e68ef3f9a522c20223.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-63_jpg.rf.343ef5463eee93e68ef3f9a522c20223.jpg: point, Data: [[419 502]\n",
      " [616 274]\n",
      " [431 591]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-63_jpg.rf.343ef5463eee93e68ef3f9a522c20223.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-63_jpg.rf.343ef5463eee93e68ef3f9a522c20223.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-63_jpg.rf.343ef5463eee93e68ef3f9a522c20223.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-63_jpg.rf.343ef5463eee93e68ef3f9a522c20223.jpg\n",
      "Image Pitted-concrete-driveway_webp.rf.1a295b83f95d88a3b98b5dd67ac4e67d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Pitted-concrete-driveway_webp.rf.1a295b83f95d88a3b98b5dd67ac4e67d.jpg: box, Data: [[280 200 401 472]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image Pitted-concrete-driveway_webp.rf.1a295b83f95d88a3b98b5dd67ac4e67d.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image Pitted-concrete-driveway_webp.rf.1a295b83f95d88a3b98b5dd67ac4e67d.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_Pitted-concrete-driveway_webp.rf.1a295b83f95d88a3b98b5dd67ac4e67d.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_Pitted-concrete-driveway_webp.rf.1a295b83f95d88a3b98b5dd67ac4e67d.jpg\n",
      "Image 7001-26_jpg.rf.28b5f963432adbc0264f760edf91a357.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-26_jpg.rf.28b5f963432adbc0264f760edf91a357.jpg: box, Data: [[  1 496 532 595]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7001-26_jpg.rf.28b5f963432adbc0264f760edf91a357.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7001-26_jpg.rf.28b5f963432adbc0264f760edf91a357.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-26_jpg.rf.28b5f963432adbc0264f760edf91a357.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-26_jpg.rf.28b5f963432adbc0264f760edf91a357.jpg\n",
      "Image 7002-46_jpg.rf.5f20e9fb37b8bc54ee5024bcd1a49775.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-46_jpg.rf.5f20e9fb37b8bc54ee5024bcd1a49775.jpg: point, Data: [[477 414]\n",
      " [540 301]\n",
      " [463 141]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-46_jpg.rf.5f20e9fb37b8bc54ee5024bcd1a49775.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-46_jpg.rf.5f20e9fb37b8bc54ee5024bcd1a49775.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-46_jpg.rf.5f20e9fb37b8bc54ee5024bcd1a49775.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-46_jpg.rf.5f20e9fb37b8bc54ee5024bcd1a49775.jpg\n",
      "Image old-cement-wall-texture-background_493325-1497_avif.rf.fb8de7c96d23acbf5af5737c3f5aa987.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for old-cement-wall-texture-background_493325-1497_avif.rf.fb8de7c96d23acbf5af5737c3f5aa987.jpg: point, Data: [[353 186]\n",
      " [424 121]\n",
      " [345 100]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image old-cement-wall-texture-background_493325-1497_avif.rf.fb8de7c96d23acbf5af5737c3f5aa987.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image old-cement-wall-texture-background_493325-1497_avif.rf.fb8de7c96d23acbf5af5737c3f5aa987.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_old-cement-wall-texture-background_493325-1497_avif.rf.fb8de7c96d23acbf5af5737c3f5aa987.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_old-cement-wall-texture-background_493325-1497_avif.rf.fb8de7c96d23acbf5af5737c3f5aa987.jpg\n",
      "Image what-are-these-holes-v0-tf1h89m5q5hb1_webp.rf.64d3f4180a20b7b6483ee967216e1057.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for what-are-these-holes-v0-tf1h89m5q5hb1_webp.rf.64d3f4180a20b7b6483ee967216e1057.jpg: point, Data: [[138 374]\n",
      " [290 351]\n",
      " [129 385]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image what-are-these-holes-v0-tf1h89m5q5hb1_webp.rf.64d3f4180a20b7b6483ee967216e1057.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image what-are-these-holes-v0-tf1h89m5q5hb1_webp.rf.64d3f4180a20b7b6483ee967216e1057.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_what-are-these-holes-v0-tf1h89m5q5hb1_webp.rf.64d3f4180a20b7b6483ee967216e1057.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_what-are-these-holes-v0-tf1h89m5q5hb1_webp.rf.64d3f4180a20b7b6483ee967216e1057.jpg\n",
      "Image 7003-162_jpg.rf.358aa953aaf0bcf271e111470692f795.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-162_jpg.rf.358aa953aaf0bcf271e111470692f795.jpg: box, Data: [[220 324 367 556]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7003-162_jpg.rf.358aa953aaf0bcf271e111470692f795.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7003-162_jpg.rf.358aa953aaf0bcf271e111470692f795.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-162_jpg.rf.358aa953aaf0bcf271e111470692f795.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-162_jpg.rf.358aa953aaf0bcf271e111470692f795.jpg\n",
      "Image 7002-152_jpg.rf.f3f8c554902b1e06650d38336872d8d9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-152_jpg.rf.f3f8c554902b1e06650d38336872d8d9.jpg: box, Data: [[500   0 588 160]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-152_jpg.rf.f3f8c554902b1e06650d38336872d8d9.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-152_jpg.rf.f3f8c554902b1e06650d38336872d8d9.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-152_jpg.rf.f3f8c554902b1e06650d38336872d8d9.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-152_jpg.rf.f3f8c554902b1e06650d38336872d8d9.jpg\n",
      "Image 7002-41_jpg.rf.75c10941e7deb4f072ed787bbfddbf94.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-41_jpg.rf.75c10941e7deb4f072ed787bbfddbf94.jpg: point, Data: [[314 391]\n",
      " [326 367]\n",
      " [361 381]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-41_jpg.rf.75c10941e7deb4f072ed787bbfddbf94.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-41_jpg.rf.75c10941e7deb4f072ed787bbfddbf94.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-41_jpg.rf.75c10941e7deb4f072ed787bbfddbf94.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-41_jpg.rf.75c10941e7deb4f072ed787bbfddbf94.jpg\n",
      "Image 7002-122_jpg.rf.a96be481b527b028ec8f290b10ec45e4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-122_jpg.rf.a96be481b527b028ec8f290b10ec45e4.jpg: point, Data: [[487 495]\n",
      " [474 443]\n",
      " [376 121]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-122_jpg.rf.a96be481b527b028ec8f290b10ec45e4.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-122_jpg.rf.a96be481b527b028ec8f290b10ec45e4.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-122_jpg.rf.a96be481b527b028ec8f290b10ec45e4.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-122_jpg.rf.a96be481b527b028ec8f290b10ec45e4.jpg\n",
      "Image 7003-15_jpg.rf.c4321c5bb9ee70d1168f318550a937f9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-15_jpg.rf.c4321c5bb9ee70d1168f318550a937f9.jpg: point, Data: [[346 233]\n",
      " [370 384]\n",
      " [615 209]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7003-15_jpg.rf.c4321c5bb9ee70d1168f318550a937f9.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7003-15_jpg.rf.c4321c5bb9ee70d1168f318550a937f9.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-15_jpg.rf.c4321c5bb9ee70d1168f318550a937f9.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-15_jpg.rf.c4321c5bb9ee70d1168f318550a937f9.jpg\n",
      "Image 10021_jpg.rf.27dc3e7ae6ef37ccc8cee732dc97cb7e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10021_jpg.rf.27dc3e7ae6ef37ccc8cee732dc97cb7e.jpg: box, Data: [[209   0 484 374]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 10021_jpg.rf.27dc3e7ae6ef37ccc8cee732dc97cb7e.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 10021_jpg.rf.27dc3e7ae6ef37ccc8cee732dc97cb7e.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10021_jpg.rf.27dc3e7ae6ef37ccc8cee732dc97cb7e.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10021_jpg.rf.27dc3e7ae6ef37ccc8cee732dc97cb7e.jpg\n",
      "Image 7002-162_jpg.rf.ba7d3dc9ba6b9c2352f856ea5f2938af.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-162_jpg.rf.ba7d3dc9ba6b9c2352f856ea5f2938af.jpg: point, Data: [[ 74 502]\n",
      " [ 39 400]\n",
      " [ 83 389]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-162_jpg.rf.ba7d3dc9ba6b9c2352f856ea5f2938af.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-162_jpg.rf.ba7d3dc9ba6b9c2352f856ea5f2938af.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-162_jpg.rf.ba7d3dc9ba6b9c2352f856ea5f2938af.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-162_jpg.rf.ba7d3dc9ba6b9c2352f856ea5f2938af.jpg\n",
      "Image 7001-139_jpg.rf.519f4029434cfd9b4e61292b7a6bed74.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-139_jpg.rf.519f4029434cfd9b4e61292b7a6bed74.jpg: box, Data: [[279   0 422 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7001-139_jpg.rf.519f4029434cfd9b4e61292b7a6bed74.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7001-139_jpg.rf.519f4029434cfd9b4e61292b7a6bed74.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-139_jpg.rf.519f4029434cfd9b4e61292b7a6bed74.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-139_jpg.rf.519f4029434cfd9b4e61292b7a6bed74.jpg\n",
      "Image 7003-18_jpg.rf.f654fe70de618f0b5b3c0ad1b46e3e79.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-18_jpg.rf.f654fe70de618f0b5b3c0ad1b46e3e79.jpg: box, Data: [[  0 188 636 564]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7003-18_jpg.rf.f654fe70de618f0b5b3c0ad1b46e3e79.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7003-18_jpg.rf.f654fe70de618f0b5b3c0ad1b46e3e79.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-18_jpg.rf.f654fe70de618f0b5b3c0ad1b46e3e79.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-18_jpg.rf.f654fe70de618f0b5b3c0ad1b46e3e79.jpg\n",
      "Image 7003-16_jpg.rf.8a8f6680174644066b5ed459e40d8ef9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-16_jpg.rf.8a8f6680174644066b5ed459e40d8ef9.jpg: point, Data: [[539 315]\n",
      " [375 626]\n",
      " [479 413]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7003-16_jpg.rf.8a8f6680174644066b5ed459e40d8ef9.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7003-16_jpg.rf.8a8f6680174644066b5ed459e40d8ef9.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-16_jpg.rf.8a8f6680174644066b5ed459e40d8ef9.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7003-16_jpg.rf.8a8f6680174644066b5ed459e40d8ef9.jpg\n",
      "Image concrete-efflorescence-white-substance-oozing-steps-known-as-caused-salts-leeching-damp-can-be-sticky-first-259924023_webp.rf.d8521b43430773837879944ead17e4cc.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-efflorescence-white-substance-oozing-steps-known-as-caused-salts-leeching-damp-can-be-sticky-first-259924023_webp.rf.d8521b43430773837879944ead17e4cc.jpg: point, Data: [[590 274]\n",
      " [ 29 274]\n",
      " [ 98 219]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image concrete-efflorescence-white-substance-oozing-steps-known-as-caused-salts-leeching-damp-can-be-sticky-first-259924023_webp.rf.d8521b43430773837879944ead17e4cc.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image concrete-efflorescence-white-substance-oozing-steps-known-as-caused-salts-leeching-damp-can-be-sticky-first-259924023_webp.rf.d8521b43430773837879944ead17e4cc.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_concrete-efflorescence-white-substance-oozing-steps-known-as-caused-salts-leeching-damp-can-be-sticky-first-259924023_webp.rf.d8521b43430773837879944ead17e4cc.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_concrete-efflorescence-white-substance-oozing-steps-known-as-caused-salts-leeching-damp-can-be-sticky-first-259924023_webp.rf.d8521b43430773837879944ead17e4cc.jpg\n",
      "Image 10045_jpg.rf.04d4188de1c967f33755e9329cc000a0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10045_jpg.rf.04d4188de1c967f33755e9329cc000a0.jpg: box, Data: [[  0 336 623 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 10045_jpg.rf.04d4188de1c967f33755e9329cc000a0.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 10045_jpg.rf.04d4188de1c967f33755e9329cc000a0.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10045_jpg.rf.04d4188de1c967f33755e9329cc000a0.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10045_jpg.rf.04d4188de1c967f33755e9329cc000a0.jpg\n",
      "Image 71332b074718b5893f67f6fb2aa6f73b54ffc0e3_jpeg.rf.faed23e522e4dff600e627d1b992bb5b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 71332b074718b5893f67f6fb2aa6f73b54ffc0e3_jpeg.rf.faed23e522e4dff600e627d1b992bb5b.jpg: box, Data: [[173 112 461 518]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 71332b074718b5893f67f6fb2aa6f73b54ffc0e3_jpeg.rf.faed23e522e4dff600e627d1b992bb5b.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 71332b074718b5893f67f6fb2aa6f73b54ffc0e3_jpeg.rf.faed23e522e4dff600e627d1b992bb5b.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_71332b074718b5893f67f6fb2aa6f73b54ffc0e3_jpeg.rf.faed23e522e4dff600e627d1b992bb5b.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_71332b074718b5893f67f6fb2aa6f73b54ffc0e3_jpeg.rf.faed23e522e4dff600e627d1b992bb5b.jpg\n",
      "Image 7002-194_jpg.rf.cfded4786c5b0c09665b527d636136a8.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-194_jpg.rf.cfded4786c5b0c09665b527d636136a8.jpg: point, Data: [[101   0]\n",
      " [ 44 100]\n",
      " [101  68]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-194_jpg.rf.cfded4786c5b0c09665b527d636136a8.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-194_jpg.rf.cfded4786c5b0c09665b527d636136a8.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-194_jpg.rf.cfded4786c5b0c09665b527d636136a8.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-194_jpg.rf.cfded4786c5b0c09665b527d636136a8.jpg\n",
      "Image PO1_jpg.rf.a9389a85ee53bd69f5e42f96085cf242.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for PO1_jpg.rf.a9389a85ee53bd69f5e42f96085cf242.jpg: box, Data: [[378 492 527 593]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image PO1_jpg.rf.a9389a85ee53bd69f5e42f96085cf242.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image PO1_jpg.rf.a9389a85ee53bd69f5e42f96085cf242.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_PO1_jpg.rf.a9389a85ee53bd69f5e42f96085cf242.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_PO1_jpg.rf.a9389a85ee53bd69f5e42f96085cf242.jpg\n",
      "Image Trachyt_Herz_Jesu_Kirche_Aachen_jpg.rf.5952b07919513f7843dd666322c8ff17.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Trachyt_Herz_Jesu_Kirche_Aachen_jpg.rf.5952b07919513f7843dd666322c8ff17.jpg: box, Data: [[ 64   7 549 559]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image Trachyt_Herz_Jesu_Kirche_Aachen_jpg.rf.5952b07919513f7843dd666322c8ff17.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image Trachyt_Herz_Jesu_Kirche_Aachen_jpg.rf.5952b07919513f7843dd666322c8ff17.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_Trachyt_Herz_Jesu_Kirche_Aachen_jpg.rf.5952b07919513f7843dd666322c8ff17.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_Trachyt_Herz_Jesu_Kirche_Aachen_jpg.rf.5952b07919513f7843dd666322c8ff17.jpg\n",
      "Image images-3-_jpg.rf.378d487bfdbe9774997c218795c1b52e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-3-_jpg.rf.378d487bfdbe9774997c218795c1b52e.jpg: point, Data: [[126 544]\n",
      " [ 86 511]\n",
      " [361 315]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image images-3-_jpg.rf.378d487bfdbe9774997c218795c1b52e.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image images-3-_jpg.rf.378d487bfdbe9774997c218795c1b52e.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_images-3-_jpg.rf.378d487bfdbe9774997c218795c1b52e.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_images-3-_jpg.rf.378d487bfdbe9774997c218795c1b52e.jpg\n",
      "Image 10146_jpg.rf.1448fe577ad0ad4e3791683c77aca7ef.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10146_jpg.rf.1448fe577ad0ad4e3791683c77aca7ef.jpg: point, Data: [[483  65]\n",
      " [433  17]\n",
      " [313  18]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 10146_jpg.rf.1448fe577ad0ad4e3791683c77aca7ef.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 10146_jpg.rf.1448fe577ad0ad4e3791683c77aca7ef.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10146_jpg.rf.1448fe577ad0ad4e3791683c77aca7ef.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10146_jpg.rf.1448fe577ad0ad4e3791683c77aca7ef.jpg\n",
      "Image 7002-124_jpg.rf.1a3eb255fc816ff8d13f0693d6f9032a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-124_jpg.rf.1a3eb255fc816ff8d13f0693d6f9032a.jpg: box, Data: [[103   0 242 130]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-124_jpg.rf.1a3eb255fc816ff8d13f0693d6f9032a.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-124_jpg.rf.1a3eb255fc816ff8d13f0693d6f9032a.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-124_jpg.rf.1a3eb255fc816ff8d13f0693d6f9032a.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-124_jpg.rf.1a3eb255fc816ff8d13f0693d6f9032a.jpg\n",
      "Image 10062_jpg.rf.85bdbf0d7ca1d7aa6cdad897ed028261.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10062_jpg.rf.85bdbf0d7ca1d7aa6cdad897ed028261.jpg: point, Data: [[530 392]\n",
      " [116 356]\n",
      " [386 364]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 10062_jpg.rf.85bdbf0d7ca1d7aa6cdad897ed028261.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 10062_jpg.rf.85bdbf0d7ca1d7aa6cdad897ed028261.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10062_jpg.rf.85bdbf0d7ca1d7aa6cdad897ed028261.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10062_jpg.rf.85bdbf0d7ca1d7aa6cdad897ed028261.jpg\n",
      "Image 10063_jpg.rf.e51ff14750880a4070ebd4dc8242fa8b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10063_jpg.rf.e51ff14750880a4070ebd4dc8242fa8b.jpg: box, Data: [[  1   0 639 492]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 10063_jpg.rf.e51ff14750880a4070ebd4dc8242fa8b.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 10063_jpg.rf.e51ff14750880a4070ebd4dc8242fa8b.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10063_jpg.rf.e51ff14750880a4070ebd4dc8242fa8b.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10063_jpg.rf.e51ff14750880a4070ebd4dc8242fa8b.jpg\n",
      "Image 10064_jpg.rf.03a7e7512851767ca801357dccd6a49b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10064_jpg.rf.03a7e7512851767ca801357dccd6a49b.jpg: box, Data: [[  0   0 639 287]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 10064_jpg.rf.03a7e7512851767ca801357dccd6a49b.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 10064_jpg.rf.03a7e7512851767ca801357dccd6a49b.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10064_jpg.rf.03a7e7512851767ca801357dccd6a49b.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10064_jpg.rf.03a7e7512851767ca801357dccd6a49b.jpg\n",
      "Image 7001-79_jpg.rf.2adf95ef63cda5606796a107c91f76b7.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-79_jpg.rf.2adf95ef63cda5606796a107c91f76b7.jpg: point, Data: [[111 629]\n",
      " [615 398]\n",
      " [ 62 455]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7001-79_jpg.rf.2adf95ef63cda5606796a107c91f76b7.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7001-79_jpg.rf.2adf95ef63cda5606796a107c91f76b7.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-79_jpg.rf.2adf95ef63cda5606796a107c91f76b7.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-79_jpg.rf.2adf95ef63cda5606796a107c91f76b7.jpg\n",
      "Image 7002-33_jpg.rf.72590d9eddfb351afb7cb0cbcb99aa69.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-33_jpg.rf.72590d9eddfb351afb7cb0cbcb99aa69.jpg: point, Data: [[ 14 483]\n",
      " [221 590]\n",
      " [245 512]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-33_jpg.rf.72590d9eddfb351afb7cb0cbcb99aa69.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-33_jpg.rf.72590d9eddfb351afb7cb0cbcb99aa69.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-33_jpg.rf.72590d9eddfb351afb7cb0cbcb99aa69.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-33_jpg.rf.72590d9eddfb351afb7cb0cbcb99aa69.jpg\n",
      "Image 7001-27_jpg.rf.35b211a2ba92caecdadfd1ebb15d9d7d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-27_jpg.rf.35b211a2ba92caecdadfd1ebb15d9d7d.jpg: point, Data: [[184 513]\n",
      " [404 449]\n",
      " [157 515]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7001-27_jpg.rf.35b211a2ba92caecdadfd1ebb15d9d7d.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7001-27_jpg.rf.35b211a2ba92caecdadfd1ebb15d9d7d.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-27_jpg.rf.35b211a2ba92caecdadfd1ebb15d9d7d.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7001-27_jpg.rf.35b211a2ba92caecdadfd1ebb15d9d7d.jpg\n",
      "Image images-6-_jpg.rf.6f05d42ec0df0d276fc1792e925a1eaf.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-6-_jpg.rf.6f05d42ec0df0d276fc1792e925a1eaf.jpg: box, Data: [[ 19  54 592 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image images-6-_jpg.rf.6f05d42ec0df0d276fc1792e925a1eaf.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image images-6-_jpg.rf.6f05d42ec0df0d276fc1792e925a1eaf.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_images-6-_jpg.rf.6f05d42ec0df0d276fc1792e925a1eaf.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_images-6-_jpg.rf.6f05d42ec0df0d276fc1792e925a1eaf.jpg\n",
      "Image 10031_jpg.rf.34a5f65dd73f98a3d7389f37da869166.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10031_jpg.rf.34a5f65dd73f98a3d7389f37da869166.jpg: box, Data: [[140   0 522 622]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 10031_jpg.rf.34a5f65dd73f98a3d7389f37da869166.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 10031_jpg.rf.34a5f65dd73f98a3d7389f37da869166.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10031_jpg.rf.34a5f65dd73f98a3d7389f37da869166.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10031_jpg.rf.34a5f65dd73f98a3d7389f37da869166.jpg\n",
      "Image 7002-81_jpg.rf.fb3f28f676754c0b197db91090d31267.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-81_jpg.rf.fb3f28f676754c0b197db91090d31267.jpg: point, Data: [[326 398]\n",
      " [365 385]\n",
      " [368 356]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-81_jpg.rf.fb3f28f676754c0b197db91090d31267.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-81_jpg.rf.fb3f28f676754c0b197db91090d31267.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-81_jpg.rf.fb3f28f676754c0b197db91090d31267.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_7002-81_jpg.rf.fb3f28f676754c0b197db91090d31267.jpg\n",
      "Image 10066_jpg.rf.4f2c688fef6906e66119534c3af68586.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10066_jpg.rf.4f2c688fef6906e66119534c3af68586.jpg: point, Data: [[280 452]\n",
      " [256 443]\n",
      " [241 559]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 10066_jpg.rf.4f2c688fef6906e66119534c3af68586.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 10066_jpg.rf.4f2c688fef6906e66119534c3af68586.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10066_jpg.rf.4f2c688fef6906e66119534c3af68586.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_10066_jpg.rf.4f2c688fef6906e66119534c3af68586.jpg\n",
      "Image anyone-know-what-these-craters-come-from-v0-u0dlp1u6el2b1_webp.rf.e1b25b99f77f4328a70f558d5cd083bf.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for anyone-know-what-these-craters-come-from-v0-u0dlp1u6el2b1_webp.rf.e1b25b99f77f4328a70f558d5cd083bf.jpg: box, Data: [[218 284 393 394]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image anyone-know-what-these-craters-come-from-v0-u0dlp1u6el2b1_webp.rf.e1b25b99f77f4328a70f558d5cd083bf.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image anyone-know-what-these-craters-come-from-v0-u0dlp1u6el2b1_webp.rf.e1b25b99f77f4328a70f558d5cd083bf.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_anyone-know-what-these-craters-come-from-v0-u0dlp1u6el2b1_webp.rf.e1b25b99f77f4328a70f558d5cd083bf.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_predictions\\pred_anyone-know-what-these-craters-come-from-v0-u0dlp1u6el2b1_webp.rf.e1b25b99f77f4328a70f558d5cd083bf.jpg\n",
      "Augmented Validation Metrics: {'mean_iou': np.float64(0.36081053632236443), 'mean_precision': np.float64(0.5089846623825302), 'mean_recall': np.float64(0.7711286795955749), 'mean_f1': np.float64(0.461478198751847)}\n",
      "Saving plot: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_aug_metrics.png\n",
      "Successfully saved plot: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\val_aug_metrics.png\n",
      "Using data augmentation: False\n",
      "\n",
      "Training SAM2...\n",
      "Model attributes: ['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_apply_non_overlapping_constraints', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_build_sam_heads', '_call_impl', '_compiled_call_impl', '_encode_memory_in_output', '_encode_new_memory', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_forward_sam_heads', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_prepare_backbone_features', '_prepare_memory_conditioned_features', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_track_step', '_use_mask_as_output', '_use_multimask', '_version', '_wrapped_call_impl', 'add_module', 'add_tpos_enc_to_obj_ptrs', 'apply', 'backbone_stride', 'bfloat16', 'binarize_mask_from_pts_for_mem_enc', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'device', 'directly_add_no_mem_embed', 'double', 'dump_patches', 'eval', 'extra_repr', 'fixed_no_obj_ptr', 'float', 'forward', 'forward_image', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'hidden_dim', 'image_encoder', 'image_size', 'iou_prediction_use_sigmoid', 'ipu', 'load_state_dict', 'mask_downsample', 'maskmem_tpos_enc', 'max_cond_frames_in_attn', 'max_obj_ptrs_in_encoder', 'mem_dim', 'memory_attention', 'memory_encoder', 'memory_temporal_stride_for_eval', 'modules', 'mtia', 'multimask_max_pt_num', 'multimask_min_pt_num', 'multimask_output_for_tracking', 'multimask_output_in_sam', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'no_mem_embed', 'no_mem_pos_enc', 'no_obj_embed_spatial', 'no_obj_ptr', 'non_overlap_masks_for_mem_enc', 'num_feature_levels', 'num_maskmem', 'obj_ptr_proj', 'obj_ptr_tpos_proj', 'only_obj_ptrs_in_the_past_for_eval', 'parameters', 'pred_obj_scores', 'pred_obj_scores_mlp', 'proj_tpos_enc_in_obj_ptrs', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'sam_image_embedding_size', 'sam_mask_decoder', 'sam_mask_decoder_extra_args', 'sam_prompt_embed_dim', 'sam_prompt_encoder', 'set_extra_state', 'set_submodule', 'share_memory', 'sigmoid_bias_for_mem_enc', 'sigmoid_scale_for_mem_enc', 'soft_no_obj_ptr', 'state_dict', 'to', 'to_empty', 'track_step', 'train', 'training', 'type', 'use_high_res_features_in_sam', 'use_mask_input_as_output_without_sam', 'use_mlp_for_obj_ptr_proj', 'use_multimask_token_for_obj_ptr', 'use_obj_ptrs_in_encoder', 'use_signed_tpos_enc_to_obj_ptrs', 'xpu', 'zero_grad']\n",
      "Model has image_encoder: True\n",
      "Image 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg: point, Data: [[551 164]\n",
      " [547 107]\n",
      " [569 223]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[551 164]\n",
      " [547 107]\n",
      " [569 223]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6902\n",
      "Image concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg: box, Data: [[336 345 373 409]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[336 345 373 409]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6920\n",
      "Image 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg: point, Data: [[ 44 463]\n",
      " [358 396]\n",
      " [ 33 614]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 44 463]\n",
      " [358 396]\n",
      " [ 33 614]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7424\n",
      "Image 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg: box, Data: [[  0   0 507 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 507 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9251\n",
      "Image Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg: box, Data: [[ 62   0 638 558]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 62   0 638 558]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6567\n",
      "Image Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg: point, Data: [[  6 348]\n",
      " [280 139]\n",
      " [104 297]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[  6 348]\n",
      " [280 139]\n",
      " [104 297]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7028\n",
      "Image 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg: point, Data: [[ 78 452]\n",
      " [490 206]\n",
      " [422 365]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 78 452]\n",
      " [490 206]\n",
      " [422 365]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6142\n",
      "Image 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg: point, Data: [[119 314]\n",
      " [119 344]\n",
      " [126 366]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[119 314]\n",
      " [119 344]\n",
      " [126 366]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7262\n",
      "Image 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg: box, Data: [[ 12   0 498 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 12   0 498 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7543\n",
      "Image 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg: box, Data: [[ 51   0 612 395]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 51   0 612 395]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6981\n",
      "Image Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg: box, Data: [[198   0 638 607]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[198   0 638 607]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6901\n",
      "Image 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg: box, Data: [[ 35  93 301 576]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 35  93 301 576]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6934\n",
      "Image 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg: point, Data: [[387 481]\n",
      " [386 426]\n",
      " [ 94 354]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[387 481]\n",
      " [386 426]\n",
      " [ 94 354]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2879\n",
      "Image patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg: box, Data: [[268 209 406 474]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[268 209 406 474]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6824\n",
      "Image 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg: point, Data: [[234 138]\n",
      " [548 135]\n",
      " [581 447]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[234 138]\n",
      " [548 135]\n",
      " [581 447]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1047\n",
      "Image 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg: point, Data: [[105  75]\n",
      " [435 244]\n",
      " [ 33  57]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[105  75]\n",
      " [435 244]\n",
      " [ 33  57]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2102\n",
      "Image 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg: box, Data: [[  0  10 629 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0  10 629 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9787\n",
      "Image images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg: point, Data: [[ 15 263]\n",
      " [585 549]\n",
      " [523 540]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 15 263]\n",
      " [585 549]\n",
      " [523 540]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6834\n",
      "Image 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg: point, Data: [[ 48  18]\n",
      " [ 65  32]\n",
      " [375 185]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 48  18]\n",
      " [ 65  32]\n",
      " [375 185]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6789\n",
      "Image 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg: box, Data: [[ 29   1 147 143]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 29   1 147 143]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6910\n",
      "Image 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg: point, Data: [[177 414]\n",
      " [453 189]\n",
      " [408 258]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[177 414]\n",
      " [453 189]\n",
      " [408 258]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2128\n",
      "Image Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg: point, Data: [[ 81 335]\n",
      " [139 365]\n",
      " [318 526]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 81 335]\n",
      " [139 365]\n",
      " [318 526]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6754\n",
      "Image 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg: point, Data: [[463  10]\n",
      " [440 120]\n",
      " [ 75  75]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[463  10]\n",
      " [440 120]\n",
      " [ 75  75]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7828\n",
      "Image 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg: box, Data: [[278 218 428 414]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[278 218 428 414]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6804\n",
      "Image istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg: box, Data: [[130 110 494 452]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[130 110 494 452]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6531\n",
      "Image crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg: point, Data: [[507 190]\n",
      " [412 294]\n",
      " [326 350]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[507 190]\n",
      " [412 294]\n",
      " [326 350]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6808\n",
      "Image 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg: box, Data: [[ 76   0 257 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 76   0 257 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6896\n",
      "Image 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg: point, Data: [[608 319]\n",
      " [589 444]\n",
      " [616 470]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[608 319]\n",
      " [589 444]\n",
      " [616 470]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6832\n",
      "Image 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg: box, Data: [[ 33   0 615 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 33   0 615 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6859\n",
      "Image 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg: box, Data: [[  0  98 637 524]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0  98 637 524]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6829\n",
      "Image 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg: box, Data: [[  0 173 636 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0 173 636 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6815\n",
      "Image 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg: box, Data: [[138  88 493 621]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[138  88 493 621]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6937\n",
      "Image stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg: point, Data: [[270 249]\n",
      " [388 145]\n",
      " [489 322]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[270 249]\n",
      " [388 145]\n",
      " [489 322]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5889\n",
      "Image 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg: box, Data: [[  5 171 318 605]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  5 171 318 605]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6923\n",
      "Image card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg: box, Data: [[297  69 487 475]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[297  69 487 475]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6564\n",
      "Image 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg: point, Data: [[307 593]\n",
      " [278 300]\n",
      " [267 403]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[307 593]\n",
      " [278 300]\n",
      " [267 403]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6892\n",
      "Image 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg: box, Data: [[127 101 260 134]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[127 101 260 134]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6922\n",
      "Image 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg: box, Data: [[ 56 335  89 421]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 56 335  89 421]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6922\n",
      "Image 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg: box, Data: [[513   0 637 274]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[513   0 637 274]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6906\n",
      "Image efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg: box, Data: [[  0   0 638 520]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 638 520]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5794\n",
      "Image 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg: point, Data: [[192 359]\n",
      " [203 420]\n",
      " [194 291]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[192 359]\n",
      " [203 420]\n",
      " [194 291]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6915\n",
      "Image 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg: point, Data: [[ 82 628]\n",
      " [132 525]\n",
      " [ 88 578]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 82 628]\n",
      " [132 525]\n",
      " [ 88 578]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6857\n",
      "Image 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg: box, Data: [[288   0 637 495]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[288   0 637 495]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6927\n",
      "Image 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg: box, Data: [[  0   0 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 639 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7416\n",
      "Image 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg: point, Data: [[ 18 388]\n",
      " [407 317]\n",
      " [342 289]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 18 388]\n",
      " [407 317]\n",
      " [342 289]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9017\n",
      "Image old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg: box, Data: [[126 118 231 630]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[126 118 231 630]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6709\n",
      "Image 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg: box, Data: [[ 63  32 639 467]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 63  32 639 467]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6953\n",
      "Image 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg: point, Data: [[452 171]\n",
      " [480 191]\n",
      " [471 182]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[452 171]\n",
      " [480 191]\n",
      " [471 182]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6941\n",
      "Image images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg: point, Data: [[137  34]\n",
      " [284 379]\n",
      " [122 137]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[137  34]\n",
      " [284 379]\n",
      " [122 137]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6415\n",
      "Image 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg: point, Data: [[613 545]\n",
      " [594 455]\n",
      " [611 567]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[613 545]\n",
      " [594 455]\n",
      " [611 567]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6119\n",
      "Image 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg: point, Data: [[ 20 294]\n",
      " [205 372]\n",
      " [320 463]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 20 294]\n",
      " [205 372]\n",
      " [320 463]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6792\n",
      "Image 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg: point, Data: [[156 176]\n",
      " [393  81]\n",
      " [508 110]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[156 176]\n",
      " [393  81]\n",
      " [508 110]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9140\n",
      "Image 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg: box, Data: [[ 11  46 232 248]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 11  46 232 248]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6904\n",
      "Image weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg: point, Data: [[307 419]\n",
      " [340 555]\n",
      " [326 593]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[307 419]\n",
      " [340 555]\n",
      " [326 593]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6921\n",
      "Image 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg: box, Data: [[123 180 639 335]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[123 180 639 335]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6348\n",
      "Image 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg: point, Data: [[197 113]\n",
      " [205  44]\n",
      " [182  56]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[197 113]\n",
      " [205  44]\n",
      " [182  56]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6988\n",
      "Image 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg: point, Data: [[452 170]\n",
      " [448  17]\n",
      " [410 200]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[452 170]\n",
      " [448  17]\n",
      " [410 200]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2580\n",
      "Image 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg: box, Data: [[  4 415 338 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  4 415 338 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6889\n",
      "Image 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg: box, Data: [[ 66 156 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 66 156 639 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8246\n",
      "Image images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg: box, Data: [[168 157 489 372]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[168 157 489 372]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6801\n",
      "Image 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg: box, Data: [[ 65   0 293 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 65   0 293 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6845\n",
      "Image 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg: box, Data: [[ 99  22 139 139]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 99  22 139 139]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6916\n",
      "Image Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg: point, Data: [[330 256]\n",
      " [281 371]\n",
      " [372 364]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[330 256]\n",
      " [281 371]\n",
      " [372 364]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6687\n",
      "Image 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg: box, Data: [[178 146 428 462]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[178 146 428 462]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6553\n",
      "Image 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg: point, Data: [[584 112]\n",
      " [534 188]\n",
      " [586 113]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[584 112]\n",
      " [534 188]\n",
      " [586 113]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6931\n",
      "Image IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg: box, Data: [[  0   0 631 345]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 631 345]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6472\n",
      "Image images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg: point, Data: [[426 425]\n",
      " [451 527]\n",
      " [347 359]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[426 425]\n",
      " [451 527]\n",
      " [347 359]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6620\n",
      "Image 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg: point, Data: [[259 611]\n",
      " [180 549]\n",
      " [126 614]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[259 611]\n",
      " [180 549]\n",
      " [126 614]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6721\n",
      "Image lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg: box, Data: [[242  65 439 419]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[242  65 439 419]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6908\n",
      "Image 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg: point, Data: [[121 227]\n",
      " [123 324]\n",
      " [105   2]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[121 227]\n",
      " [123 324]\n",
      " [105   2]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6847\n",
      "Image 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg: point, Data: [[168 206]\n",
      " [ 45 326]\n",
      " [470 407]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[168 206]\n",
      " [ 45 326]\n",
      " [470 407]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9777\n",
      "Image 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg: box, Data: [[146   0 179  68]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[146   0 179  68]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6922\n",
      "Image Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg: point, Data: [[262 322]\n",
      " [287 322]\n",
      " [279 408]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[262 322]\n",
      " [287 322]\n",
      " [279 408]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6723\n",
      "Image 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg: point, Data: [[333 282]\n",
      " [222 504]\n",
      " [381 576]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[333 282]\n",
      " [222 504]\n",
      " [381 576]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6558\n",
      "Image 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg: box, Data: [[579 438 607 539]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[579 438 607 539]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6920\n",
      "Image istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg: box, Data: [[241 201 457 434]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[241 201 457 434]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6626\n",
      "Image Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg: box, Data: [[469 484 535 555]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[469 484 535 555]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6902\n",
      "Image old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg: point, Data: [[207 348]\n",
      " [241 336]\n",
      " [207 303]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[207 348]\n",
      " [241 336]\n",
      " [207 303]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6961\n",
      "Image 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg: point, Data: [[454 591]\n",
      " [357 225]\n",
      " [546 507]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[454 591]\n",
      " [357 225]\n",
      " [546 507]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8036\n",
      "Image 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg: point, Data: [[428 168]\n",
      " [544 125]\n",
      " [128  24]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[428 168]\n",
      " [544 125]\n",
      " [128  24]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.4539\n",
      "Image concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg: point, Data: [[327 166]\n",
      " [322 176]\n",
      " [294 411]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[327 166]\n",
      " [322 176]\n",
      " [294 411]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6902\n",
      "Image 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg: box, Data: [[ 30  42 247 539]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 30  42 247 539]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6897\n",
      "Image 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg: point, Data: [[470 421]\n",
      " [414 142]\n",
      " [339 281]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[470 421]\n",
      " [414 142]\n",
      " [339 281]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9527\n",
      "Image what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg: point, Data: [[364 128]\n",
      " [440  91]\n",
      " [227 276]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[364 128]\n",
      " [440  91]\n",
      " [227 276]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0135\n",
      "Image 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg: box, Data: [[ 92 490 190 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 92 490 190 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6910\n",
      "Image 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg: box, Data: [[  1 149 600 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  1 149 600 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9886\n",
      "Image 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg: box, Data: [[ 13   9 249 249]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 13   9 249 249]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6909\n",
      "Image 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg: box, Data: [[  5  89 591 603]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  5  89 591 603]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6932\n",
      "Image 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg: box, Data: [[ 22 417 104 521]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 22 417 104 521]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6905\n",
      "Image 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg: point, Data: [[194 622]\n",
      " [431 567]\n",
      " [ 61 478]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[194 622]\n",
      " [431 567]\n",
      " [ 61 478]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7857\n",
      "Image images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg: point, Data: [[220 347]\n",
      " [178 176]\n",
      " [178 105]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[220 347]\n",
      " [178 176]\n",
      " [178 105]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6365\n",
      "Image 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg: point, Data: [[411 598]\n",
      " [377 457]\n",
      " [364 548]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[411 598]\n",
      " [377 457]\n",
      " [364 548]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6800\n",
      "Image d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg: box, Data: [[152   1 380 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[152   1 380 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6867\n",
      "Image PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg: box, Data: [[ 85 127 573 576]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 85 127 573 576]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5823\n",
      "Image 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg: point, Data: [[509 539]\n",
      " [519 592]\n",
      " [473 446]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[509 539]\n",
      " [519 592]\n",
      " [473 446]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6940\n",
      "Image 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg: box, Data: [[301 424 432 572]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[301 424 432 572]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6914\n",
      "Image concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg: box, Data: [[140 120 495 503]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[140 120 495 503]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6378\n",
      "Image How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg: box, Data: [[177 129 345 494]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[177 129 345 494]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6565\n",
      "Image 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg: box, Data: [[ 21   0 543 636]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 21   0 543 636]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6856\n",
      "Image 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg: box, Data: [[405   0 518 480]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[405   0 518 480]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6924\n",
      "Image 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg: box, Data: [[233   1 630 302]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[233   1 630 302]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6913\n",
      "Image 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg: box, Data: [[282  68 312 173]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[282  68 312 173]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6918\n",
      "Image 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg: point, Data: [[510 401]\n",
      " [510 392]\n",
      " [479 415]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[510 401]\n",
      " [510 392]\n",
      " [479 415]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6871\n",
      "Image Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg: box, Data: [[  0   0 638 638]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 638 638]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7462\n",
      "Image 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg: box, Data: [[ 48   0 176 178]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 48   0 176 178]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6918\n",
      "Image CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg: box, Data: [[ 76  25 310 448]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 76  25 310 448]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6410\n",
      "Image 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg: point, Data: [[198 105]\n",
      " [ 12 360]\n",
      " [201 224]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[198 105]\n",
      " [ 12 360]\n",
      " [201 224]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7013\n",
      "Image 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg: point, Data: [[314  62]\n",
      " [606 590]\n",
      " [329  15]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[314  62]\n",
      " [606 590]\n",
      " [329  15]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6916\n",
      "Image 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg: point, Data: [[447 469]\n",
      " [249 535]\n",
      " [253 431]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[447 469]\n",
      " [249 535]\n",
      " [253 431]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6213\n",
      "Image images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg: box, Data: [[ 65   0 513 417]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 65   0 513 417]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6888\n",
      "Image 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg: point, Data: [[617 310]\n",
      " [362 276]\n",
      " [386 371]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[617 310]\n",
      " [362 276]\n",
      " [386 371]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7019\n",
      "Image 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg: point, Data: [[209  37]\n",
      " [180 125]\n",
      " [227 254]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[209  37]\n",
      " [180 125]\n",
      " [227 254]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0950\n",
      "Image 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg: point, Data: [[164  23]\n",
      " [400 430]\n",
      " [335 540]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[164  23]\n",
      " [400 430]\n",
      " [335 540]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5908\n",
      "Image 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg: point, Data: [[ 96 170]\n",
      " [ 70 148]\n",
      " [ 68 487]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 96 170]\n",
      " [ 70 148]\n",
      " [ 68 487]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2222\n",
      "Image 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg: point, Data: [[132 122]\n",
      " [111 156]\n",
      " [123 158]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[132 122]\n",
      " [111 156]\n",
      " [123 158]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6903\n",
      "Image 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg: box, Data: [[167 345 192 392]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[167 345 192 392]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6927\n",
      "Image 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg: box, Data: [[  6  10 632 510]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  6  10 632 510]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6970\n",
      "Image Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg: point, Data: [[534 135]\n",
      " [529 341]\n",
      " [544 374]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[534 135]\n",
      " [529 341]\n",
      " [544 374]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6131\n",
      "Image 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg: point, Data: [[399 508]\n",
      " [348 536]\n",
      " [444 438]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[399 508]\n",
      " [348 536]\n",
      " [444 438]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8014\n",
      "Image dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg: box, Data: [[ 80   0 637 609]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 80   0 637 609]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8884\n",
      "Image 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg: box, Data: [[  1   0 639 598]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  1   0 639 598]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9833\n",
      "Image 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg: box, Data: [[131 303 197 534]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[131 303 197 534]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6906\n",
      "Image 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg: box, Data: [[271  68 622 593]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[271  68 622 593]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6881\n",
      "Image 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg: box, Data: [[283   0 637 229]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[283   0 637 229]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6877\n",
      "Image 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg: box, Data: [[  4 563 104 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  4 563 104 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6922\n",
      "Image 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg: box, Data: [[ 63 349 461 547]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 63 349 461 547]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6904\n",
      "Image 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg: point, Data: [[141 540]\n",
      " [107 171]\n",
      " [286 581]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[141 540]\n",
      " [107 171]\n",
      " [286 581]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9896\n",
      "Image 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg: point, Data: [[404 545]\n",
      " [589 584]\n",
      " [521 567]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[404 545]\n",
      " [589 584]\n",
      " [521 567]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6687\n",
      "Image 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg: point, Data: [[526 488]\n",
      " [399 491]\n",
      " [393 592]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[526 488]\n",
      " [399 491]\n",
      " [393 592]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6761\n",
      "Image 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg: box, Data: [[522   4 559 172]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[522   4 559 172]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6911\n",
      "Image 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg: point, Data: [[115 287]\n",
      " [  5 203]\n",
      " [180 168]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[115 287]\n",
      " [  5 203]\n",
      " [180 168]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0251\n",
      "Image 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg: box, Data: [[285 188 327 397]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[285 188 327 397]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6912\n",
      "Image 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg: point, Data: [[397 209]\n",
      " [430 155]\n",
      " [516  19]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[397 209]\n",
      " [430 155]\n",
      " [516  19]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6865\n",
      "Image 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg: box, Data: [[  0 228 390 626]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0 228 390 626]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6166\n",
      "Epoch 1/10, Loss: 0.7341\n",
      "Saving checkpoint: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\checkpoints\\sam2_epoch_1.pt\n",
      "Successfully saved checkpoint: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\checkpoints\\sam2_epoch_1.pt\n",
      "Image Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg: point, Data: [[104 514]\n",
      " [549 130]\n",
      " [113 516]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[104 514]\n",
      " [549 130]\n",
      " [113 516]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7090\n",
      "Image 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg: point, Data: [[538  22]\n",
      " [305 181]\n",
      " [631 145]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[538  22]\n",
      " [305 181]\n",
      " [631 145]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9449\n",
      "Image old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg: box, Data: [[187 180 316 417]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[187 180 316 417]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6876\n",
      "Image d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg: box, Data: [[152   1 380 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[152   1 380 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6867\n",
      "Image 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg: point, Data: [[472 183]\n",
      " [421  57]\n",
      " [467  93]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[472 183]\n",
      " [421  57]\n",
      " [467  93]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6899\n",
      "Image 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg: point, Data: [[175  30]\n",
      " [160  44]\n",
      " [153  20]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[175  30]\n",
      " [160  44]\n",
      " [153  20]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6923\n",
      "Image concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg: box, Data: [[140 120 495 503]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[140 120 495 503]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6378\n",
      "Image 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg: box, Data: [[288   0 637 495]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[288   0 637 495]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6927\n",
      "Image 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg: box, Data: [[301 424 432 572]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[301 424 432 572]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6914\n",
      "Image 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg: box, Data: [[ 56 335  89 421]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 56 335  89 421]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6922\n",
      "Image CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg: point, Data: [[189 198]\n",
      " [144  85]\n",
      " [255 347]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[189 198]\n",
      " [144  85]\n",
      " [255 347]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6411\n",
      "Image 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg: point, Data: [[157 189]\n",
      " [168 349]\n",
      " [127 411]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[157 189]\n",
      " [168 349]\n",
      " [127 411]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1474\n",
      "Image 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg: box, Data: [[  0  54 631 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0  54 631 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8243\n",
      "Image images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg: point, Data: [[355 387]\n",
      " [306 339]\n",
      " [431 542]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[355 387]\n",
      " [306 339]\n",
      " [431 542]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6987\n",
      "Image istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg: box, Data: [[241 201 457 434]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[241 201 457 434]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6626\n",
      "Image 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg: box, Data: [[299   0 628 635]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[299   0 628 635]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6920\n",
      "Image 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg: point, Data: [[400 379]\n",
      " [500 236]\n",
      " [216 427]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[400 379]\n",
      " [500 236]\n",
      " [216 427]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9771\n",
      "Image 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg: box, Data: [[416 148 555 225]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[416 148 555 225]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6933\n",
      "Image 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg: box, Data: [[146 374 639 636]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[146 374 639 636]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6862\n",
      "Image 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg: box, Data: [[ 63  32 639 467]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 63  32 639 467]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6953\n",
      "Image images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg: box, Data: [[ 65   0 513 417]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 65   0 513 417]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6888\n",
      "Image 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg: point, Data: [[203 133]\n",
      " [152 127]\n",
      " [214 123]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[203 133]\n",
      " [152 127]\n",
      " [214 123]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6919\n",
      "Image Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg: point, Data: [[517 525]\n",
      " [505 485]\n",
      " [493 535]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[517 525]\n",
      " [505 485]\n",
      " [493 535]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6904\n",
      "Image 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg: box, Data: [[319 371 639 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[319 371 639 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6886\n",
      "Image 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg: point, Data: [[612 344]\n",
      " [425 397]\n",
      " [203 150]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[612 344]\n",
      " [425 397]\n",
      " [203 150]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0599\n",
      "Image images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg: point, Data: [[402 372]\n",
      " [254 258]\n",
      " [335 352]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[402 372]\n",
      " [254 258]\n",
      " [335 352]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6839\n",
      "Image 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg: point, Data: [[  8 520]\n",
      " [ 39 541]\n",
      " [ 21 511]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[  8 520]\n",
      " [ 39 541]\n",
      " [ 21 511]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6912\n",
      "Image 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg: box, Data: [[191   0 639 630]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[191   0 639 630]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6881\n",
      "Image 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg: point, Data: [[528 101]\n",
      " [543  56]\n",
      " [551 165]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[528 101]\n",
      " [543  56]\n",
      " [551 165]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6905\n",
      "Image 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg: point, Data: [[628 224]\n",
      " [581 253]\n",
      " [614 109]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[628 224]\n",
      " [581 253]\n",
      " [614 109]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0170\n",
      "Image Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg: point, Data: [[525 379]\n",
      " [131 558]\n",
      " [551 194]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[525 379]\n",
      " [131 558]\n",
      " [551 194]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6940\n",
      "Image 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg: point, Data: [[114  80]\n",
      " [121 108]\n",
      " [146  88]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[114  80]\n",
      " [121 108]\n",
      " [146  88]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6915\n",
      "Image 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg: point, Data: [[141 348]\n",
      " [268  12]\n",
      " [267 240]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[141 348]\n",
      " [268  12]\n",
      " [267 240]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8556\n",
      "Image 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg: point, Data: [[342 499]\n",
      " [ 35 591]\n",
      " [250 362]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[342 499]\n",
      " [ 35 591]\n",
      " [250 362]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6124\n",
      "Image 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg: box, Data: [[  0   0 523 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 523 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6861\n",
      "Image 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg: box, Data: [[ 90  47 345 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 90  47 345 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6906\n",
      "Image 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg: point, Data: [[ 80  15]\n",
      " [242  38]\n",
      " [234  98]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 80  15]\n",
      " [242  38]\n",
      " [234  98]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7255\n",
      "Image 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg: point, Data: [[241 451]\n",
      " [359 494]\n",
      " [276 538]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[241 451]\n",
      " [359 494]\n",
      " [276 538]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6791\n",
      "Image 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg: point, Data: [[ 79 525]\n",
      " [188 450]\n",
      " [100 483]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 79 525]\n",
      " [188 450]\n",
      " [100 483]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6846\n",
      "Image stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg: box, Data: [[ 17  73 524 595]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 17  73 524 595]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6681\n",
      "Image 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg: box, Data: [[ 74 311 439 491]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 74 311 439 491]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6933\n",
      "Image 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg: point, Data: [[311 127]\n",
      " [306 103]\n",
      " [297 108]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[311 127]\n",
      " [306 103]\n",
      " [297 108]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6937\n",
      "Image 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg: point, Data: [[528 305]\n",
      " [ 81 553]\n",
      " [351 335]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[528 305]\n",
      " [ 81 553]\n",
      " [351 335]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7228\n",
      "Image 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg: point, Data: [[ 86 131]\n",
      " [129 135]\n",
      " [159 152]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 86 131]\n",
      " [129 135]\n",
      " [159 152]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6886\n",
      "Image 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg: box, Data: [[278 218 428 414]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[278 218 428 414]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6804\n",
      "Image 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg: point, Data: [[ 90 339]\n",
      " [ 83 254]\n",
      " [ 62 349]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 90 339]\n",
      " [ 83 254]\n",
      " [ 62 349]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6895\n",
      "Image 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg: point, Data: [[564 100]\n",
      " [557 258]\n",
      " [571 120]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[564 100]\n",
      " [557 258]\n",
      " [571 120]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2613\n",
      "Image 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg: point, Data: [[633 468]\n",
      " [637 383]\n",
      " [602 441]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[633 468]\n",
      " [637 383]\n",
      " [602 441]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6869\n",
      "Image Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg: point, Data: [[582 346]\n",
      " [280 125]\n",
      " [346 175]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[582 346]\n",
      " [280 125]\n",
      " [346 175]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6923\n",
      "Image 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg: point, Data: [[333 152]\n",
      " [283 376]\n",
      " [133 259]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[333 152]\n",
      " [283 376]\n",
      " [133 259]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7030\n",
      "Image PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg: box, Data: [[ 85 127 573 576]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 85 127 573 576]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5823\n",
      "Image Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg: point, Data: [[337 489]\n",
      " [  4 161]\n",
      " [309  40]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[337 489]\n",
      " [  4 161]\n",
      " [309  40]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.4302\n",
      "Image 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg: box, Data: [[579 438 607 539]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[579 438 607 539]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6920\n",
      "Image concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg: point, Data: [[306 164]\n",
      " [338 157]\n",
      " [285 148]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[306 164]\n",
      " [338 157]\n",
      " [285 148]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6908\n",
      "Image 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg: box, Data: [[114   0 382 496]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[114   0 382 496]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7025\n",
      "Image dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg: point, Data: [[207 422]\n",
      " [538  51]\n",
      " [635  21]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[207 422]\n",
      " [538  51]\n",
      " [635  21]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7301\n",
      "Image 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg: point, Data: [[293 218]\n",
      " [320 370]\n",
      " [316 380]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[293 218]\n",
      " [320 370]\n",
      " [316 380]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6912\n",
      "Image 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg: box, Data: [[  0 101 639 591]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0 101 639 591]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6836\n",
      "Image 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg: point, Data: [[117 134]\n",
      " [128  82]\n",
      " [ 99  39]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[117 134]\n",
      " [128  82]\n",
      " [ 99  39]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6961\n",
      "Image 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg: box, Data: [[ 66 156 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 66 156 639 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8246\n",
      "Image 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg: point, Data: [[ 51 472]\n",
      " [ 49 575]\n",
      " [586 569]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 51 472]\n",
      " [ 49 575]\n",
      " [586 569]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0348\n",
      "Image 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg: box, Data: [[  2 387 412 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  2 387 412 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6886\n",
      "Image concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg: point, Data: [[348 401]\n",
      " [336 386]\n",
      " [357 393]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[348 401]\n",
      " [336 386]\n",
      " [357 393]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6919\n",
      "Image 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg: box, Data: [[405   0 518 480]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[405   0 518 480]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6924\n",
      "Image 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg: point, Data: [[489 472]\n",
      " [401 291]\n",
      " [379 207]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[489 472]\n",
      " [401 291]\n",
      " [379 207]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8760\n",
      "Image 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg: box, Data: [[ 22 417 104 521]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 22 417 104 521]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6905\n",
      "Image 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg: point, Data: [[145 627]\n",
      " [264 481]\n",
      " [ 54 632]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[145 627]\n",
      " [264 481]\n",
      " [ 54 632]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6730\n",
      "Image How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg: point, Data: [[317 210]\n",
      " [282 231]\n",
      " [194 385]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[317 210]\n",
      " [282 231]\n",
      " [194 385]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6727\n",
      "Image 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg: point, Data: [[515 324]\n",
      " [391 270]\n",
      " [575 580]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[515 324]\n",
      " [391 270]\n",
      " [575 580]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5958\n",
      "Image 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg: point, Data: [[175  11]\n",
      " [571 119]\n",
      " [409 130]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[175  11]\n",
      " [571 119]\n",
      " [409 130]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6514\n",
      "Image 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg: box, Data: [[ 66   1 577 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 66   1 577 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6022\n",
      "Image 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg: point, Data: [[515  23]\n",
      " [240  96]\n",
      " [555 111]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[515  23]\n",
      " [240  96]\n",
      " [555 111]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9039\n",
      "Image 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg: box, Data: [[367 328 582 624]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[367 328 582 624]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6913\n",
      "Image 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg: box, Data: [[166  11 211 148]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[166  11 211 148]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6918\n",
      "Image istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg: point, Data: [[306 348]\n",
      " [261 223]\n",
      " [281 234]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[306 348]\n",
      " [261 223]\n",
      " [281 234]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6552\n",
      "Image 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg: box, Data: [[ 48   0 176 178]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 48   0 176 178]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6918\n",
      "Image 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg: box, Data: [[167 345 192 392]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[167 345 192 392]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6927\n",
      "Image 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg: box, Data: [[ 11  46 232 248]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 11  46 232 248]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6904\n",
      "Image 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg: box, Data: [[  1   0 639 304]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  1   0 639 304]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6856\n",
      "Image 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg: point, Data: [[494 540]\n",
      " [483 457]\n",
      " [374 565]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[494 540]\n",
      " [483 457]\n",
      " [374 565]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6839\n",
      "Image 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg: point, Data: [[279  79]\n",
      " [342  42]\n",
      " [241  22]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[279  79]\n",
      " [342  42]\n",
      " [241  22]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7097\n",
      "Image 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg: point, Data: [[337 306]\n",
      " [361 456]\n",
      " [238 443]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[337 306]\n",
      " [361 456]\n",
      " [238 443]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6919\n",
      "Image 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg: box, Data: [[  5  89 591 603]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  5  89 591 603]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6932\n",
      "Image 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg: point, Data: [[507 251]\n",
      " [164 280]\n",
      " [454 230]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[507 251]\n",
      " [164 280]\n",
      " [454 230]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7715\n",
      "Image 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg: box, Data: [[ 29   1 147 143]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 29   1 147 143]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6910\n",
      "Image 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg: box, Data: [[258   0 542 573]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[258   0 542 573]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6866\n",
      "Image lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg: box, Data: [[242  65 439 419]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[242  65 439 419]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6908\n",
      "Image 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg: point, Data: [[180 107]\n",
      " [346  53]\n",
      " [389 487]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[180 107]\n",
      " [346  53]\n",
      " [389 487]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9698\n",
      "Image 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg: box, Data: [[  0   0 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 639 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7416\n",
      "Image 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg: box, Data: [[  0   3 436 309]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   3 436 309]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6900\n",
      "Image 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg: point, Data: [[243 340]\n",
      " [ 69 108]\n",
      " [257 368]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[243 340]\n",
      " [ 69 108]\n",
      " [257 368]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8700\n",
      "Image 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg: box, Data: [[  0   0 639 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 639 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7873\n",
      "Image efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg: box, Data: [[  0   0 638 520]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 638 520]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5794\n",
      "Image 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg: point, Data: [[312 288]\n",
      " [381 344]\n",
      " [451 255]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[312 288]\n",
      " [381 344]\n",
      " [451 255]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8415\n",
      "Image 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg: box, Data: [[  3   4 636 565]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  3   4 636 565]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6830\n",
      "Image 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg: point, Data: [[ 79 407]\n",
      " [ 45 206]\n",
      " [112 430]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 79 407]\n",
      " [ 45 206]\n",
      " [112 430]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2026\n",
      "Image images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg: box, Data: [[  7  98 587 583]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  7  98 587 583]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6766\n",
      "Image 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg: box, Data: [[  0  98 637 524]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0  98 637 524]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6829\n",
      "Image 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg: point, Data: [[556 150]\n",
      " [305 123]\n",
      " [474 118]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[556 150]\n",
      " [305 123]\n",
      " [474 118]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7067\n",
      "Image 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg: box, Data: [[141  46 615 339]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[141  46 615 339]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7786\n",
      "Image 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg: box, Data: [[  1   0 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  1   0 639 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8042\n",
      "Image 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg: point, Data: [[ 30 619]\n",
      " [ 22 603]\n",
      " [ 42 586]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 30 619]\n",
      " [ 22 603]\n",
      " [ 42 586]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6894\n",
      "Image Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg: box, Data: [[ 77 215 591 603]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 77 215 591 603]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6769\n",
      "Image images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg: point, Data: [[172 460]\n",
      " [206 343]\n",
      " [173  58]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[172 460]\n",
      " [206 343]\n",
      " [173  58]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6339\n",
      "Image what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg: point, Data: [[495 152]\n",
      " [466 131]\n",
      " [480 155]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[495 152]\n",
      " [466 131]\n",
      " [480 155]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0133\n",
      "Image images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg: point, Data: [[182 379]\n",
      " [226 198]\n",
      " [273 424]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[182 379]\n",
      " [226 198]\n",
      " [273 424]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6420\n",
      "Image 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg: box, Data: [[  0   4 363 518]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   4 363 518]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7005\n",
      "Image 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg: box, Data: [[ 12   0 498 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 12   0 498 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7543\n",
      "Image 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg: box, Data: [[ 30  42 247 539]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 30  42 247 539]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6897\n",
      "Image 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg: box, Data: [[ 76   0 257 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 76   0 257 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6896\n",
      "Image 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg: point, Data: [[232 214]\n",
      " [173 593]\n",
      " [ 97 414]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[232 214]\n",
      " [173 593]\n",
      " [ 97 414]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9575\n",
      "Image Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg: box, Data: [[146 160 323 442]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[146 160 323 442]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6613\n",
      "Image old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg: point, Data: [[183 321]\n",
      " [192 556]\n",
      " [143 163]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[183 321]\n",
      " [192 556]\n",
      " [143 163]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6828\n",
      "Image Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg: point, Data: [[617 361]\n",
      " [395 116]\n",
      " [338 250]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[617 361]\n",
      " [395 116]\n",
      " [338 250]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6917\n",
      "Image 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg: point, Data: [[115 352]\n",
      " [100 311]\n",
      " [124 301]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[115 352]\n",
      " [100 311]\n",
      " [124 301]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7238\n",
      "Image IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg: box, Data: [[  0   0 631 345]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 631 345]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6472\n",
      "Image Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg: box, Data: [[  0   0 610 638]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 610 638]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7143\n",
      "Image weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg: point, Data: [[307 220]\n",
      " [316 418]\n",
      " [317 577]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[307 220]\n",
      " [316 418]\n",
      " [317 577]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6911\n",
      "Image 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg: point, Data: [[143 557]\n",
      " [134 505]\n",
      " [110 527]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[143 557]\n",
      " [134 505]\n",
      " [110 527]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6890\n",
      "Image 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg: box, Data: [[  0  89 513 495]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0  89 513 495]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6914\n",
      "Image 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg: box, Data: [[  2 217 521 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  2 217 521 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6852\n",
      "Image crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg: point, Data: [[376 233]\n",
      " [285 370]\n",
      " [247 242]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[376 233]\n",
      " [285 370]\n",
      " [247 242]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6811\n",
      "Image 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg: point, Data: [[553 630]\n",
      " [ 42 564]\n",
      " [ 47 598]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[553 630]\n",
      " [ 42 564]\n",
      " [ 47 598]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6850\n",
      "Image 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg: box, Data: [[321 260 455 638]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[321 260 455 638]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6692\n",
      "Image 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg: point, Data: [[197 420]\n",
      " [162 418]\n",
      " [171 362]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[197 420]\n",
      " [162 418]\n",
      " [171 362]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7670\n",
      "Image 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg: point, Data: [[ 47 323]\n",
      " [315 413]\n",
      " [243 343]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 47 323]\n",
      " [315 413]\n",
      " [243 343]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6342\n",
      "Image 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg: box, Data: [[  0   0 512 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 512 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5652\n",
      "Image 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg: box, Data: [[138  88 493 621]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[138  88 493 621]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6937\n",
      "Image patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg: box, Data: [[268 209 406 474]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[268 209 406 474]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6824\n",
      "Image 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg: box, Data: [[  0   0 507 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 507 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9251\n",
      "Image 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg: point, Data: [[ 56 450]\n",
      " [ 68 432]\n",
      " [170 605]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 56 450]\n",
      " [ 68 432]\n",
      " [170 605]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7615\n",
      "Image 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg: box, Data: [[  1   0 639 598]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  1   0 639 598]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9833\n",
      "Image card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg: box, Data: [[297  69 487 475]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[297  69 487 475]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6564\n",
      "Image 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg: point, Data: [[ 73 102]\n",
      " [405 271]\n",
      " [ 88  51]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 73 102]\n",
      " [405 271]\n",
      " [ 88  51]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6778\n",
      "Epoch 2/10, Loss: 0.7281\n",
      "Saving checkpoint: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\checkpoints\\sam2_epoch_2.pt\n",
      "Successfully saved checkpoint: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\checkpoints\\sam2_epoch_2.pt\n",
      "Image 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg: point, Data: [[161 506]\n",
      " [113 451]\n",
      " [106 446]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[161 506]\n",
      " [113 451]\n",
      " [106 446]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6844\n",
      "Image crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg: box, Data: [[156  37 556 414]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[156  37 556 414]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6602\n",
      "Image 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg: box, Data: [[  3 207 639 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  3 207 639 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6869\n",
      "Image what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg: point, Data: [[284 155]\n",
      " [264 364]\n",
      " [624 106]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[284 155]\n",
      " [264 364]\n",
      " [624 106]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0145\n",
      "Image 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg: box, Data: [[167 345 192 392]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[167 345 192 392]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6927\n",
      "Image 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg: point, Data: [[ 95 127]\n",
      " [226  80]\n",
      " [ 93 138]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 95 127]\n",
      " [226  80]\n",
      " [ 93 138]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2152\n",
      "Image 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg: point, Data: [[347 545]\n",
      " [311 377]\n",
      " [387 534]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[347 545]\n",
      " [311 377]\n",
      " [387 534]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7703\n",
      "Image 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg: box, Data: [[285 188 327 397]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[285 188 327 397]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6912\n",
      "Image 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg: point, Data: [[ 74 208]\n",
      " [292 536]\n",
      " [ 44 121]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 74 208]\n",
      " [292 536]\n",
      " [ 44 121]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2706\n",
      "Image 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg: point, Data: [[483 133]\n",
      " [426  52]\n",
      " [405 172]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[483 133]\n",
      " [426  52]\n",
      " [405 172]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2691\n",
      "Image 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg: point, Data: [[520 610]\n",
      " [168 425]\n",
      " [604 506]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[520 610]\n",
      " [168 425]\n",
      " [604 506]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0435\n",
      "Image 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg: point, Data: [[148 556]\n",
      " [313 253]\n",
      " [233 220]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[148 556]\n",
      " [313 253]\n",
      " [233 220]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9593\n",
      "Image 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg: point, Data: [[ 28  71]\n",
      " [  2  90]\n",
      " [392 269]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 28  71]\n",
      " [  2  90]\n",
      " [392 269]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7888\n",
      "Image images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg: box, Data: [[168 157 489 372]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[168 157 489 372]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6801\n",
      "Image Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg: box, Data: [[ 77 215 591 603]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 77 215 591 603]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6769\n",
      "Image 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg: box, Data: [[367 328 582 624]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[367 328 582 624]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6913\n",
      "Image 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg: box, Data: [[ 51   0 612 395]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 51   0 612 395]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6981\n",
      "Image 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg: box, Data: [[ 92 490 190 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 92 490 190 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6910\n",
      "Image 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg: point, Data: [[576 114]\n",
      " [545 180]\n",
      " [539 201]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[576 114]\n",
      " [545 180]\n",
      " [539 201]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6924\n",
      "Image old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg: point, Data: [[189 171]\n",
      " [135 240]\n",
      " [202 448]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[189 171]\n",
      " [135 240]\n",
      " [202 448]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6818\n",
      "Image 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg: box, Data: [[  0  54 631 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0  54 631 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8243\n",
      "Image 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg: point, Data: [[250  68]\n",
      " [308  27]\n",
      " [620 187]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[250  68]\n",
      " [308  27]\n",
      " [620 187]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7772\n",
      "Image 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg: box, Data: [[ 68   2 172 422]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 68   2 172 422]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6893\n",
      "Image 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg: box, Data: [[321 260 455 638]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[321 260 455 638]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6692\n",
      "Image 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg: box, Data: [[  1 149 600 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  1 149 600 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9886\n",
      "Image 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg: point, Data: [[446 138]\n",
      " [389 362]\n",
      " [279 195]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[446 138]\n",
      " [389 362]\n",
      " [279 195]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0458\n",
      "Image 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg: box, Data: [[ 13 256 359 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 13 256 359 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6799\n",
      "Image 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg: point, Data: [[359 115]\n",
      " [430 293]\n",
      " [601 342]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[359 115]\n",
      " [430 293]\n",
      " [601 342]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1020\n",
      "Image 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg: point, Data: [[464 327]\n",
      " [483 365]\n",
      " [157 604]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[464 327]\n",
      " [483 365]\n",
      " [157 604]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6954\n",
      "Image 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg: point, Data: [[423 205]\n",
      " [213 194]\n",
      " [113 200]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[423 205]\n",
      " [213 194]\n",
      " [113 200]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6344\n",
      "Image 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg: point, Data: [[ 58 381]\n",
      " [ 78 409]\n",
      " [ 66 417]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 58 381]\n",
      " [ 78 409]\n",
      " [ 66 417]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6919\n",
      "Image efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg: point, Data: [[566 177]\n",
      " [391 253]\n",
      " [ 96 401]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[566 177]\n",
      " [391 253]\n",
      " [ 96 401]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5752\n",
      "Image weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg: box, Data: [[283   0 352 638]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[283   0 352 638]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6919\n",
      "Image 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg: point, Data: [[587 374]\n",
      " [631 560]\n",
      " [462 488]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[587 374]\n",
      " [631 560]\n",
      " [462 488]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6846\n",
      "Image images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg: box, Data: [[116   0 382 466]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[116   0 382 466]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6428\n",
      "Image 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg: point, Data: [[533 587]\n",
      " [130 572]\n",
      " [118 610]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[533 587]\n",
      " [130 572]\n",
      " [118 610]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2563\n",
      "Image 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg: point, Data: [[530 310]\n",
      " [327 183]\n",
      " [464 334]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[530 310]\n",
      " [327 183]\n",
      " [464 334]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9401\n",
      "Image 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg: box, Data: [[ 74 311 439 491]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 74 311 439 491]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6933\n",
      "Image card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg: point, Data: [[486 332]\n",
      " [411 400]\n",
      " [333 142]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[486 332]\n",
      " [411 400]\n",
      " [333 142]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0122\n",
      "Image 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg: point, Data: [[209 257]\n",
      " [224 219]\n",
      " [140 219]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[209 257]\n",
      " [224 219]\n",
      " [140 219]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1118\n",
      "Image 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg: point, Data: [[296 398]\n",
      " [404 360]\n",
      " [412 303]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[296 398]\n",
      " [404 360]\n",
      " [412 303]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8217\n",
      "Image CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg: point, Data: [[228 340]\n",
      " [195 407]\n",
      " [156 440]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[228 340]\n",
      " [195 407]\n",
      " [156 440]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6825\n",
      "Image 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg: box, Data: [[103  35 147 169]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[103  35 147 169]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6899\n",
      "Image 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg: box, Data: [[  4 415 338 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  4 415 338 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6889\n",
      "Image Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg: box, Data: [[  0   0 610 638]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 610 638]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7143\n",
      "Image 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg: box, Data: [[114   0 382 496]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[114   0 382 496]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7025\n",
      "Image 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg: box, Data: [[  0   0 639 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 639 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7873\n",
      "Image 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg: box, Data: [[  0 228 390 626]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0 228 390 626]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6166\n",
      "Image 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg: point, Data: [[111  89]\n",
      " [129  55]\n",
      " [139 161]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[111  89]\n",
      " [129  55]\n",
      " [139 161]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7322\n",
      "Image 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg: point, Data: [[374  81]\n",
      " [286 191]\n",
      " [286 211]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[374  81]\n",
      " [286 191]\n",
      " [286 211]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1568\n",
      "Image Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg: box, Data: [[ 62   0 638 558]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 62   0 638 558]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6567\n",
      "Image 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg: box, Data: [[ 22 417 104 521]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 22 417 104 521]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6905\n",
      "Image images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg: box, Data: [[ 84  38 319 507]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 84  38 319 507]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6351\n",
      "Image concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg: box, Data: [[140 120 495 503]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[140 120 495 503]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6378\n",
      "Image stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg: box, Data: [[ 17  73 524 595]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 17  73 524 595]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6681\n",
      "Image lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg: box, Data: [[242  65 439 419]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[242  65 439 419]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6908\n",
      "Image 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg: point, Data: [[489 265]\n",
      " [308  31]\n",
      " [282 337]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[489 265]\n",
      " [308  31]\n",
      " [282 337]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6646\n",
      "Image 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg: point, Data: [[492 189]\n",
      " [372 161]\n",
      " [ 23 273]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[492 189]\n",
      " [372 161]\n",
      " [ 23 273]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2290\n",
      "Image Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg: point, Data: [[522 534]\n",
      " [500 505]\n",
      " [505 508]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[522 534]\n",
      " [500 505]\n",
      " [505 508]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6904\n",
      "Image 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg: point, Data: [[ 44 125]\n",
      " [401 287]\n",
      " [104  93]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 44 125]\n",
      " [401 287]\n",
      " [104  93]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6873\n",
      "Image 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg: box, Data: [[141  46 615 339]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[141  46 615 339]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7786\n",
      "Image 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg: point, Data: [[468 563]\n",
      " [397 138]\n",
      " [470 486]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[468 563]\n",
      " [397 138]\n",
      " [470 486]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7941\n",
      "Image 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg: point, Data: [[313 614]\n",
      " [439 395]\n",
      " [167 587]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[313 614]\n",
      " [439 395]\n",
      " [167 587]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7192\n",
      "Image images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg: point, Data: [[345 151]\n",
      " [506 194]\n",
      " [503 244]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[345 151]\n",
      " [506 194]\n",
      " [503 244]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6825\n",
      "Image 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg: box, Data: [[ 66 156 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 66 156 639 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8246\n",
      "Image 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg: box, Data: [[  2 217 521 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  2 217 521 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6852\n",
      "Image 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg: box, Data: [[ 65   0 293 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 65   0 293 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6845\n",
      "Image 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg: box, Data: [[ 33   0 615 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 33   0 615 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6859\n",
      "Image 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg: box, Data: [[513   0 637 274]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[513   0 637 274]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6906\n",
      "Image 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg: point, Data: [[133 100]\n",
      " [131  24]\n",
      " [133  74]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[133 100]\n",
      " [131  24]\n",
      " [133  74]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6922\n",
      "Image images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg: box, Data: [[306 309 471 583]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[306 309 471 583]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6653\n",
      "Image concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg: point, Data: [[327 184]\n",
      " [326 401]\n",
      " [325 184]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[327 184]\n",
      " [326 401]\n",
      " [325 184]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6903\n",
      "Image 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg: point, Data: [[205  78]\n",
      " [198 124]\n",
      " [173  77]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[205  78]\n",
      " [198 124]\n",
      " [173  77]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2607\n",
      "Image 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg: point, Data: [[101 159]\n",
      " [ 30 321]\n",
      " [ 36  17]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[101 159]\n",
      " [ 30 321]\n",
      " [ 36  17]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8742\n",
      "Image 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg: box, Data: [[539  79 576 272]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[539  79 576 272]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6914\n",
      "Image Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg: box, Data: [[  0   0 638 638]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 638 638]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7462\n",
      "Image 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg: point, Data: [[426 478]\n",
      " [392 439]\n",
      " [334 539]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[426 478]\n",
      " [392 439]\n",
      " [334 539]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2623\n",
      "Image 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg: box, Data: [[ 90  47 345 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 90  47 345 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6906\n",
      "Image 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg: point, Data: [[ 80 113]\n",
      " [ 44  33]\n",
      " [ 87  39]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 80 113]\n",
      " [ 44  33]\n",
      " [ 87  39]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7572\n",
      "Image patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg: point, Data: [[360 221]\n",
      " [363 261]\n",
      " [370 285]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[360 221]\n",
      " [363 261]\n",
      " [370 285]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6842\n",
      "Image 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg: point, Data: [[433 461]\n",
      " [403 622]\n",
      " [281 539]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[433 461]\n",
      " [403 622]\n",
      " [281 539]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6723\n",
      "Image Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg: point, Data: [[298 310]\n",
      " [352 281]\n",
      " [396 576]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[298 310]\n",
      " [352 281]\n",
      " [396 576]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6825\n",
      "Image 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg: box, Data: [[131 303 197 534]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[131 303 197 534]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6906\n",
      "Image How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg: box, Data: [[177 129 345 494]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[177 129 345 494]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6565\n",
      "Image 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg: box, Data: [[103   0 310 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[103   0 310 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6858\n",
      "Image 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg: box, Data: [[191   0 639 630]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[191   0 639 630]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6881\n",
      "Image 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg: box, Data: [[  1   0 639 304]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  1   0 639 304]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6856\n",
      "Image 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg: box, Data: [[125   0 506 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[125   0 506 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6912\n",
      "Image 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg: box, Data: [[ 36  23 116 577]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 36  23 116 577]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6919\n",
      "Image Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg: box, Data: [[  0   0 636 612]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 636 612]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2551\n",
      "Image 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg: box, Data: [[  0   0 523 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 523 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6861\n",
      "Image dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg: box, Data: [[ 80   0 637 609]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 80   0 637 609]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8884\n",
      "Image 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg: point, Data: [[263 310]\n",
      " [428 183]\n",
      " [419 370]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[263 310]\n",
      " [428 183]\n",
      " [419 370]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.4414\n",
      "Image concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg: point, Data: [[353 370]\n",
      " [339 384]\n",
      " [362 355]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[353 370]\n",
      " [339 384]\n",
      " [362 355]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6920\n",
      "Image 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg: box, Data: [[ 11  46 232 248]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 11  46 232 248]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6904\n",
      "Image 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg: point, Data: [[ 17 636]\n",
      " [ 34 615]\n",
      " [ 13 592]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 17 636]\n",
      " [ 34 615]\n",
      " [ 13 592]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6910\n",
      "Image 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg: point, Data: [[261 279]\n",
      " [151 251]\n",
      " [ 62 229]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[261 279]\n",
      " [151 251]\n",
      " [ 62 229]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6702\n",
      "Image 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg: box, Data: [[522   4 559 172]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[522   4 559 172]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6911\n",
      "Image d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg: box, Data: [[152   1 380 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[152   1 380 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6867\n",
      "Image Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg: point, Data: [[ 40 103]\n",
      " [585 453]\n",
      " [596 452]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 40 103]\n",
      " [585 453]\n",
      " [596 452]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7000\n",
      "Image 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg: box, Data: [[416 148 555 225]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[416 148 555 225]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6933\n",
      "Image 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg: point, Data: [[206 439]\n",
      " [ 42 365]\n",
      " [540 262]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[206 439]\n",
      " [ 42 365]\n",
      " [540 262]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6154\n",
      "Image istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg: box, Data: [[130 110 494 452]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[130 110 494 452]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6531\n",
      "Image 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg: point, Data: [[ 53 596]\n",
      " [302 427]\n",
      " [  7 183]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 53 596]\n",
      " [302 427]\n",
      " [  7 183]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0424\n",
      "Image 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg: box, Data: [[146   0 179  68]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[146   0 179  68]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6922\n",
      "Image 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg: box, Data: [[299   0 628 635]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[299   0 628 635]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6920\n",
      "Image 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg: point, Data: [[500 364]\n",
      " [405   4]\n",
      " [466 406]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[500 364]\n",
      " [405   4]\n",
      " [466 406]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6943\n",
      "Image 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg: box, Data: [[288   0 637 495]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[288   0 637 495]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6927\n",
      "Image 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg: point, Data: [[409 138]\n",
      " [308 422]\n",
      " [361 238]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[409 138]\n",
      " [308 422]\n",
      " [361 238]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7778\n",
      "Image 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg: box, Data: [[  0  89 513 495]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0  89 513 495]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6914\n",
      "Image 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg: point, Data: [[197 303]\n",
      " [601 192]\n",
      " [475 208]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[197 303]\n",
      " [601 192]\n",
      " [475 208]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7851\n",
      "Image old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg: point, Data: [[204 394]\n",
      " [223 393]\n",
      " [191 224]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[204 394]\n",
      " [223 393]\n",
      " [191 224]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6893\n",
      "Image 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg: box, Data: [[283   0 637 229]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[283   0 637 229]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6877\n",
      "Image images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg: point, Data: [[504 333]\n",
      " [178  68]\n",
      " [507 288]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[504 333]\n",
      " [178  68]\n",
      " [507 288]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6900\n",
      "Image Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg: box, Data: [[146 160 323 442]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[146 160 323 442]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6613\n",
      "Image 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg: box, Data: [[  0   4 363 518]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   4 363 518]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7005\n",
      "Image 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg: box, Data: [[  5  89 591 603]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  5  89 591 603]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6932\n",
      "Image 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg: point, Data: [[144 123]\n",
      " [177 132]\n",
      " [216 107]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[144 123]\n",
      " [177 132]\n",
      " [216 107]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6935\n",
      "Image 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg: point, Data: [[129 320]\n",
      " [136 395]\n",
      " [105 303]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[129 320]\n",
      " [136 395]\n",
      " [105 303]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7425\n",
      "Image 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg: box, Data: [[  6  10 632 510]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  6  10 632 510]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6970\n",
      "Image 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg: box, Data: [[282  68 312 173]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[282  68 312 173]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6918\n",
      "Image istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg: box, Data: [[241 201 457 434]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[241 201 457 434]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6626\n",
      "Image 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg: point, Data: [[608 314]\n",
      " [601 328]\n",
      " [583 524]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[608 314]\n",
      " [601 328]\n",
      " [583 524]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1911\n",
      "Image IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg: box, Data: [[  0   0 631 345]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 631 345]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6472\n",
      "Image PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg: point, Data: [[513 184]\n",
      " [488 389]\n",
      " [527 508]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[513 184]\n",
      " [488 389]\n",
      " [527 508]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7746\n",
      "Image 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg: box, Data: [[ 63  32 639 467]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 63  32 639 467]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6953\n",
      "Image 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg: point, Data: [[184 194]\n",
      " [184 392]\n",
      " [451 249]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[184 194]\n",
      " [184 392]\n",
      " [451 249]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6784\n",
      "Image 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg: point, Data: [[433 386]\n",
      " [481 529]\n",
      " [217 533]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[433 386]\n",
      " [481 529]\n",
      " [217 533]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5906\n",
      "Image 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg: box, Data: [[ 76   0 257 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 76   0 257 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6896\n",
      "Image 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg: box, Data: [[ 12   0 498 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 12   0 498 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7543\n",
      "Image 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg: point, Data: [[146 396]\n",
      " [146 366]\n",
      " [273 260]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[146 396]\n",
      " [146 366]\n",
      " [273 260]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6996\n",
      "Image 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg: point, Data: [[ 52 599]\n",
      " [ 69 471]\n",
      " [404 475]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 52 599]\n",
      " [ 69 471]\n",
      " [404 475]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9177\n",
      "Image 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg: box, Data: [[178 146 428 462]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[178 146 428 462]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6553\n",
      "Image 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg: box, Data: [[579 438 607 539]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[579 438 607 539]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6920\n",
      "Epoch 3/10, Loss: 0.7629\n",
      "Saving checkpoint: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\checkpoints\\sam2_epoch_3.pt\n",
      "Successfully saved checkpoint: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\checkpoints\\sam2_epoch_3.pt\n",
      "Image 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg: box, Data: [[166  11 211 148]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[166  11 211 148]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6918\n",
      "Image images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg: box, Data: [[ 84  38 319 507]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 84  38 319 507]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6351\n",
      "Image 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg: point, Data: [[563 280]\n",
      " [564 212]\n",
      " [617 269]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[563 280]\n",
      " [564 212]\n",
      " [617 269]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9779\n",
      "Image 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg: point, Data: [[615 202]\n",
      " [626 160]\n",
      " [590 174]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[615 202]\n",
      " [626 160]\n",
      " [590 174]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6920\n",
      "Image istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg: box, Data: [[241 201 457 434]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[241 201 457 434]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6626\n",
      "Image images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg: point, Data: [[369 432]\n",
      " [402 571]\n",
      " [410 409]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[369 432]\n",
      " [402 571]\n",
      " [410 409]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6652\n",
      "Image 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg: point, Data: [[573 111]\n",
      " [541 227]\n",
      " [548 181]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[573 111]\n",
      " [541 227]\n",
      " [548 181]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2936\n",
      "Image 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg: point, Data: [[295 112]\n",
      " [242 121]\n",
      " [349 487]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[295 112]\n",
      " [242 121]\n",
      " [349 487]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9607\n",
      "Image 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg: point, Data: [[481 187]\n",
      " [443 165]\n",
      " [431 157]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[481 187]\n",
      " [443 165]\n",
      " [431 157]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6936\n",
      "Image 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg: box, Data: [[  1 128 639 587]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  1 128 639 587]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6845\n",
      "Image 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg: box, Data: [[288   0 637 495]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[288   0 637 495]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6927\n",
      "Image 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg: point, Data: [[105  34]\n",
      " [147 108]\n",
      " [223  93]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[105  34]\n",
      " [147 108]\n",
      " [223  93]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9636\n",
      "Image Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg: box, Data: [[  0   0 636 612]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 636 612]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2551\n",
      "Image 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg: box, Data: [[285 188 327 397]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[285 188 327 397]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6912\n",
      "Image 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg: point, Data: [[ 34 427]\n",
      " [ 86 473]\n",
      " [ 78 478]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 34 427]\n",
      " [ 86 473]\n",
      " [ 78 478]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6898\n",
      "Image old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg: point, Data: [[186 384]\n",
      " [178 350]\n",
      " [166 231]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[186 384]\n",
      " [178 350]\n",
      " [166 231]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6772\n",
      "Image 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg: point, Data: [[481 581]\n",
      " [475 478]\n",
      " [392 284]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[481 581]\n",
      " [475 478]\n",
      " [392 284]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9294\n",
      "Image 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg: point, Data: [[255 439]\n",
      " [134 349]\n",
      " [629 124]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[255 439]\n",
      " [134 349]\n",
      " [629 124]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7872\n",
      "Image 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg: box, Data: [[ 92 490 190 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 92 490 190 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6910\n",
      "Image 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg: box, Data: [[  0 101 639 591]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0 101 639 591]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6836\n",
      "Image 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg: box, Data: [[  1   0 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  1   0 639 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8042\n",
      "Image 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg: box, Data: [[  1 149 600 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  1 149 600 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9886\n",
      "Image 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg: box, Data: [[271  68 622 593]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[271  68 622 593]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6881\n",
      "Image 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg: box, Data: [[299   0 628 635]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[299   0 628 635]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6920\n",
      "Image 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg: box, Data: [[231 374 533 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[231 374 533 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6820\n",
      "Image concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg: box, Data: [[336 345 373 409]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[336 345 373 409]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6920\n",
      "Image concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg: point, Data: [[336 456]\n",
      " [209 481]\n",
      " [339 278]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[336 456]\n",
      " [209 481]\n",
      " [339 278]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6487\n",
      "Image 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg: point, Data: [[255 269]\n",
      " [ 95 292]\n",
      " [313 311]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[255 269]\n",
      " [ 95 292]\n",
      " [313 311]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8846\n",
      "Image Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg: point, Data: [[318 310]\n",
      " [316 293]\n",
      " [354 404]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[318 310]\n",
      " [316 293]\n",
      " [354 404]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6687\n",
      "Image 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg: point, Data: [[499  23]\n",
      " [509  37]\n",
      " [536 197]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[499  23]\n",
      " [509  37]\n",
      " [536 197]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6924\n",
      "Image 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg: point, Data: [[ 61 184]\n",
      " [ 31 188]\n",
      " [ 82 175]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 61 184]\n",
      " [ 31 188]\n",
      " [ 82 175]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6933\n",
      "Image 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg: point, Data: [[433 401]\n",
      " [382 569]\n",
      " [575 406]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[433 401]\n",
      " [382 569]\n",
      " [575 406]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5567\n",
      "Image 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg: box, Data: [[ 76   0 257 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 76   0 257 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6896\n",
      "Image 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg: box, Data: [[167 345 192 392]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[167 345 192 392]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6927\n",
      "Image 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg: point, Data: [[308 437]\n",
      " [383 462]\n",
      " [311 463]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[308 437]\n",
      " [383 462]\n",
      " [311 463]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6931\n",
      "Image Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg: box, Data: [[ 77 215 591 603]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 77 215 591 603]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6769\n",
      "Image 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg: point, Data: [[445  81]\n",
      " [107 230]\n",
      " [ 33 356]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[445  81]\n",
      " [107 230]\n",
      " [ 33 356]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0468\n",
      "Image istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg: box, Data: [[130 110 494 452]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[130 110 494 452]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6531\n",
      "Image 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg: point, Data: [[114 122]\n",
      " [ 34 115]\n",
      " [ 55  76]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[114 122]\n",
      " [ 34 115]\n",
      " [ 55  76]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2659\n",
      "Image Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg: point, Data: [[115 397]\n",
      " [425 155]\n",
      " [441 262]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[115 397]\n",
      " [425 155]\n",
      " [441 262]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7810\n",
      "Image 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg: point, Data: [[ 29 282]\n",
      " [ 30 385]\n",
      " [ 65 385]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 29 282]\n",
      " [ 30 385]\n",
      " [ 65 385]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0127\n",
      "Image 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg: box, Data: [[579 438 607 539]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[579 438 607 539]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6920\n",
      "Image card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg: box, Data: [[297  69 487 475]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[297  69 487 475]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6564\n",
      "Image 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg: point, Data: [[ 76 212]\n",
      " [ 85 307]\n",
      " [ 81 418]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 76 212]\n",
      " [ 85 307]\n",
      " [ 81 418]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6907\n",
      "Image 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg: point, Data: [[ 24  72]\n",
      " [249 100]\n",
      " [ 27  74]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 24  72]\n",
      " [249 100]\n",
      " [ 27  74]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9406\n",
      "Image 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg: box, Data: [[114   0 382 496]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[114   0 382 496]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7025\n",
      "Image Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg: box, Data: [[  0   0 638 638]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 638 638]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7462\n",
      "Image 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg: point, Data: [[473 339]\n",
      " [405  11]\n",
      " [437 477]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[473 339]\n",
      " [405  11]\n",
      " [437 477]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9595\n",
      "Image 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg: box, Data: [[ 48   0 176 178]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 48   0 176 178]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6918\n",
      "Image 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg: point, Data: [[202 230]\n",
      " [144 323]\n",
      " [232 426]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[202 230]\n",
      " [144 323]\n",
      " [232 426]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1770\n",
      "Image 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg: point, Data: [[606 561]\n",
      " [623 465]\n",
      " [619 333]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[606 561]\n",
      " [623 465]\n",
      " [619 333]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6833\n",
      "Image 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg: point, Data: [[289 473]\n",
      " [571 603]\n",
      " [361 465]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[289 473]\n",
      " [571 603]\n",
      " [361 465]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1379\n",
      "Image images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg: point, Data: [[329 153]\n",
      " [176 416]\n",
      " [198 507]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[329 153]\n",
      " [176 416]\n",
      " [198 507]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8415\n",
      "Image How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg: box, Data: [[177 129 345 494]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[177 129 345 494]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6565\n",
      "Image patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg: box, Data: [[268 209 406 474]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[268 209 406 474]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6824\n",
      "Image CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg: point, Data: [[132  89]\n",
      " [289 250]\n",
      " [224  36]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[132  89]\n",
      " [289 250]\n",
      " [224  36]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6439\n",
      "Image 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg: box, Data: [[367 328 582 624]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[367 328 582 624]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6913\n",
      "Image 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg: point, Data: [[164 542]\n",
      " [ 37 461]\n",
      " [147 569]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[164 542]\n",
      " [ 37 461]\n",
      " [147 569]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7501\n",
      "Image 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg: box, Data: [[  0   1 414 318]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   1 414 318]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6890\n",
      "Image 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg: point, Data: [[165 509]\n",
      " [190 328]\n",
      " [164 362]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[165 509]\n",
      " [190 328]\n",
      " [164 362]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6891\n",
      "Image 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg: point, Data: [[ 64 383]\n",
      " [ 60 398]\n",
      " [ 72 344]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 64 383]\n",
      " [ 60 398]\n",
      " [ 72 344]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6922\n",
      "Image images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg: point, Data: [[200 152]\n",
      " [148 366]\n",
      " [294 166]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[200 152]\n",
      " [148 366]\n",
      " [294 166]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6520\n",
      "Image 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg: point, Data: [[558 171]\n",
      " [531  40]\n",
      " [539  20]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[558 171]\n",
      " [531  40]\n",
      " [539  20]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6913\n",
      "Image efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg: box, Data: [[  0   0 638 520]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 638 520]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5794\n",
      "Image 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg: point, Data: [[426 477]\n",
      " [430 335]\n",
      " [389 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[426 477]\n",
      " [430 335]\n",
      " [389 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6756\n",
      "Image 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg: box, Data: [[178 146 428 462]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[178 146 428 462]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6553\n",
      "Image 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg: point, Data: [[ 75 518]\n",
      " [116 435]\n",
      " [ 85 516]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 75 518]\n",
      " [116 435]\n",
      " [ 85 516]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2121\n",
      "Image Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg: point, Data: [[337 103]\n",
      " [374 123]\n",
      " [352 238]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[337 103]\n",
      " [374 123]\n",
      " [352 238]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6838\n",
      "Image 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg: point, Data: [[101 502]\n",
      " [266 130]\n",
      " [ 50 546]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[101 502]\n",
      " [266 130]\n",
      " [ 50 546]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.4527\n",
      "Image 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg: point, Data: [[ 71 406]\n",
      " [ 82 378]\n",
      " [ 90 364]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 71 406]\n",
      " [ 82 378]\n",
      " [ 90 364]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6905\n",
      "Image 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg: box, Data: [[141  46 615 339]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[141  46 615 339]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7786\n",
      "Image 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg: box, Data: [[ 65   0 293 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 65   0 293 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6845\n",
      "Image 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg: box, Data: [[ 21   0 543 636]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 21   0 543 636]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6856\n",
      "Image 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg: box, Data: [[  0 128 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0 128 639 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6848\n",
      "Image 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg: point, Data: [[138 401]\n",
      " [123 330]\n",
      " [134 313]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[138 401]\n",
      " [123 330]\n",
      " [134 313]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7538\n",
      "Image 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg: point, Data: [[285 518]\n",
      " [268 505]\n",
      " [263 599]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[285 518]\n",
      " [268 505]\n",
      " [263 599]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7840\n",
      "Image 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg: point, Data: [[177 372]\n",
      " [484 561]\n",
      " [552 193]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[177 372]\n",
      " [484 561]\n",
      " [552 193]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0458\n",
      "Image 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg: point, Data: [[140 378]\n",
      " [147 437]\n",
      " [165 497]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[140 378]\n",
      " [147 437]\n",
      " [165 497]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6811\n",
      "Image d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg: point, Data: [[294  23]\n",
      " [222  94]\n",
      " [224  74]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[294  23]\n",
      " [222  94]\n",
      " [224  74]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6921\n",
      "Image 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg: point, Data: [[518 465]\n",
      " [428 244]\n",
      " [440  34]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[518 465]\n",
      " [428 244]\n",
      " [440  34]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1576\n",
      "Image 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg: box, Data: [[280   0 637 424]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[280   0 637 424]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6832\n",
      "Image 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg: box, Data: [[ 13 256 359 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 13 256 359 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6799\n",
      "Image 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg: point, Data: [[628 615]\n",
      " [499  44]\n",
      " [624 565]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[628 615]\n",
      " [499  44]\n",
      " [624 565]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1215\n",
      "Image concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg: box, Data: [[285 131 358 417]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[285 131 358 417]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6902\n",
      "Image 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg: box, Data: [[319 371 639 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[319 371 639 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6886\n",
      "Image what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg: point, Data: [[455 256]\n",
      " [224 209]\n",
      " [281 218]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[455 256]\n",
      " [224 209]\n",
      " [281 218]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0083\n",
      "Image 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg: point, Data: [[330 452]\n",
      " [ 20 610]\n",
      " [318 472]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[330 452]\n",
      " [ 20 610]\n",
      " [318 472]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1961\n",
      "Image 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg: box, Data: [[  4 563 104 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  4 563 104 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6922\n",
      "Image 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg: box, Data: [[  0   4 363 518]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   4 363 518]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7005\n",
      "Image 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg: box, Data: [[  4 466 599 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  4 466 599 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6899\n",
      "Image dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg: point, Data: [[188 500]\n",
      " [574 107]\n",
      " [393  15]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[188 500]\n",
      " [574 107]\n",
      " [393  15]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7289\n",
      "Image 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg: point, Data: [[415 171]\n",
      " [469 171]\n",
      " [453 201]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[415 171]\n",
      " [469 171]\n",
      " [453 201]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2529\n",
      "Image 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg: box, Data: [[  1   0 639 304]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  1   0 639 304]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6856\n",
      "Image 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg: point, Data: [[ 31 292]\n",
      " [ 18 623]\n",
      " [ 53 237]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 31 292]\n",
      " [ 18 623]\n",
      " [ 53 237]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6971\n",
      "Image 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg: point, Data: [[218 121]\n",
      " [153 104]\n",
      " [188 121]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[218 121]\n",
      " [153 104]\n",
      " [188 121]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7053\n",
      "Image IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg: point, Data: [[444  70]\n",
      " [ 35 300]\n",
      " [306 214]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[444  70]\n",
      " [ 35 300]\n",
      " [306 214]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7127\n",
      "Image Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg: point, Data: [[228 426]\n",
      " [276 401]\n",
      " [306 381]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[228 426]\n",
      " [276 401]\n",
      " [306 381]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6895\n",
      "Image 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg: box, Data: [[123 180 639 335]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[123 180 639 335]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6348\n",
      "Image stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg: point, Data: [[400 480]\n",
      " [499 172]\n",
      " [220 320]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[400 480]\n",
      " [499 172]\n",
      " [220 320]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9027\n",
      "Image images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg: point, Data: [[314 303]\n",
      " [254 208]\n",
      " [308 190]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[314 303]\n",
      " [254 208]\n",
      " [308 190]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6887\n",
      "Image images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg: point, Data: [[415 370]\n",
      " [451 354]\n",
      " [440 289]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[415 370]\n",
      " [451 354]\n",
      " [440 289]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6863\n",
      "Image Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg: box, Data: [[  0   0 638 580]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 638 580]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7183\n",
      "Image 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg: box, Data: [[146   0 179  68]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[146   0 179  68]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6922\n",
      "Image 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg: box, Data: [[ 66   1 577 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 66   1 577 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6022\n",
      "Image PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg: box, Data: [[ 85 127 573 576]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 85 127 573 576]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5823\n",
      "Image old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg: box, Data: [[187 180 316 417]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[187 180 316 417]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6876\n",
      "Image 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg: box, Data: [[ 74 311 439 491]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 74 311 439 491]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6933\n",
      "Image 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg: point, Data: [[180 167]\n",
      " [247  60]\n",
      " [270 167]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[180 167]\n",
      " [247  60]\n",
      " [270 167]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8966\n",
      "Image 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg: point, Data: [[454 354]\n",
      " [485 403]\n",
      " [170 587]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[454 354]\n",
      " [485 403]\n",
      " [170 587]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7964\n",
      "Image lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg: point, Data: [[282 105]\n",
      " [272 108]\n",
      " [255  94]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[282 105]\n",
      " [272 108]\n",
      " [255  94]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6911\n",
      "Image 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg: point, Data: [[107  42]\n",
      " [116 123]\n",
      " [133 129]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[107  42]\n",
      " [116 123]\n",
      " [133 129]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7018\n",
      "Image 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg: box, Data: [[  0 102 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0 102 639 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5382\n",
      "Image 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg: box, Data: [[  0 173 636 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0 173 636 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6815\n",
      "Image weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg: point, Data: [[301 425]\n",
      " [312 598]\n",
      " [296  41]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[301 425]\n",
      " [312 598]\n",
      " [296  41]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6956\n",
      "Image crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg: box, Data: [[156  37 556 414]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[156  37 556 414]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6602\n",
      "Image 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg: point, Data: [[ 89 308]\n",
      " [ 54 636]\n",
      " [147 566]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 89 308]\n",
      " [ 54 636]\n",
      " [147 566]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6860\n",
      "Image 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg: box, Data: [[ 35  93 301 576]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 35  93 301 576]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6934\n",
      "Image 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg: box, Data: [[  1   0 639 598]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  1   0 639 598]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9833\n",
      "Image 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg: point, Data: [[470 385]\n",
      " [382 108]\n",
      " [410 371]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[470 385]\n",
      " [382 108]\n",
      " [410 371]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8245\n",
      "Image 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg: point, Data: [[531  87]\n",
      " [285 164]\n",
      " [439 225]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[531  87]\n",
      " [285 164]\n",
      " [439 225]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1172\n",
      "Image Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg: point, Data: [[528 538]\n",
      " [509 512]\n",
      " [488 549]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[528 538]\n",
      " [509 512]\n",
      " [488 549]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6901\n",
      "Image 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg: point, Data: [[470 495]\n",
      " [ 34 277]\n",
      " [146 135]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[470 495]\n",
      " [ 34 277]\n",
      " [146 135]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7909\n",
      "Image 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg: box, Data: [[103  35 147 169]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[103  35 147 169]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6899\n",
      "Image 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg: point, Data: [[307 365]\n",
      " [396 341]\n",
      " [370 313]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[307 365]\n",
      " [396 341]\n",
      " [370 313]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6824\n",
      "Image 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg: point, Data: [[593  81]\n",
      " [596 110]\n",
      " [592 172]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[593  81]\n",
      " [596 110]\n",
      " [592 172]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6889\n",
      "Image 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg: box, Data: [[  0  54 631 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0  54 631 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8243\n",
      "Image 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg: box, Data: [[  0   0 512 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 512 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5652\n",
      "Image 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg: box, Data: [[  0 228 390 626]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0 228 390 626]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6166\n",
      "Image 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg: box, Data: [[ 51   0 612 395]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 51   0 612 395]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6981\n",
      "Image 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg: point, Data: [[ 98 192]\n",
      " [111  73]\n",
      " [167 444]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 98 192]\n",
      " [111  73]\n",
      " [167 444]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9874\n",
      "Image 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg: box, Data: [[  4 415 338 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  4 415 338 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6889\n",
      "Image 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg: box, Data: [[103   0 310 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[103   0 310 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6858\n",
      "Image 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg: point, Data: [[287 136]\n",
      " [296  84]\n",
      " [302 105]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[287 136]\n",
      " [296  84]\n",
      " [302 105]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6923\n",
      "Image 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg: point, Data: [[202 496]\n",
      " [256 523]\n",
      " [172 566]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[202 496]\n",
      " [256 523]\n",
      " [172 566]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6908\n",
      "Epoch 4/10, Loss: 0.7638\n",
      "Saving checkpoint: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\checkpoints\\sam2_epoch_4.pt\n",
      "Successfully saved checkpoint: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\checkpoints\\sam2_epoch_4.pt\n",
      "Image 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg: box, Data: [[ 13 256 359 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 13 256 359 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6799\n",
      "Image 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg: point, Data: [[110 115]\n",
      " [145  26]\n",
      " [ 16  60]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[110 115]\n",
      " [145  26]\n",
      " [ 16  60]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6726\n",
      "Image 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg: box, Data: [[ 68   2 172 422]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 68   2 172 422]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6893\n",
      "Image 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg: box, Data: [[  0  10 629 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0  10 629 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9787\n",
      "Image images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg: box, Data: [[ 65   0 513 417]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 65   0 513 417]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6888\n",
      "Image 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg: point, Data: [[182 453]\n",
      " [568 539]\n",
      " [530 434]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[182 453]\n",
      " [568 539]\n",
      " [530 434]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5972\n",
      "Image Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg: point, Data: [[285 259]\n",
      " [217 316]\n",
      " [535 286]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[285 259]\n",
      " [217 316]\n",
      " [535 286]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1290\n",
      "Image 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg: point, Data: [[594 346]\n",
      " [634 456]\n",
      " [593 368]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[594 346]\n",
      " [634 456]\n",
      " [593 368]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0750\n",
      "Image d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg: point, Data: [[221  74]\n",
      " [361 317]\n",
      " [375 367]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[221  74]\n",
      " [361 317]\n",
      " [375 367]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6871\n",
      "Image 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg: box, Data: [[131 303 197 534]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[131 303 197 534]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6906\n",
      "Image 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg: box, Data: [[114   0 382 496]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[114   0 382 496]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7025\n",
      "Image 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg: point, Data: [[341  95]\n",
      " [410 258]\n",
      " [ 14 151]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[341  95]\n",
      " [410 258]\n",
      " [ 14 151]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7768\n",
      "Image 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg: point, Data: [[257 117]\n",
      " [ 72 202]\n",
      " [500 275]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[257 117]\n",
      " [ 72 202]\n",
      " [500 275]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1602\n",
      "Image 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg: box, Data: [[  0   0 507 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 507 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9251\n",
      "Image 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg: point, Data: [[178 592]\n",
      " [110 131]\n",
      " [218 110]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[178 592]\n",
      " [110 131]\n",
      " [218 110]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2381\n",
      "Image 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg: point, Data: [[312   9]\n",
      " [320  46]\n",
      " [336  81]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[312   9]\n",
      " [320  46]\n",
      " [336  81]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6916\n",
      "Image 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg: box, Data: [[ 36  23 116 577]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 36  23 116 577]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6919\n",
      "Image 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg: box, Data: [[  0 102 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0 102 639 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5382\n",
      "Image 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg: point, Data: [[517 477]\n",
      " [447  19]\n",
      " [430 395]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[517 477]\n",
      " [447  19]\n",
      " [430 395]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2723\n",
      "Image 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg: box, Data: [[  3 207 639 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  3 207 639 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6869\n",
      "Image 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg: point, Data: [[276  89]\n",
      " [139 538]\n",
      " [466 364]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[276  89]\n",
      " [139 538]\n",
      " [466 364]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2849\n",
      "Image concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg: box, Data: [[140 120 495 503]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[140 120 495 503]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6378\n",
      "Image 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg: point, Data: [[111  56]\n",
      " [123 151]\n",
      " [131 109]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[111  56]\n",
      " [123 151]\n",
      " [131 109]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6901\n",
      "Image 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg: point, Data: [[592 531]\n",
      " [563 541]\n",
      " [575 521]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[592 531]\n",
      " [563 541]\n",
      " [575 521]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6909\n",
      "Image 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg: box, Data: [[  4 415 338 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  4 415 338 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6889\n",
      "Image 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg: box, Data: [[146 374 639 636]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[146 374 639 636]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6862\n",
      "Image 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg: point, Data: [[445 615]\n",
      " [492 449]\n",
      " [503 273]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[445 615]\n",
      " [492 449]\n",
      " [503 273]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7497\n",
      "Image 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg: box, Data: [[ 11  46 232 248]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 11  46 232 248]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6904\n",
      "Image 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg: box, Data: [[  0 128 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0 128 639 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6848\n",
      "Image 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg: point, Data: [[ 99   6]\n",
      " [ 49 122]\n",
      " [181  23]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 99   6]\n",
      " [ 49 122]\n",
      " [181  23]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6776\n",
      "Image patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg: point, Data: [[310 452]\n",
      " [371 313]\n",
      " [330 306]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[310 452]\n",
      " [371 313]\n",
      " [330 306]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6850\n",
      "Image old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg: point, Data: [[210 288]\n",
      " [206 374]\n",
      " [243 381]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[210 288]\n",
      " [206 374]\n",
      " [243 381]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6892\n",
      "Image 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg: point, Data: [[322  22]\n",
      " [357 160]\n",
      " [ 84 282]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[322  22]\n",
      " [357 160]\n",
      " [ 84 282]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2114\n",
      "Image dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg: point, Data: [[187 497]\n",
      " [574 108]\n",
      " [611 552]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[187 497]\n",
      " [574 108]\n",
      " [611 552]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2844\n",
      "Image Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg: point, Data: [[367 477]\n",
      " [429 370]\n",
      " [330 449]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[367 477]\n",
      " [429 370]\n",
      " [330 449]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6536\n",
      "Image 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg: point, Data: [[580 457]\n",
      " [605 446]\n",
      " [582 539]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[580 457]\n",
      " [605 446]\n",
      " [582 539]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6930\n",
      "Image PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg: box, Data: [[ 85 127 573 576]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 85 127 573 576]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5823\n",
      "Image 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg: point, Data: [[269 229]\n",
      " [407 217]\n",
      " [347 196]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[269 229]\n",
      " [407 217]\n",
      " [347 196]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7785\n",
      "Image 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg: box, Data: [[ 95 298 151 401]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 95 298 151 401]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6921\n",
      "Image images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg: box, Data: [[  7  98 587 583]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  7  98 587 583]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6766\n",
      "Image 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg: point, Data: [[ 38 320]\n",
      " [582 421]\n",
      " [256 364]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 38 320]\n",
      " [582 421]\n",
      " [256 364]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7869\n",
      "Image crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg: box, Data: [[156  37 556 414]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[156  37 556 414]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6602\n",
      "Image 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg: box, Data: [[280   0 637 424]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[280   0 637 424]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6832\n",
      "Image 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg: point, Data: [[226 156]\n",
      " [ 35 214]\n",
      " [167 256]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[226 156]\n",
      " [ 35 214]\n",
      " [167 256]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1380\n",
      "Image 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg: box, Data: [[ 12   0 498 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 12   0 498 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7543\n",
      "Image 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg: point, Data: [[479 122]\n",
      " [594 100]\n",
      " [623  45]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[479 122]\n",
      " [594 100]\n",
      " [623  45]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6849\n",
      "Image 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg: box, Data: [[167 345 192 392]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[167 345 192 392]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6927\n",
      "Image images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg: point, Data: [[459 466]\n",
      " [406 482]\n",
      " [413 461]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[459 466]\n",
      " [406 482]\n",
      " [413 461]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6647\n",
      "Image 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg: point, Data: [[481 185]\n",
      " [247 399]\n",
      " [439 526]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[481 185]\n",
      " [247 399]\n",
      " [439 526]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8042\n",
      "Image IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg: point, Data: [[577 273]\n",
      " [357 159]\n",
      " [521  63]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[577 273]\n",
      " [357 159]\n",
      " [521  63]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7118\n",
      "Image 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg: box, Data: [[522   4 559 172]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[522   4 559 172]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6911\n",
      "Image 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg: point, Data: [[466 489]\n",
      " [523 565]\n",
      " [424 376]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[466 489]\n",
      " [523 565]\n",
      " [424 376]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6989\n",
      "Image efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg: box, Data: [[  0   0 638 520]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 638 520]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5794\n",
      "Image 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg: point, Data: [[568 101]\n",
      " [301 207]\n",
      " [379 348]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[568 101]\n",
      " [301 207]\n",
      " [379 348]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0455\n",
      "Image 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg: point, Data: [[ 82 152]\n",
      " [ 71 214]\n",
      " [ 14 145]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 82 152]\n",
      " [ 71 214]\n",
      " [ 14 145]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2219\n",
      "Image 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg: point, Data: [[349 457]\n",
      " [ 21 406]\n",
      " [229 459]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[349 457]\n",
      " [ 21 406]\n",
      " [229 459]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8856\n",
      "Image 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg: point, Data: [[ 61 633]\n",
      " [ 98 636]\n",
      " [ 64 591]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 61 633]\n",
      " [ 98 636]\n",
      " [ 64 591]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6894\n",
      "Image lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg: box, Data: [[242  65 439 419]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[242  65 439 419]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6908\n",
      "Image 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg: point, Data: [[571 216]\n",
      " [562 249]\n",
      " [552  91]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[571 216]\n",
      " [562 249]\n",
      " [552  91]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6909\n",
      "Image stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg: point, Data: [[262 582]\n",
      " [162 360]\n",
      " [417 329]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[262 582]\n",
      " [162 360]\n",
      " [417 329]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6824\n",
      "Image 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg: box, Data: [[  0  22 639 492]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0  22 639 492]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8514\n",
      "Image 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg: box, Data: [[178 146 428 462]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[178 146 428 462]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6553\n",
      "Image card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg: point, Data: [[367  90]\n",
      " [484 166]\n",
      " [429 369]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[367  90]\n",
      " [484 166]\n",
      " [429 369]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9954\n",
      "Image images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg: point, Data: [[310 183]\n",
      " [448 248]\n",
      " [243 285]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[310 183]\n",
      " [448 248]\n",
      " [243 285]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7376\n",
      "Image 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg: point, Data: [[138   9]\n",
      " [ 58  43]\n",
      " [135 124]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[138   9]\n",
      " [ 58  43]\n",
      " [135 124]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6885\n",
      "Image Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg: box, Data: [[  0   0 638 638]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 638 638]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7462\n",
      "Image 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg: point, Data: [[401  44]\n",
      " [440  65]\n",
      " [444 203]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[401  44]\n",
      " [440  65]\n",
      " [444 203]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6909\n",
      "Image Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg: point, Data: [[631 247]\n",
      " [132 513]\n",
      " [399 232]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[631 247]\n",
      " [132 513]\n",
      " [399 232]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7827\n",
      "Image 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg: point, Data: [[306 142]\n",
      " [240  32]\n",
      " [202 234]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[306 142]\n",
      " [240  32]\n",
      " [202 234]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9639\n",
      "Image 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg: point, Data: [[482 189]\n",
      " [436 164]\n",
      " [455 177]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[482 189]\n",
      " [436 164]\n",
      " [455 177]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6937\n",
      "Image How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg: box, Data: [[177 129 345 494]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[177 129 345 494]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6565\n",
      "Image 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg: box, Data: [[  0   0 523 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 523 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6861\n",
      "Image 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg: point, Data: [[149 540]\n",
      " [  6 420]\n",
      " [175 570]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[149 540]\n",
      " [  6 420]\n",
      " [175 570]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7483\n",
      "Image 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg: box, Data: [[278 218 428 414]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[278 218 428 414]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6804\n",
      "Image what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg: point, Data: [[298  82]\n",
      " [278  87]\n",
      " [326 356]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[298  82]\n",
      " [278  87]\n",
      " [326 356]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6762\n",
      "Image Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg: point, Data: [[340 306]\n",
      " [334 431]\n",
      " [321 266]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[340 306]\n",
      " [334 431]\n",
      " [321 266]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6687\n",
      "Image 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg: box, Data: [[125   0 506 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[125   0 506 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6912\n",
      "Image 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg: box, Data: [[  0   0 512 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 512 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5652\n",
      "Image 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg: box, Data: [[ 22 417 104 521]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 22 417 104 521]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6905\n",
      "Image 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg: box, Data: [[  1   0 639 598]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  1   0 639 598]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9833\n",
      "Image 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg: box, Data: [[282  68 312 173]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[282  68 312 173]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6918\n",
      "Image images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg: box, Data: [[ 84  38 319 507]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 84  38 319 507]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6351\n",
      "Image 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg: point, Data: [[155 539]\n",
      " [180 630]\n",
      " [ 98 516]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[155 539]\n",
      " [180 630]\n",
      " [ 98 516]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6898\n",
      "Image concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg: box, Data: [[285 131 358 417]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[285 131 358 417]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6902\n",
      "Image 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg: box, Data: [[  6  10 632 510]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  6  10 632 510]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6970\n",
      "Image weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg: box, Data: [[283   0 352 638]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[283   0 352 638]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6919\n",
      "Image Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg: box, Data: [[  0   0 636 612]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 636 612]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2551\n",
      "Image 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg: box, Data: [[ 33   0 615 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 33   0 615 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6859\n",
      "Image 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg: point, Data: [[ 42 348]\n",
      " [131 320]\n",
      " [298 361]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 42 348]\n",
      " [131 320]\n",
      " [298 361]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6367\n",
      "Image 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg: box, Data: [[  4 466 599 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  4 466 599 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6899\n",
      "Image 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg: point, Data: [[171  66]\n",
      " [178  68]\n",
      " [161  44]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[171  66]\n",
      " [178  68]\n",
      " [161  44]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6929\n",
      "Image 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg: point, Data: [[415 480]\n",
      " [ 94 365]\n",
      " [ 82 347]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[415 480]\n",
      " [ 94 365]\n",
      " [ 82 347]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2873\n",
      "Image 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg: point, Data: [[564 115]\n",
      " [601 160]\n",
      " [601 183]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[564 115]\n",
      " [601 160]\n",
      " [601 183]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6900\n",
      "Image 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg: point, Data: [[132 103]\n",
      " [134 136]\n",
      " [129  36]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[132 103]\n",
      " [134 136]\n",
      " [129  36]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0606\n",
      "Image 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg: point, Data: [[472 445]\n",
      " [536  79]\n",
      " [503 566]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[472 445]\n",
      " [536  79]\n",
      " [503 566]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2192\n",
      "Image 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg: point, Data: [[183 373]\n",
      " [182 279]\n",
      " [ 44 162]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[183 373]\n",
      " [182 279]\n",
      " [ 44 162]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9641\n",
      "Image 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg: box, Data: [[  2 387 412 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  2 387 412 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6886\n",
      "Image 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg: box, Data: [[ 66   1 577 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 66   1 577 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6022\n",
      "Image old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg: box, Data: [[126 118 231 630]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[126 118 231 630]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6709\n",
      "Image 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg: point, Data: [[500  46]\n",
      " [240 128]\n",
      " [541 186]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[500  46]\n",
      " [240 128]\n",
      " [541 186]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7465\n",
      "Image CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg: box, Data: [[ 76  25 310 448]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 76  25 310 448]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6410\n",
      "Image 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg: point, Data: [[ 61 187]\n",
      " [291 570]\n",
      " [ 61 209]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 61 187]\n",
      " [291 570]\n",
      " [ 61 209]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2577\n",
      "Image 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg: point, Data: [[ 66  41]\n",
      " [171  81]\n",
      " [ 83  57]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 66  41]\n",
      " [171  81]\n",
      " [ 83  57]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1019\n",
      "Image 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg: point, Data: [[572 547]\n",
      " [ 33 268]\n",
      " [306 155]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[572 547]\n",
      " [ 33 268]\n",
      " [306 155]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2233\n",
      "Image 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg: point, Data: [[418 630]\n",
      " [614 595]\n",
      " [339 483]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[418 630]\n",
      " [614 595]\n",
      " [339 483]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1040\n",
      "Image 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg: point, Data: [[276  90]\n",
      " [613 263]\n",
      " [297  28]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[276  90]\n",
      " [613 263]\n",
      " [297  28]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7101\n",
      "Image 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg: box, Data: [[  0  54 631 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0  54 631 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8243\n",
      "Image 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg: point, Data: [[408 235]\n",
      " [105 148]\n",
      " [417 151]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[408 235]\n",
      " [105 148]\n",
      " [417 151]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0137\n",
      "Image 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg: box, Data: [[127 101 260 134]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[127 101 260 134]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6922\n",
      "Image Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg: box, Data: [[146 160 323 442]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[146 160 323 442]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6613\n",
      "Image 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg: box, Data: [[231 374 533 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[231 374 533 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6820\n",
      "Image 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg: box, Data: [[258   0 542 573]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[258   0 542 573]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6866\n",
      "Image concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg: box, Data: [[336 345 373 409]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[336 345 373 409]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6920\n",
      "Image 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg: box, Data: [[  0   0 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 639 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7416\n",
      "Image Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg: point, Data: [[506 489]\n",
      " [503 554]\n",
      " [493 516]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[506 489]\n",
      " [503 554]\n",
      " [493 516]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6902\n",
      "Image 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg: box, Data: [[  3   4 636 565]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  3   4 636 565]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6830\n",
      "Image istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg: box, Data: [[241 201 457 434]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[241 201 457 434]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6626\n",
      "Image 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg: point, Data: [[218  34]\n",
      " [ 92 570]\n",
      " [188 624]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[218  34]\n",
      " [ 92 570]\n",
      " [188 624]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9583\n",
      "Image 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg: point, Data: [[288 205]\n",
      " [342 328]\n",
      " [304 166]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[288 205]\n",
      " [342 328]\n",
      " [304 166]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8721\n",
      "Image 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg: box, Data: [[ 66 156 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 66 156 639 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8246\n",
      "Image 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg: box, Data: [[285 188 327 397]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[285 188 327 397]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6912\n",
      "Image Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg: point, Data: [[386  86]\n",
      " [210 587]\n",
      " [364  78]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[386  86]\n",
      " [210 587]\n",
      " [364  78]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6878\n",
      "Image images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg: point, Data: [[124 204]\n",
      " [308 237]\n",
      " [382 227]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[124 204]\n",
      " [308 237]\n",
      " [382 227]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6620\n",
      "Image 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg: point, Data: [[167 119]\n",
      " [192  36]\n",
      " [196  66]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[167 119]\n",
      " [192  36]\n",
      " [196  66]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6959\n",
      "Image 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg: box, Data: [[141  46 615 339]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[141  46 615 339]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7786\n",
      "Image 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg: point, Data: [[496 481]\n",
      " [192 302]\n",
      " [  2 320]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[496 481]\n",
      " [192 302]\n",
      " [  2 320]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0117\n",
      "Image 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg: point, Data: [[128 461]\n",
      " [189 518]\n",
      " [113 537]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[128 461]\n",
      " [189 518]\n",
      " [113 537]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7012\n",
      "Image istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg: point, Data: [[440 225]\n",
      " [270 291]\n",
      " [391 181]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[440 225]\n",
      " [270 291]\n",
      " [391 181]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6572\n",
      "Image 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg: point, Data: [[344 353]\n",
      " [401 319]\n",
      " [361 305]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[344 353]\n",
      " [401 319]\n",
      " [361 305]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6745\n",
      "Image 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg: point, Data: [[ 65 381]\n",
      " [ 60 409]\n",
      " [ 68 385]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 65 381]\n",
      " [ 60 409]\n",
      " [ 68 385]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6924\n",
      "Image 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg: point, Data: [[622  93]\n",
      " [290 373]\n",
      " [295 490]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[622  93]\n",
      " [290 373]\n",
      " [295 490]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7011\n",
      "Image 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg: box, Data: [[  2 217 521 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  2 217 521 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6852\n",
      "Image 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg: point, Data: [[374 463]\n",
      " [392 547]\n",
      " [403 551]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[374 463]\n",
      " [392 547]\n",
      " [403 551]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6894\n",
      "Image 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg: point, Data: [[310 438]\n",
      " [293 532]\n",
      " [149  57]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[310 438]\n",
      " [293 532]\n",
      " [149  57]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1754\n",
      "Epoch 5/10, Loss: 0.7875\n",
      "Saving checkpoint: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\checkpoints\\sam2_epoch_5.pt\n",
      "Successfully saved checkpoint: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\checkpoints\\sam2_epoch_5.pt\n",
      "Image efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg: point, Data: [[259 357]\n",
      " [318 433]\n",
      " [273 398]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[259 357]\n",
      " [318 433]\n",
      " [273 398]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5887\n",
      "Image concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg: box, Data: [[140 120 495 503]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[140 120 495 503]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6378\n",
      "Image CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg: point, Data: [[270 221]\n",
      " [108  52]\n",
      " [120  53]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[270 221]\n",
      " [108  52]\n",
      " [120  53]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6429\n",
      "Image 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg: point, Data: [[183 458]\n",
      " [363 391]\n",
      " [232 295]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[183 458]\n",
      " [363 391]\n",
      " [232 295]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6780\n",
      "Image 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg: box, Data: [[258   0 542 573]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[258   0 542 573]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6866\n",
      "Image 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg: box, Data: [[ 68   2 172 422]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 68   2 172 422]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6893\n",
      "Image 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg: point, Data: [[480 373]\n",
      " [175 542]\n",
      " [173 556]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[480 373]\n",
      " [175 542]\n",
      " [173 556]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8283\n",
      "Image 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg: point, Data: [[561 117]\n",
      " [ 89  28]\n",
      " [ 29 161]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[561 117]\n",
      " [ 89  28]\n",
      " [ 29 161]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6505\n",
      "Image stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg: box, Data: [[ 17  73 524 595]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 17  73 524 595]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6681\n",
      "Image concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg: point, Data: [[340 376]\n",
      " [371 352]\n",
      " [360 386]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[340 376]\n",
      " [371 352]\n",
      " [360 386]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6919\n",
      "Image 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg: box, Data: [[405   0 518 480]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[405   0 518 480]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6924\n",
      "Image dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg: box, Data: [[ 80   0 637 609]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 80   0 637 609]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8884\n",
      "Image 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg: point, Data: [[156  14]\n",
      " [163  15]\n",
      " [168  11]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[156  14]\n",
      " [163  15]\n",
      " [168  11]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6929\n",
      "Image 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg: point, Data: [[474 183]\n",
      " [481 186]\n",
      " [434 167]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[474 183]\n",
      " [481 186]\n",
      " [434 167]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6941\n",
      "Image 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg: point, Data: [[284 505]\n",
      " [155 630]\n",
      " [255  50]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[284 505]\n",
      " [155 630]\n",
      " [255  50]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9921\n",
      "Image 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg: point, Data: [[ 71 156]\n",
      " [ 23 536]\n",
      " [  1 332]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 71 156]\n",
      " [ 23 536]\n",
      " [  1 332]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7303\n",
      "Image 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg: box, Data: [[  0   0 512 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 512 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5652\n",
      "Image 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg: box, Data: [[103  35 147 169]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[103  35 147 169]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6899\n",
      "Image 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg: point, Data: [[237 616]\n",
      " [235 469]\n",
      " [111 543]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[237 616]\n",
      " [235 469]\n",
      " [111 543]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6774\n",
      "Image How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg: box, Data: [[177 129 345 494]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[177 129 345 494]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6565\n",
      "Image Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg: point, Data: [[323 291]\n",
      " [174 480]\n",
      " [435 241]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[323 291]\n",
      " [174 480]\n",
      " [435 241]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6927\n",
      "Image 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg: box, Data: [[ 76   0 257 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 76   0 257 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6896\n",
      "Image images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg: box, Data: [[306 309 471 583]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[306 309 471 583]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6653\n",
      "Image 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg: box, Data: [[ 66 156 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 66 156 639 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8246\n",
      "Image 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg: point, Data: [[451 122]\n",
      " [423 184]\n",
      " [473  22]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[451 122]\n",
      " [423 184]\n",
      " [473  22]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2597\n",
      "Image 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg: point, Data: [[ 57 176]\n",
      " [234 471]\n",
      " [238 395]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 57 176]\n",
      " [234 471]\n",
      " [238 395]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0060\n",
      "Image 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg: box, Data: [[283   0 637 229]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[283   0 637 229]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6877\n",
      "Image 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg: box, Data: [[  0  54 631 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0  54 631 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8243\n",
      "Image 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg: box, Data: [[299   0 628 635]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[299   0 628 635]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6920\n",
      "Image 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg: point, Data: [[478 578]\n",
      " [515 494]\n",
      " [501 479]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[478 578]\n",
      " [515 494]\n",
      " [501 479]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8550\n",
      "Image Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg: box, Data: [[198   0 638 607]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[198   0 638 607]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6901\n",
      "Image patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg: box, Data: [[268 209 406 474]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[268 209 406 474]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6824\n",
      "Image Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg: point, Data: [[141 444]\n",
      " [  9 495]\n",
      " [141 437]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[141 444]\n",
      " [  9 495]\n",
      " [141 437]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6930\n",
      "Image 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg: box, Data: [[ 63  32 639 467]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 63  32 639 467]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6953\n",
      "Image 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg: point, Data: [[213 192]\n",
      " [383  98]\n",
      " [483 458]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[213 192]\n",
      " [383  98]\n",
      " [483 458]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9412\n",
      "Image 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg: box, Data: [[227  16 589 216]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[227  16 589 216]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6939\n",
      "Image 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg: point, Data: [[ 52  16]\n",
      " [ 88 150]\n",
      " [ 85 104]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 52  16]\n",
      " [ 88 150]\n",
      " [ 85 104]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2292\n",
      "Image 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg: point, Data: [[125 558]\n",
      " [119 604]\n",
      " [165 523]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[125 558]\n",
      " [119 604]\n",
      " [165 523]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6871\n",
      "Image Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg: point, Data: [[411 512]\n",
      " [252 440]\n",
      " [319 366]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[411 512]\n",
      " [252 440]\n",
      " [319 366]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6423\n",
      "Image 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg: box, Data: [[288   0 637 495]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[288   0 637 495]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6927\n",
      "Image 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg: box, Data: [[ 33   0 615 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 33   0 615 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6859\n",
      "Image 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg: point, Data: [[469 193]\n",
      " [558 547]\n",
      " [408 233]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[469 193]\n",
      " [558 547]\n",
      " [408 233]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6830\n",
      "Image 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg: box, Data: [[  3 207 639 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  3 207 639 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6869\n",
      "Image 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg: box, Data: [[ 13 256 359 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 13 256 359 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6799\n",
      "Image 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg: point, Data: [[531 227]\n",
      " [298 201]\n",
      " [413 141]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[531 227]\n",
      " [298 201]\n",
      " [413 141]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0470\n",
      "Image 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg: point, Data: [[295 138]\n",
      " [289 113]\n",
      " [287 140]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[295 138]\n",
      " [289 113]\n",
      " [287 140]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6923\n",
      "Image 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg: point, Data: [[ 37 469]\n",
      " [ 69 512]\n",
      " [101 454]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 37 469]\n",
      " [ 69 512]\n",
      " [101 454]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6895\n",
      "Image 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg: box, Data: [[  1   0 639 304]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  1   0 639 304]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6856\n",
      "Image images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg: point, Data: [[258 146]\n",
      " [219 110]\n",
      " [164 462]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[258 146]\n",
      " [219 110]\n",
      " [164 462]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6418\n",
      "Image 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg: box, Data: [[  0   0 507 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 507 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9251\n",
      "Image 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg: point, Data: [[ 33 380]\n",
      " [205 546]\n",
      " [231 350]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 33 380]\n",
      " [205 546]\n",
      " [231 350]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6196\n",
      "Image 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg: box, Data: [[ 36  23 116 577]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 36  23 116 577]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6919\n",
      "Image 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg: point, Data: [[171 130]\n",
      " [170 122]\n",
      " [223 117]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[171 130]\n",
      " [170 122]\n",
      " [223 117]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6935\n",
      "Image 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg: point, Data: [[590 294]\n",
      " [630 270]\n",
      " [604 146]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[590 294]\n",
      " [630 270]\n",
      " [604 146]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6935\n",
      "Image Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg: box, Data: [[  0   0 610 638]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 610 638]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7143\n",
      "Image istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg: point, Data: [[392 337]\n",
      " [270 272]\n",
      " [269 320]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[392 337]\n",
      " [270 272]\n",
      " [269 320]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6628\n",
      "Image PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg: point, Data: [[494 509]\n",
      " [264 169]\n",
      " [417 312]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[494 509]\n",
      " [264 169]\n",
      " [417 312]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6762\n",
      "Image 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg: box, Data: [[ 12   0 498 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 12   0 498 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7543\n",
      "Image 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg: box, Data: [[ 63 349 461 547]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 63 349 461 547]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6904\n",
      "Image 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg: box, Data: [[  0   4 363 518]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   4 363 518]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7005\n",
      "Image 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg: point, Data: [[571 569]\n",
      " [294 350]\n",
      " [550 350]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[571 569]\n",
      " [294 350]\n",
      " [550 350]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5967\n",
      "Image 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg: point, Data: [[446 622]\n",
      " [517 610]\n",
      " [545 468]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[446 622]\n",
      " [517 610]\n",
      " [545 468]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6811\n",
      "Image 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg: point, Data: [[175 406]\n",
      " [151 322]\n",
      " [226 136]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[175 406]\n",
      " [151 322]\n",
      " [226 136]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1877\n",
      "Image IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg: box, Data: [[  0   0 631 345]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 631 345]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6472\n",
      "Image 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg: point, Data: [[186 425]\n",
      " [266 383]\n",
      " [333 578]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[186 425]\n",
      " [266 383]\n",
      " [333 578]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6814\n",
      "Image 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg: point, Data: [[605 259]\n",
      " [610  39]\n",
      " [569   1]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[605 259]\n",
      " [610  39]\n",
      " [569   1]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9570\n",
      "Image card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg: box, Data: [[297  69 487 475]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[297  69 487 475]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6564\n",
      "Image 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg: box, Data: [[131 303 197 534]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[131 303 197 534]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6906\n",
      "Image 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg: point, Data: [[113 447]\n",
      " [ 24 351]\n",
      " [ 13 352]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[113 447]\n",
      " [ 24 351]\n",
      " [ 13 352]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9552\n",
      "Image 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg: box, Data: [[522   4 559 172]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[522   4 559 172]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6911\n",
      "Image 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg: box, Data: [[123 180 639 335]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[123 180 639 335]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6348\n",
      "Image Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg: point, Data: [[529 518]\n",
      " [484 491]\n",
      " [531 494]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[529 518]\n",
      " [484 491]\n",
      " [531 494]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6900\n",
      "Image 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg: box, Data: [[  0   1 414 318]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   1 414 318]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6890\n",
      "Image 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg: point, Data: [[224  42]\n",
      " [157 200]\n",
      " [149 478]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[224  42]\n",
      " [157 200]\n",
      " [149 478]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9149\n",
      "Image 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg: box, Data: [[ 11  46 232 248]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 11  46 232 248]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6904\n",
      "Image 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg: point, Data: [[568 317]\n",
      " [626 485]\n",
      " [630 531]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[568 317]\n",
      " [626 485]\n",
      " [630 531]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1887\n",
      "Image 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg: point, Data: [[ 39 216]\n",
      " [144  64]\n",
      " [154  47]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 39 216]\n",
      " [144  64]\n",
      " [154  47]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1363\n",
      "Image 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg: point, Data: [[484  78]\n",
      " [399 245]\n",
      " [412 208]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[484  78]\n",
      " [399 245]\n",
      " [412 208]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9463\n",
      "Image concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg: point, Data: [[323 179]\n",
      " [317 171]\n",
      " [353 169]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[323 179]\n",
      " [317 171]\n",
      " [353 169]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6908\n",
      "Image 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg: point, Data: [[639 328]\n",
      " [400 622]\n",
      " [356 480]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[639 328]\n",
      " [400 622]\n",
      " [356 480]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7420\n",
      "Image 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg: point, Data: [[462 190]\n",
      " [412 124]\n",
      " [352 304]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[462 190]\n",
      " [412 124]\n",
      " [352 304]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8029\n",
      "Image images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg: point, Data: [[554 523]\n",
      " [493 259]\n",
      " [410 223]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[554 523]\n",
      " [493 259]\n",
      " [410 223]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6838\n",
      "Image 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg: point, Data: [[ 23 432]\n",
      " [ 23 422]\n",
      " [175 523]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 23 432]\n",
      " [ 23 422]\n",
      " [175 523]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7550\n",
      "Image 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg: box, Data: [[ 48   0 176 178]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 48   0 176 178]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6918\n",
      "Image 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg: point, Data: [[149 509]\n",
      " [127 532]\n",
      " [206 339]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[149 509]\n",
      " [127 532]\n",
      " [206 339]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.4553\n",
      "Image 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg: box, Data: [[367 328 582 624]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[367 328 582 624]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6913\n",
      "Image 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg: box, Data: [[  6  10 632 510]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  6  10 632 510]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6970\n",
      "Image 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg: point, Data: [[174 130]\n",
      " [211  27]\n",
      " [207  43]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[174 130]\n",
      " [211  27]\n",
      " [207  43]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6909\n",
      "Image 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg: box, Data: [[ 99  22 139 139]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 99  22 139 139]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6916\n",
      "Image 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg: point, Data: [[308 408]\n",
      " [103  50]\n",
      " [263 415]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[308 408]\n",
      " [103  50]\n",
      " [263 415]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2386\n",
      "Image 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg: point, Data: [[188 379]\n",
      " [265 265]\n",
      " [124 305]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[188 379]\n",
      " [265 265]\n",
      " [124 305]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7953\n",
      "Image 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg: point, Data: [[518 403]\n",
      " [105 363]\n",
      " [176 113]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[518 403]\n",
      " [105 363]\n",
      " [176 113]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0129\n",
      "Image 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg: point, Data: [[530 460]\n",
      " [ 42 359]\n",
      " [ 93 200]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[530 460]\n",
      " [ 42 359]\n",
      " [ 93 200]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6143\n",
      "Image weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg: box, Data: [[283   0 352 638]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[283   0 352 638]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6919\n",
      "Image 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg: point, Data: [[145  44]\n",
      " [104 112]\n",
      " [ 73 139]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[145  44]\n",
      " [104 112]\n",
      " [ 73 139]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2690\n",
      "Image 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg: box, Data: [[  0 128 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0 128 639 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6848\n",
      "Image crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg: box, Data: [[156  37 556 414]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[156  37 556 414]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6602\n",
      "Image 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg: box, Data: [[285 188 327 397]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[285 188 327 397]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6912\n",
      "Image 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg: box, Data: [[ 74 311 439 491]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 74 311 439 491]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6933\n",
      "Image 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg: box, Data: [[539  79 576 272]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[539  79 576 272]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6914\n",
      "Image 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg: box, Data: [[301 424 432 572]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[301 424 432 572]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6914\n",
      "Image 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg: point, Data: [[417 265]\n",
      " [361 265]\n",
      " [426 246]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[417 265]\n",
      " [361 265]\n",
      " [426 246]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9562\n",
      "Image 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg: point, Data: [[188 380]\n",
      " [185 392]\n",
      " [178 374]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[188 380]\n",
      " [185 392]\n",
      " [178 374]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6926\n",
      "Image 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg: point, Data: [[525 541]\n",
      " [427 628]\n",
      " [309 523]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[525 541]\n",
      " [427 628]\n",
      " [309 523]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7852\n",
      "Image 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg: box, Data: [[579 438 607 539]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[579 438 607 539]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6920\n",
      "Image 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg: box, Data: [[  1 149 600 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  1 149 600 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9886\n",
      "Image 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg: box, Data: [[  0 173 636 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0 173 636 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6815\n",
      "Image Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg: box, Data: [[146 160 323 442]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[146 160 323 442]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6613\n",
      "Image istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg: box, Data: [[130 110 494 452]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[130 110 494 452]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6531\n",
      "Image 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg: box, Data: [[ 66   1 577 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 66   1 577 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6022\n",
      "Image images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg: box, Data: [[168 157 489 372]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[168 157 489 372]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6801\n",
      "Image what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg: box, Data: [[ 71   5 627 452]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 71   5 627 452]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6258\n",
      "Image 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg: box, Data: [[ 51   0 612 395]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 51   0 612 395]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6981\n",
      "Image 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg: point, Data: [[446 430]\n",
      " [175  76]\n",
      " [ 40  42]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[446 430]\n",
      " [175  76]\n",
      " [ 40  42]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8757\n",
      "Image 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg: box, Data: [[114   0 382 496]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[114   0 382 496]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7025\n",
      "Image Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg: box, Data: [[ 77 215 591 603]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 77 215 591 603]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6769\n",
      "Image 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg: point, Data: [[345 486]\n",
      " [ 53 491]\n",
      " [120 545]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[345 486]\n",
      " [ 53 491]\n",
      " [120 545]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2359\n",
      "Image 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg: box, Data: [[141  46 615 339]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[141  46 615 339]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7786\n",
      "Image d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg: point, Data: [[198 364]\n",
      " [226  54]\n",
      " [351 372]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[198 364]\n",
      " [226  54]\n",
      " [351 372]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6964\n",
      "Image 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg: box, Data: [[ 56 335  89 421]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 56 335  89 421]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6922\n",
      "Image 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg: point, Data: [[ 96 598]\n",
      " [ 60 588]\n",
      " [513 497]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 96 598]\n",
      " [ 60 588]\n",
      " [513 497]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6843\n",
      "Image 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg: point, Data: [[231 134]\n",
      " [195 109]\n",
      " [101 120]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[231 134]\n",
      " [195 109]\n",
      " [101 120]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1653\n",
      "Image lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg: box, Data: [[242  65 439 419]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[242  65 439 419]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6908\n",
      "Image old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg: box, Data: [[126 118 231 630]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[126 118 231 630]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6709\n",
      "Image 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg: box, Data: [[  1   0 639 598]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  1   0 639 598]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9833\n",
      "Image 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg: point, Data: [[ 81 564]\n",
      " [ 15 595]\n",
      " [ 68 583]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 81 564]\n",
      " [ 15 595]\n",
      " [ 68 583]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1444\n",
      "Image Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg: box, Data: [[  0   0 638 638]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 638 638]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7462\n",
      "Image 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg: box, Data: [[321 260 455 638]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[321 260 455 638]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6692\n",
      "Image 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg: box, Data: [[191   0 639 630]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[191   0 639 630]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6881\n",
      "Image old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg: point, Data: [[193 240]\n",
      " [221 361]\n",
      " [273 195]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[193 240]\n",
      " [221 361]\n",
      " [273 195]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6863\n",
      "Image images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg: box, Data: [[ 84  38 319 507]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 84  38 319 507]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6351\n",
      "Image images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg: point, Data: [[212  78]\n",
      " [510 400]\n",
      " [ 78  27]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[212  78]\n",
      " [510 400]\n",
      " [ 78  27]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2715\n",
      "Image 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg: point, Data: [[125 316]\n",
      " [113 312]\n",
      " [113 342]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[125 316]\n",
      " [113 312]\n",
      " [113 342]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7273\n",
      "Image 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg: point, Data: [[637 585]\n",
      " [605 607]\n",
      " [379 584]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[637 585]\n",
      " [605 607]\n",
      " [379 584]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6607\n",
      "Epoch 6/10, Loss: 0.7609\n",
      "Saving checkpoint: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\checkpoints\\sam2_epoch_6.pt\n",
      "Successfully saved checkpoint: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\checkpoints\\sam2_epoch_6.pt\n",
      "Image 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg: point, Data: [[178  64]\n",
      " [146  16]\n",
      " [176  41]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[178  64]\n",
      " [146  16]\n",
      " [176  41]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6923\n",
      "Image 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg: point, Data: [[263 454]\n",
      " [ 36 190]\n",
      " [227 336]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[263 454]\n",
      " [ 36 190]\n",
      " [227 336]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2696\n",
      "Image 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg: box, Data: [[ 11  46 232 248]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 11  46 232 248]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6904\n",
      "Image 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg: box, Data: [[  0 173 636 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0 173 636 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6815\n",
      "Image stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg: box, Data: [[ 17  73 524 595]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 17  73 524 595]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6681\n",
      "Image 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg: point, Data: [[500 525]\n",
      " [395 474]\n",
      " [478 441]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[500 525]\n",
      " [395 474]\n",
      " [478 441]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2093\n",
      "Image 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg: box, Data: [[ 63  32 639 467]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 63  32 639 467]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6953\n",
      "Image 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg: box, Data: [[114   0 382 496]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[114   0 382 496]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7025\n",
      "Image 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg: point, Data: [[ 48 114]\n",
      " [ 59  57]\n",
      " [ 48  93]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 48 114]\n",
      " [ 59  57]\n",
      " [ 48  93]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6899\n",
      "Image 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg: point, Data: [[214 472]\n",
      " [223 361]\n",
      " [400 376]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[214 472]\n",
      " [223 361]\n",
      " [400 376]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9731\n",
      "Image 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg: point, Data: [[588 625]\n",
      " [380 510]\n",
      " [544 581]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[588 625]\n",
      " [380 510]\n",
      " [544 581]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9474\n",
      "Image Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg: box, Data: [[198   0 638 607]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[198   0 638 607]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6901\n",
      "Image 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg: box, Data: [[  0  98 637 524]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0  98 637 524]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6829\n",
      "Image 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg: point, Data: [[401 339]\n",
      " [383 294]\n",
      " [397 306]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[401 339]\n",
      " [383 294]\n",
      " [397 306]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6740\n",
      "Image CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg: box, Data: [[ 76  25 310 448]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 76  25 310 448]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6410\n",
      "Image 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg: point, Data: [[292 125]\n",
      " [492 365]\n",
      " [282 122]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[292 125]\n",
      " [492 365]\n",
      " [282 122]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6921\n",
      "Image 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg: point, Data: [[492 171]\n",
      " [364 395]\n",
      " [313 159]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[492 171]\n",
      " [364 395]\n",
      " [313 159]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9502\n",
      "Image old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg: box, Data: [[126 118 231 630]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[126 118 231 630]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6709\n",
      "Image weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg: box, Data: [[283   0 352 638]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[283   0 352 638]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6919\n",
      "Image 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg: point, Data: [[478 118]\n",
      " [440  99]\n",
      " [426 129]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[478 118]\n",
      " [440  99]\n",
      " [426 129]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7213\n",
      "Image 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg: box, Data: [[  0 128 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0 128 639 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6848\n",
      "Image 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg: box, Data: [[301 424 432 572]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[301 424 432 572]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6914\n",
      "Image 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg: box, Data: [[288   0 637 495]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[288   0 637 495]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6927\n",
      "Image 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg: point, Data: [[ 57 597]\n",
      " [ 41  68]\n",
      " [ 51 466]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 57 597]\n",
      " [ 41  68]\n",
      " [ 51 466]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2290\n",
      "Image 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg: box, Data: [[  0   0 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 639 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7416\n",
      "Image 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg: point, Data: [[ 82 138]\n",
      " [ 83  48]\n",
      " [107 393]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 82 138]\n",
      " [ 83  48]\n",
      " [107 393]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1278\n",
      "Image 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg: box, Data: [[ 13 256 359 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 13 256 359 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6799\n",
      "Image 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg: box, Data: [[  0   4 363 518]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   4 363 518]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7005\n",
      "Image istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg: box, Data: [[241 201 457 434]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[241 201 457 434]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6626\n",
      "Image 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg: box, Data: [[141  46 615 339]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[141  46 615 339]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7786\n",
      "Image 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg: point, Data: [[144 424]\n",
      " [162 314]\n",
      " [145 387]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[144 424]\n",
      " [162 314]\n",
      " [145 387]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0551\n",
      "Image 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg: box, Data: [[  0 228 390 626]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0 228 390 626]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6166\n",
      "Image 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg: box, Data: [[416 148 555 225]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[416 148 555 225]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6933\n",
      "Image 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg: box, Data: [[  0   0 523 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 523 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6861\n",
      "Image 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg: point, Data: [[601 174]\n",
      " [594 168]\n",
      " [238  45]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[601 174]\n",
      " [594 168]\n",
      " [238  45]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8099\n",
      "Image 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg: point, Data: [[466 376]\n",
      " [471 469]\n",
      " [501 394]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[466 376]\n",
      " [471 469]\n",
      " [501 394]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6892\n",
      "Image images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg: box, Data: [[168 157 489 372]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[168 157 489 372]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6801\n",
      "Image concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg: point, Data: [[343 492]\n",
      " [353 226]\n",
      " [176 358]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[343 492]\n",
      " [353 226]\n",
      " [176 358]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6350\n",
      "Image 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg: box, Data: [[127 101 260 134]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[127 101 260 134]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6922\n",
      "Image 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg: box, Data: [[ 30  42 247 539]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 30  42 247 539]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6897\n",
      "Image 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg: box, Data: [[ 95 298 151 401]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 95 298 151 401]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6921\n",
      "Image 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg: box, Data: [[319 371 639 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[319 371 639 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6886\n",
      "Image images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg: point, Data: [[174 119]\n",
      " [163 125]\n",
      " [334  99]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[174 119]\n",
      " [163 125]\n",
      " [334  99]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6812\n",
      "Image 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg: point, Data: [[ 14 225]\n",
      " [555 490]\n",
      " [599 447]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 14 225]\n",
      " [555 490]\n",
      " [599 447]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0370\n",
      "Image 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg: point, Data: [[378 457]\n",
      " [237 528]\n",
      " [  5 137]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[378 457]\n",
      " [237 528]\n",
      " [  5 137]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6658\n",
      "Image card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg: point, Data: [[455 327]\n",
      " [323  96]\n",
      " [477 118]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[455 327]\n",
      " [323  96]\n",
      " [477 118]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9840\n",
      "Image istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg: point, Data: [[371 369]\n",
      " [330 321]\n",
      " [351 358]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[371 369]\n",
      " [330 321]\n",
      " [351 358]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6565\n",
      "Image 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg: box, Data: [[167 345 192 392]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[167 345 192 392]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6927\n",
      "Image images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg: point, Data: [[471 315]\n",
      " [497 355]\n",
      " [430 346]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[471 315]\n",
      " [497 355]\n",
      " [430 346]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6844\n",
      "Image Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg: box, Data: [[469 484 535 555]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[469 484 535 555]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6902\n",
      "Image 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg: point, Data: [[524  55]\n",
      " [515  34]\n",
      " [610 160]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[524  55]\n",
      " [515  34]\n",
      " [610 160]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2547\n",
      "Image 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg: box, Data: [[ 22 417 104 521]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 22 417 104 521]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6905\n",
      "Image 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg: box, Data: [[ 33   0 615 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 33   0 615 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6859\n",
      "Image 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg: box, Data: [[231 374 533 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[231 374 533 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6820\n",
      "Image 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg: point, Data: [[  8 409]\n",
      " [ 26 483]\n",
      " [ 45 428]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[  8 409]\n",
      " [ 26 483]\n",
      " [ 45 428]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7003\n",
      "Image 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg: box, Data: [[ 76   0 257 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 76   0 257 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6896\n",
      "Image 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg: point, Data: [[ 23 602]\n",
      " [ 32 620]\n",
      " [548 574]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 23 602]\n",
      " [ 32 620]\n",
      " [548 574]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6838\n",
      "Image 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg: point, Data: [[131 407]\n",
      " [223 581]\n",
      " [374 563]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[131 407]\n",
      " [223 581]\n",
      " [374 563]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5791\n",
      "Image 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg: point, Data: [[196 139]\n",
      " [194  94]\n",
      " [195 146]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[196 139]\n",
      " [194  94]\n",
      " [195 146]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2761\n",
      "Image efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg: box, Data: [[  0   0 638 520]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 638 520]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5794\n",
      "Image 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg: box, Data: [[ 99  22 139 139]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 99  22 139 139]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6916\n",
      "Image 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg: point, Data: [[566 372]\n",
      " [557 109]\n",
      " [598 452]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[566 372]\n",
      " [557 109]\n",
      " [598 452]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9962\n",
      "Image 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg: box, Data: [[  5  89 591 603]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  5  89 591 603]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6932\n",
      "Image lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg: box, Data: [[242  65 439 419]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[242  65 439 419]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6908\n",
      "Image 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg: point, Data: [[278 560]\n",
      " [295 575]\n",
      " [267 573]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[278 560]\n",
      " [295 575]\n",
      " [267 573]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7699\n",
      "Image 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg: box, Data: [[178 146 428 462]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[178 146 428 462]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6553\n",
      "Image IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg: point, Data: [[613 124]\n",
      " [565 188]\n",
      " [537 141]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[613 124]\n",
      " [565 188]\n",
      " [537 141]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7122\n",
      "Image Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg: point, Data: [[230 250]\n",
      " [294 343]\n",
      " [276 362]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[230 250]\n",
      " [294 343]\n",
      " [276 362]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6667\n",
      "Image 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg: box, Data: [[  0 102 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0 102 639 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5382\n",
      "Image PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg: box, Data: [[ 85 127 573 576]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 85 127 573 576]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5823\n",
      "Image 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg: point, Data: [[534 109]\n",
      " [559 159]\n",
      " [534 142]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[534 109]\n",
      " [559 159]\n",
      " [534 142]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6910\n",
      "Image patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg: box, Data: [[268 209 406 474]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[268 209 406 474]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6824\n",
      "Image How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg: point, Data: [[203 413]\n",
      " [224 437]\n",
      " [273 265]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[203 413]\n",
      " [224 437]\n",
      " [273 265]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8204\n",
      "Image 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg: point, Data: [[415 168]\n",
      " [127 128]\n",
      " [ 76  50]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[415 168]\n",
      " [127 128]\n",
      " [ 76  50]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2028\n",
      "Image 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg: box, Data: [[ 29   1 147 143]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 29   1 147 143]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6910\n",
      "Image 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg: box, Data: [[  3   4 636 565]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  3   4 636 565]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6830\n",
      "Image 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg: box, Data: [[568 300 637 576]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[568 300 637 576]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6898\n",
      "Image 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg: point, Data: [[561 243]\n",
      " [459 271]\n",
      " [160 612]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[561 243]\n",
      " [459 271]\n",
      " [160 612]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7352\n",
      "Image old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg: point, Data: [[285 181]\n",
      " [215 250]\n",
      " [310 208]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[285 181]\n",
      " [215 250]\n",
      " [310 208]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6894\n",
      "Image Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg: box, Data: [[ 77 215 591 603]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 77 215 591 603]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6769\n",
      "Image 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg: point, Data: [[299 240]\n",
      " [291 194]\n",
      " [300 263]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[299 240]\n",
      " [291 194]\n",
      " [300 263]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6923\n",
      "Image 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg: box, Data: [[  0 101 639 591]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0 101 639 591]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6836\n",
      "Image 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg: point, Data: [[218 510]\n",
      " [568 515]\n",
      " [580 523]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[218 510]\n",
      " [568 515]\n",
      " [580 523]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7760\n",
      "Image 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg: box, Data: [[ 65   0 293 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 65   0 293 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6845\n",
      "Image 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg: box, Data: [[ 51   0 612 395]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 51   0 612 395]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6981\n",
      "Image 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg: box, Data: [[103   0 310 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[103   0 310 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6858\n",
      "Image 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg: point, Data: [[ 99 578]\n",
      " [121 581]\n",
      " [ 99 558]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 99 578]\n",
      " [121 581]\n",
      " [ 99 558]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7016\n",
      "Image 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg: point, Data: [[539 152]\n",
      " [423 202]\n",
      " [424  87]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[539 152]\n",
      " [423 202]\n",
      " [424  87]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1426\n",
      "Image 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg: box, Data: [[278 218 428 414]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[278 218 428 414]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6804\n",
      "Image 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg: point, Data: [[121 127]\n",
      " [ 82 177]\n",
      " [ 75 171]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[121 127]\n",
      " [ 82 177]\n",
      " [ 75 171]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6918\n",
      "Image 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg: point, Data: [[485 189]\n",
      " [141 150]\n",
      " [185 144]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[485 189]\n",
      " [141 150]\n",
      " [185 144]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9595\n",
      "Image 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg: box, Data: [[227  16 589 216]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[227  16 589 216]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6939\n",
      "Image 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg: point, Data: [[395 464]\n",
      " [194 415]\n",
      " [505 239]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[395 464]\n",
      " [194 415]\n",
      " [505 239]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8040\n",
      "Image images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg: box, Data: [[  7  98 587 583]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  7  98 587 583]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6766\n",
      "Image images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg: point, Data: [[186 258]\n",
      " [293  81]\n",
      " [128 245]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[186 258]\n",
      " [293  81]\n",
      " [128 245]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9549\n",
      "Image 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg: box, Data: [[  4 415 338 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  4 415 338 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6889\n",
      "Image 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg: point, Data: [[317 339]\n",
      " [258 568]\n",
      " [449 330]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[317 339]\n",
      " [258 568]\n",
      " [449 330]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7859\n",
      "Image 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg: point, Data: [[609 579]\n",
      " [331 102]\n",
      " [364 345]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[609 579]\n",
      " [331 102]\n",
      " [364 345]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0407\n",
      "Image 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg: box, Data: [[146 374 639 636]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[146 374 639 636]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6862\n",
      "Image 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg: box, Data: [[  0   0 507 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 507 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9251\n",
      "Image 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg: point, Data: [[116 368]\n",
      " [ 10 355]\n",
      " [557 353]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[116 368]\n",
      " [ 10 355]\n",
      " [557 353]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7867\n",
      "Image Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg: box, Data: [[  0   0 610 638]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 610 638]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7143\n",
      "Image what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg: point, Data: [[308  25]\n",
      " [346 292]\n",
      " [432 100]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[308  25]\n",
      " [346 292]\n",
      " [432 100]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0129\n",
      "Image dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg: point, Data: [[573 407]\n",
      " [188 499]\n",
      " [166 378]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[573 407]\n",
      " [188 499]\n",
      " [166 378]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6928\n",
      "Image 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg: box, Data: [[ 74 311 439 491]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 74 311 439 491]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6933\n",
      "Image 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg: box, Data: [[283   0 637 229]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[283   0 637 229]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6877\n",
      "Image 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg: point, Data: [[259 359]\n",
      " [395  28]\n",
      " [192 156]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[259 359]\n",
      " [395  28]\n",
      " [192 156]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8759\n",
      "Image images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg: point, Data: [[357 582]\n",
      " [432 532]\n",
      " [426 496]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[357 582]\n",
      " [432 532]\n",
      " [426 496]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6633\n",
      "Image 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg: point, Data: [[47 26]\n",
      " [39 96]\n",
      " [33 12]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[47 26]\n",
      " [39 96]\n",
      " [33 12]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6819\n",
      "Image Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg: point, Data: [[507 597]\n",
      " [344 188]\n",
      " [510  11]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[507 597]\n",
      " [344 188]\n",
      " [510  11]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6926\n",
      "Image 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg: box, Data: [[ 63 349 461 547]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 63 349 461 547]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6904\n",
      "Image 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg: box, Data: [[  0  10 629 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0  10 629 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9787\n",
      "Image concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg: box, Data: [[336 345 373 409]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[336 345 373 409]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6920\n",
      "Image 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg: box, Data: [[539  79 576 272]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[539  79 576 272]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6914\n",
      "Image 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg: point, Data: [[ 81 191]\n",
      " [353 373]\n",
      " [184 261]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 81 191]\n",
      " [353 373]\n",
      " [184 261]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5965\n",
      "Image 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg: box, Data: [[125   0 506 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[125   0 506 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6912\n",
      "Image 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg: box, Data: [[191   0 639 630]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[191   0 639 630]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6881\n",
      "Image 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg: box, Data: [[  0  54 631 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0  54 631 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8243\n",
      "Image Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg: box, Data: [[  0   0 638 638]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 638 638]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7462\n",
      "Image 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg: box, Data: [[  2 387 412 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  2 387 412 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6886\n",
      "Image 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg: point, Data: [[104 622]\n",
      " [ 39 574]\n",
      " [ 15 627]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[104 622]\n",
      " [ 39 574]\n",
      " [ 15 627]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6892\n",
      "Image 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg: box, Data: [[ 56 335  89 421]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 56 335  89 421]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6922\n",
      "Image 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg: box, Data: [[123 180 639 335]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[123 180 639 335]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6348\n",
      "Image Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg: box, Data: [[  0   0 638 580]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 638 580]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7183\n",
      "Image 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg: box, Data: [[282  68 312 173]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[282  68 312 173]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6918\n",
      "Image crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg: point, Data: [[262 312]\n",
      " [423 260]\n",
      " [486 218]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[262 312]\n",
      " [423 260]\n",
      " [486 218]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6843\n",
      "Image 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg: point, Data: [[599 524]\n",
      " [582 522]\n",
      " [579 494]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[599 524]\n",
      " [582 522]\n",
      " [579 494]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6918\n",
      "Image Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg: box, Data: [[ 62   0 638 558]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 62   0 638 558]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6567\n",
      "Image 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg: point, Data: [[ 85 202]\n",
      " [127 365]\n",
      " [125 402]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 85 202]\n",
      " [127 365]\n",
      " [125 402]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2280\n",
      "Image 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg: point, Data: [[128 160]\n",
      " [104 113]\n",
      " [132 125]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[128 160]\n",
      " [104 113]\n",
      " [132 125]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6903\n",
      "Image d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg: point, Data: [[184 343]\n",
      " [379 359]\n",
      " [207  97]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[184 343]\n",
      " [379 359]\n",
      " [207  97]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6970\n",
      "Image 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg: point, Data: [[604 605]\n",
      " [324  57]\n",
      " [301  48]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[604 605]\n",
      " [324  57]\n",
      " [301  48]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6914\n",
      "Image concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg: box, Data: [[285 131 358 417]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[285 131 358 417]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6902\n",
      "Image 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg: point, Data: [[109 148]\n",
      " [308 194]\n",
      " [329 262]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[109 148]\n",
      " [308 194]\n",
      " [329 262]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0103\n",
      "Epoch 7/10, Loss: 0.7584\n",
      "Saving checkpoint: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\checkpoints\\sam2_epoch_7.pt\n",
      "Successfully saved checkpoint: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\checkpoints\\sam2_epoch_7.pt\n",
      "Image 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg: box, Data: [[  0   4 363 518]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   4 363 518]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7005\n",
      "Image images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg: point, Data: [[187 191]\n",
      " [238 155]\n",
      " [134 177]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[187 191]\n",
      " [238 155]\n",
      " [134 177]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6659\n",
      "Image 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg: point, Data: [[ 47   3]\n",
      " [ 21 253]\n",
      " [ 65 335]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 47   3]\n",
      " [ 21 253]\n",
      " [ 65 335]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1832\n",
      "Image weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg: point, Data: [[311 408]\n",
      " [296  31]\n",
      " [308  41]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[311 408]\n",
      " [296  31]\n",
      " [308  41]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6921\n",
      "Image stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg: point, Data: [[348 377]\n",
      " [208 390]\n",
      " [187 463]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[348 377]\n",
      " [208 390]\n",
      " [187 463]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6037\n",
      "Image 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg: box, Data: [[  5 171 318 605]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  5 171 318 605]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6923\n",
      "Image 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg: point, Data: [[126 230]\n",
      " [ 95 262]\n",
      " [218 395]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[126 230]\n",
      " [ 95 262]\n",
      " [218 395]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7966\n",
      "Image istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg: box, Data: [[241 201 457 434]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[241 201 457 434]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6626\n",
      "Image 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg: box, Data: [[103  35 147 169]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[103  35 147 169]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6899\n",
      "Image images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg: box, Data: [[306 309 471 583]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[306 309 471 583]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6653\n",
      "Image Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg: box, Data: [[  0   0 636 612]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 636 612]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2551\n",
      "Image what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg: point, Data: [[233 187]\n",
      " [189 178]\n",
      " [400 323]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[233 187]\n",
      " [189 178]\n",
      " [400 323]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0106\n",
      "Image 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg: point, Data: [[383 432]\n",
      " [192 232]\n",
      " [139 439]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[383 432]\n",
      " [192 232]\n",
      " [139 439]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8041\n",
      "Image 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg: box, Data: [[233   1 630 302]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[233   1 630 302]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6913\n",
      "Image 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg: box, Data: [[278 218 428 414]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[278 218 428 414]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6804\n",
      "Image 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg: point, Data: [[229 304]\n",
      " [340 340]\n",
      " [448 303]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[229 304]\n",
      " [340 340]\n",
      " [448 303]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5919\n",
      "Image 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg: point, Data: [[586 396]\n",
      " [407 439]\n",
      " [475 548]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[586 396]\n",
      " [407 439]\n",
      " [475 548]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6737\n",
      "Image 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg: point, Data: [[489 508]\n",
      " [466 582]\n",
      " [409 255]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[489 508]\n",
      " [466 582]\n",
      " [409 255]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9994\n",
      "Image 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg: point, Data: [[ 80 364]\n",
      " [ 84 421]\n",
      " [ 66 410]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 80 364]\n",
      " [ 84 421]\n",
      " [ 66 410]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6921\n",
      "Image 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg: point, Data: [[193 403]\n",
      " [ 76 496]\n",
      " [ 76 385]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[193 403]\n",
      " [ 76 496]\n",
      " [ 76 385]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9093\n",
      "Image 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg: point, Data: [[466 329]\n",
      " [552 614]\n",
      " [401 434]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[466 329]\n",
      " [552 614]\n",
      " [401 434]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2344\n",
      "Image 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg: point, Data: [[133 398]\n",
      " [126 337]\n",
      " [140 331]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[133 398]\n",
      " [126 337]\n",
      " [140 331]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7475\n",
      "Image 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg: point, Data: [[222 581]\n",
      " [ 26 382]\n",
      " [358 490]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[222 581]\n",
      " [ 26 382]\n",
      " [358 490]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8079\n",
      "Image images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg: point, Data: [[121 424]\n",
      " [214 149]\n",
      " [ 92 304]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[121 424]\n",
      " [214 149]\n",
      " [ 92 304]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9595\n",
      "Image 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg: point, Data: [[333 591]\n",
      " [630 564]\n",
      " [555 555]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[333 591]\n",
      " [630 564]\n",
      " [555 555]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6641\n",
      "Image 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg: box, Data: [[ 74 311 439 491]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 74 311 439 491]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6933\n",
      "Image 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg: box, Data: [[  4 466 599 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  4 466 599 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6899\n",
      "Image 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg: box, Data: [[  0   0 507 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 507 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9251\n",
      "Image 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg: box, Data: [[301 424 432 572]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[301 424 432 572]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6914\n",
      "Image 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg: point, Data: [[463 115]\n",
      " [434 155]\n",
      " [449 145]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[463 115]\n",
      " [434 155]\n",
      " [449 145]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6941\n",
      "Image 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg: point, Data: [[596 291]\n",
      " [434 575]\n",
      " [401 275]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[596 291]\n",
      " [434 575]\n",
      " [401 275]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5945\n",
      "Image 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg: box, Data: [[258   0 542 573]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[258   0 542 573]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6866\n",
      "Image 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg: box, Data: [[  0   0 523 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 523 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6861\n",
      "Image Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg: box, Data: [[198   0 638 607]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[198   0 638 607]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6901\n",
      "Image 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg: point, Data: [[ 67 252]\n",
      " [247 486]\n",
      " [267 383]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 67 252]\n",
      " [247 486]\n",
      " [267 383]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0053\n",
      "Image 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg: box, Data: [[  4 563 104 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  4 563 104 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6922\n",
      "Image 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg: point, Data: [[167 103]\n",
      " [170 170]\n",
      " [155 126]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[167 103]\n",
      " [170 170]\n",
      " [155 126]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6927\n",
      "Image card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg: point, Data: [[452 276]\n",
      " [313 235]\n",
      " [373 242]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[452 276]\n",
      " [313 235]\n",
      " [373 242]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7054\n",
      "Image Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg: box, Data: [[ 77 215 591 603]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 77 215 591 603]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6769\n",
      "Image 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg: box, Data: [[166  11 211 148]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[166  11 211 148]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6918\n",
      "Image 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg: box, Data: [[ 68   2 172 422]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 68   2 172 422]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6893\n",
      "Image concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg: point, Data: [[320 175]\n",
      " [343 166]\n",
      " [309 174]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[320 175]\n",
      " [343 166]\n",
      " [309 174]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6908\n",
      "Image 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg: point, Data: [[396 476]\n",
      " [416 623]\n",
      " [351 557]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[396 476]\n",
      " [416 623]\n",
      " [351 557]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6728\n",
      "Image Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg: point, Data: [[423  66]\n",
      " [482 270]\n",
      " [206 414]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[423  66]\n",
      " [482 270]\n",
      " [206 414]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8252\n",
      "Image 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg: box, Data: [[ 66 156 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 66 156 639 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8246\n",
      "Image 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg: box, Data: [[167 345 192 392]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[167 345 192 392]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6927\n",
      "Image 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg: point, Data: [[599 142]\n",
      " [139 225]\n",
      " [320 226]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[599 142]\n",
      " [139 225]\n",
      " [320 226]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1566\n",
      "Image 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg: point, Data: [[427 155]\n",
      " [482 191]\n",
      " [459 183]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[427 155]\n",
      " [482 191]\n",
      " [459 183]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6941\n",
      "Image 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg: box, Data: [[231 374 533 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[231 374 533 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6820\n",
      "Image images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg: point, Data: [[334 105]\n",
      " [309 168]\n",
      " [450 260]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[334 105]\n",
      " [309 168]\n",
      " [450 260]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6816\n",
      "Image 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg: point, Data: [[365 310]\n",
      " [ 10  76]\n",
      " [380 214]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[365 310]\n",
      " [ 10  76]\n",
      " [380 214]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7126\n",
      "Image istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg: point, Data: [[365 290]\n",
      " [319 373]\n",
      " [226 346]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[365 290]\n",
      " [319 373]\n",
      " [226 346]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6586\n",
      "Image 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg: box, Data: [[282  68 312 173]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[282  68 312 173]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6918\n",
      "Image 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg: box, Data: [[ 33   0 615 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 33   0 615 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6859\n",
      "Image 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg: point, Data: [[373  67]\n",
      " [359 215]\n",
      " [627 137]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[373  67]\n",
      " [359 215]\n",
      " [627 137]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7173\n",
      "Image 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg: point, Data: [[ 52 108]\n",
      " [ 91 142]\n",
      " [399 412]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 52 108]\n",
      " [ 91 142]\n",
      " [399 412]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9808\n",
      "Image 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg: box, Data: [[ 90  47 345 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 90  47 345 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6906\n",
      "Image 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg: point, Data: [[305 199]\n",
      " [301 215]\n",
      " [225 273]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[305 199]\n",
      " [301 215]\n",
      " [225 273]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6387\n",
      "Image 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg: box, Data: [[ 51   0 612 395]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 51   0 612 395]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6981\n",
      "Image 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg: box, Data: [[ 13   9 249 249]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 13   9 249 249]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6909\n",
      "Image 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg: point, Data: [[346 132]\n",
      " [535 385]\n",
      " [445 393]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[346 132]\n",
      " [535 385]\n",
      " [445 393]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9044\n",
      "Image 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg: point, Data: [[110  83]\n",
      " [ 65  13]\n",
      " [111  32]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[110  83]\n",
      " [ 65  13]\n",
      " [111  32]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7104\n",
      "Image 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg: point, Data: [[121  90]\n",
      " [133  43]\n",
      " [133  54]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[121  90]\n",
      " [133  43]\n",
      " [133  54]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6923\n",
      "Image 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg: box, Data: [[123 180 639 335]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[123 180 639 335]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6348\n",
      "Image 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg: box, Data: [[285 188 327 397]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[285 188 327 397]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6912\n",
      "Image 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg: box, Data: [[141  46 615 339]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[141  46 615 339]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7786\n",
      "Image 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg: box, Data: [[131 303 197 534]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[131 303 197 534]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6906\n",
      "Image 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg: point, Data: [[147 560]\n",
      " [138 566]\n",
      " [128 528]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[147 560]\n",
      " [138 566]\n",
      " [128 528]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6905\n",
      "Image 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg: box, Data: [[280   0 637 424]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[280   0 637 424]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6832\n",
      "Image 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg: box, Data: [[579 438 607 539]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[579 438 607 539]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6920\n",
      "Image images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg: point, Data: [[414 388]\n",
      " [428 376]\n",
      " [ 71  25]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[414 388]\n",
      " [428 376]\n",
      " [ 71  25]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6890\n",
      "Image lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg: box, Data: [[242  65 439 419]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[242  65 439 419]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6908\n",
      "Image 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg: box, Data: [[568 300 637 576]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[568 300 637 576]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6898\n",
      "Image 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg: point, Data: [[124 125]\n",
      " [566 114]\n",
      " [ 56  13]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[124 125]\n",
      " [566 114]\n",
      " [ 56  13]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6508\n",
      "Image d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg: box, Data: [[152   1 380 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[152   1 380 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6867\n",
      "Image 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg: point, Data: [[433 420]\n",
      " [564 392]\n",
      " [634 451]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[433 420]\n",
      " [564 392]\n",
      " [634 451]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0123\n",
      "Image Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg: box, Data: [[146 160 323 442]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[146 160 323 442]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6613\n",
      "Image 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg: box, Data: [[ 21   0 543 636]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 21   0 543 636]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6856\n",
      "Image old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg: box, Data: [[126 118 231 630]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[126 118 231 630]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6709\n",
      "Image 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg: box, Data: [[127 101 260 134]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[127 101 260 134]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6922\n",
      "Image 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg: point, Data: [[435 326]\n",
      " [475 541]\n",
      " [ 33 251]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[435 326]\n",
      " [475 541]\n",
      " [ 33 251]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6587\n",
      "Image PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg: point, Data: [[282 389]\n",
      " [524 409]\n",
      " [385 469]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[282 389]\n",
      " [524 409]\n",
      " [385 469]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5832\n",
      "Image 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg: point, Data: [[319 302]\n",
      " [290 402]\n",
      " [175 304]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[319 302]\n",
      " [290 402]\n",
      " [175 304]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7094\n",
      "Image How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg: box, Data: [[177 129 345 494]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[177 129 345 494]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6565\n",
      "Image 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg: point, Data: [[ 96 341]\n",
      " [ 35 428]\n",
      " [ 52 186]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 96 341]\n",
      " [ 35 428]\n",
      " [ 52 186]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6658\n",
      "Image crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg: box, Data: [[156  37 556 414]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[156  37 556 414]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6602\n",
      "Image CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg: box, Data: [[ 76  25 310 448]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 76  25 310 448]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6410\n",
      "Image 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg: point, Data: [[272 106]\n",
      " [272 168]\n",
      " [250 394]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[272 106]\n",
      " [272 168]\n",
      " [250 394]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9456\n",
      "Image 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg: point, Data: [[303 385]\n",
      " [275 309]\n",
      " [165 340]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[303 385]\n",
      " [275 309]\n",
      " [165 340]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6378\n",
      "Image 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg: point, Data: [[209  85]\n",
      " [230 309]\n",
      " [203 235]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[209  85]\n",
      " [230 309]\n",
      " [203 235]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1294\n",
      "Image 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg: point, Data: [[321 330]\n",
      " [301 509]\n",
      " [127 431]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[321 330]\n",
      " [301 509]\n",
      " [127 431]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7953\n",
      "Image 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg: point, Data: [[405 274]\n",
      " [446 337]\n",
      " [563 295]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[405 274]\n",
      " [446 337]\n",
      " [563 295]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0463\n",
      "Image patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg: box, Data: [[268 209 406 474]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[268 209 406 474]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6824\n",
      "Image 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg: box, Data: [[522   4 559 172]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[522   4 559 172]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6911\n",
      "Image old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg: box, Data: [[187 180 316 417]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[187 180 316 417]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6876\n",
      "Image 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg: box, Data: [[  0 173 636 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0 173 636 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6815\n",
      "Image 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg: point, Data: [[ 14  39]\n",
      " [ 21  51]\n",
      " [216  83]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 14  39]\n",
      " [ 21  51]\n",
      " [216  83]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7043\n",
      "Image concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg: box, Data: [[140 120 495 503]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[140 120 495 503]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6378\n",
      "Image 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg: box, Data: [[288   0 637 495]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[288   0 637 495]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6927\n",
      "Image 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg: box, Data: [[  1   0 639 598]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  1   0 639 598]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9833\n",
      "Image 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg: box, Data: [[114   0 382 496]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[114   0 382 496]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7025\n",
      "Image IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg: point, Data: [[334 124]\n",
      " [203 213]\n",
      " [613 303]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[334 124]\n",
      " [203 213]\n",
      " [613 303]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7079\n",
      "Image Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg: box, Data: [[469 484 535 555]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[469 484 535 555]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6902\n",
      "Image 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg: point, Data: [[116 631]\n",
      " [ 89 591]\n",
      " [ 79 606]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[116 631]\n",
      " [ 89 591]\n",
      " [ 79 606]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6872\n",
      "Image 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg: point, Data: [[331 157]\n",
      " [312 122]\n",
      " [486 239]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[331 157]\n",
      " [312 122]\n",
      " [486 239]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6817\n",
      "Image 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg: point, Data: [[210 314]\n",
      " [153 257]\n",
      " [160 615]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[210 314]\n",
      " [153 257]\n",
      " [160 615]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9617\n",
      "Image 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg: point, Data: [[305 589]\n",
      " [232 395]\n",
      " [343 443]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[305 589]\n",
      " [232 395]\n",
      " [343 443]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7850\n",
      "Image 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg: box, Data: [[178 146 428 462]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[178 146 428 462]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6553\n",
      "Image 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg: box, Data: [[405   0 518 480]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[405   0 518 480]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6924\n",
      "Image 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg: box, Data: [[ 30  42 247 539]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 30  42 247 539]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6897\n",
      "Image 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg: point, Data: [[518  42]\n",
      " [239 109]\n",
      " [535 202]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[518  42]\n",
      " [239 109]\n",
      " [535 202]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8991\n",
      "Image Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg: box, Data: [[  0   0 610 638]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 610 638]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7143\n",
      "Image 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg: point, Data: [[ 90 229]\n",
      " [ 34 222]\n",
      " [224  60]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 90 229]\n",
      " [ 34 222]\n",
      " [224  60]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6957\n",
      "Image 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg: box, Data: [[ 22 417 104 521]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 22 417 104 521]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6905\n",
      "Image efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg: point, Data: [[595 243]\n",
      " [465 376]\n",
      " [588 132]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[595 243]\n",
      " [465 376]\n",
      " [588 132]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9330\n",
      "Image Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg: box, Data: [[  0   0 638 580]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 638 580]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7183\n",
      "Image 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg: box, Data: [[  3 207 639 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  3 207 639 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6869\n",
      "Image 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg: point, Data: [[275 113]\n",
      " [143 574]\n",
      " [452 373]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[275 113]\n",
      " [143 574]\n",
      " [452 373]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2607\n",
      "Image images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg: point, Data: [[292 191]\n",
      " [351 328]\n",
      " [317 203]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[292 191]\n",
      " [351 328]\n",
      " [317 203]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6770\n",
      "Image 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg: box, Data: [[146   0 179  68]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[146   0 179  68]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6922\n",
      "Image 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg: point, Data: [[308 217]\n",
      " [445 129]\n",
      " [396 225]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[308 217]\n",
      " [445 129]\n",
      " [396 225]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1183\n",
      "Image 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg: box, Data: [[539  79 576 272]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[539  79 576 272]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6914\n",
      "Image 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg: point, Data: [[491 421]\n",
      " [335 458]\n",
      " [419 458]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[491 421]\n",
      " [335 458]\n",
      " [419 458]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6082\n",
      "Image 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg: box, Data: [[  1 149 600 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  1 149 600 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9886\n",
      "Image 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg: box, Data: [[299   0 628 635]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[299   0 628 635]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6920\n",
      "Image 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg: box, Data: [[  3   4 636 565]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  3   4 636 565]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6830\n",
      "Image 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg: box, Data: [[513   0 637 274]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[513   0 637 274]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6906\n",
      "Image Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg: point, Data: [[ 14 317]\n",
      " [200 314]\n",
      " [173 296]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 14 317]\n",
      " [200 314]\n",
      " [173 296]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.4299\n",
      "Image dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg: box, Data: [[ 80   0 637 609]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 80   0 637 609]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8884\n",
      "Image concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg: box, Data: [[336 345 373 409]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[336 345 373 409]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6920\n",
      "Image 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg: box, Data: [[ 36  23 116 577]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 36  23 116 577]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6919\n",
      "Image 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg: box, Data: [[ 76   0 257 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 76   0 257 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6896\n",
      "Image 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg: point, Data: [[ 17 576]\n",
      " [ 69 438]\n",
      " [173 523]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 17 576]\n",
      " [ 69 438]\n",
      " [173 523]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1295\n",
      "Image 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg: box, Data: [[191   0 639 630]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[191   0 639 630]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6881\n",
      "Epoch 8/10, Loss: 0.7519\n",
      "Saving checkpoint: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\checkpoints\\sam2_epoch_8.pt\n",
      "Successfully saved checkpoint: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\checkpoints\\sam2_epoch_8.pt\n",
      "Image 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg: point, Data: [[198 302]\n",
      " [495 394]\n",
      " [204 320]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[198 302]\n",
      " [495 394]\n",
      " [204 320]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6912\n",
      "Image concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg: box, Data: [[336 345 373 409]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[336 345 373 409]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6920\n",
      "Image 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg: box, Data: [[ 51   0 612 395]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 51   0 612 395]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6981\n",
      "Image 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg: box, Data: [[103  35 147 169]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[103  35 147 169]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6899\n",
      "Image 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg: box, Data: [[ 35  93 301 576]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 35  93 301 576]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6934\n",
      "Image 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg: point, Data: [[ 15 202]\n",
      " [ 40 208]\n",
      " [219 141]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 15 202]\n",
      " [ 40 208]\n",
      " [219 141]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2529\n",
      "Image 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg: box, Data: [[393  17 485 203]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[393  17 485 203]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6914\n",
      "Image 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg: point, Data: [[616 304]\n",
      " [361 533]\n",
      " [319 599]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[616 304]\n",
      " [361 533]\n",
      " [319 599]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7424\n",
      "Image 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg: box, Data: [[  1   0 639 598]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  1   0 639 598]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9833\n",
      "Image 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg: point, Data: [[  2 422]\n",
      " [180 561]\n",
      " [131 301]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[  2 422]\n",
      " [180 561]\n",
      " [131 301]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7871\n",
      "Image 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg: box, Data: [[539  79 576 272]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[539  79 576 272]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6914\n",
      "Image 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg: box, Data: [[ 74 311 439 491]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 74 311 439 491]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6933\n",
      "Image 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg: point, Data: [[606 506]\n",
      " [585 484]\n",
      " [584 520]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[606 506]\n",
      " [585 484]\n",
      " [584 520]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6918\n",
      "Image 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg: point, Data: [[163 597]\n",
      " [ 73 563]\n",
      " [ 77 298]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[163 597]\n",
      " [ 73 563]\n",
      " [ 77 298]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6206\n",
      "Image 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg: box, Data: [[  0   1 414 318]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   1 414 318]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6890\n",
      "Image stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg: point, Data: [[196 326]\n",
      " [431 117]\n",
      " [333 448]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[196 326]\n",
      " [431 117]\n",
      " [333 448]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6088\n",
      "Image Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg: point, Data: [[393 313]\n",
      " [552 201]\n",
      " [408 380]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[393 313]\n",
      " [552 201]\n",
      " [408 380]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6958\n",
      "Image 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg: point, Data: [[168 505]\n",
      " [154 372]\n",
      " [170 385]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[168 505]\n",
      " [154 372]\n",
      " [170 385]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6886\n",
      "Image 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg: box, Data: [[  4 466 599 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  4 466 599 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6899\n",
      "Image 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg: point, Data: [[539 528]\n",
      " [179 423]\n",
      " [322 134]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[539 528]\n",
      " [179 423]\n",
      " [322 134]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6830\n",
      "Image images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg: point, Data: [[266 385]\n",
      " [169 473]\n",
      " [248 502]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[266 385]\n",
      " [169 473]\n",
      " [248 502]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6645\n",
      "Image Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg: point, Data: [[547 178]\n",
      " [347 206]\n",
      " [361 442]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[547 178]\n",
      " [347 206]\n",
      " [361 442]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.4323\n",
      "Image 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg: point, Data: [[ 92 202]\n",
      " [113 107]\n",
      " [ 81 456]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 92 202]\n",
      " [113 107]\n",
      " [ 81 456]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2325\n",
      "Image 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg: box, Data: [[  1 128 639 587]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  1 128 639 587]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6845\n",
      "Image 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg: box, Data: [[178 146 428 462]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[178 146 428 462]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6553\n",
      "Image 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg: point, Data: [[146  33]\n",
      " [154  11]\n",
      " [160  43]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[146  33]\n",
      " [154  11]\n",
      " [160  43]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6921\n",
      "Image 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg: point, Data: [[ 71 311]\n",
      " [126 382]\n",
      " [567 505]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 71 311]\n",
      " [126 382]\n",
      " [567 505]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0131\n",
      "Image 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg: box, Data: [[  0 173 636 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0 173 636 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6815\n",
      "Image dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg: point, Data: [[634 483]\n",
      " [ 81 369]\n",
      " [596 599]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[634 483]\n",
      " [ 81 369]\n",
      " [596 599]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2942\n",
      "Image PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg: box, Data: [[ 85 127 573 576]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 85 127 573 576]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5823\n",
      "Image 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg: point, Data: [[180 119]\n",
      " [284 123]\n",
      " [269   1]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[180 119]\n",
      " [284 123]\n",
      " [269   1]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9728\n",
      "Image efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg: point, Data: [[440 104]\n",
      " [591 166]\n",
      " [341 249]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[440 104]\n",
      " [591 166]\n",
      " [341 249]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5782\n",
      "Image d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg: box, Data: [[152   1 380 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[152   1 380 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6867\n",
      "Image 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg: point, Data: [[430  10]\n",
      " [283 524]\n",
      " [411 278]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[430  10]\n",
      " [283 524]\n",
      " [411 278]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0458\n",
      "Image old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg: box, Data: [[187 180 316 417]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[187 180 316 417]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6876\n",
      "Image 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg: box, Data: [[367 328 582 624]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[367 328 582 624]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6913\n",
      "Image 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg: point, Data: [[400 120]\n",
      " [391 266]\n",
      " [149  95]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[400 120]\n",
      " [391 266]\n",
      " [149  95]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9118\n",
      "Image 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg: point, Data: [[316  17]\n",
      " [620 626]\n",
      " [340  19]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[316  17]\n",
      " [620 626]\n",
      " [340  19]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6912\n",
      "Image 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg: point, Data: [[199 383]\n",
      " [140 343]\n",
      " [364  93]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[199 383]\n",
      " [140 343]\n",
      " [364  93]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2656\n",
      "Image lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg: point, Data: [[246 105]\n",
      " [276 106]\n",
      " [254  92]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[246 105]\n",
      " [276 106]\n",
      " [254  92]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6911\n",
      "Image 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg: box, Data: [[  1   0 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  1   0 639 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8042\n",
      "Image 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg: point, Data: [[588 244]\n",
      " [579  47]\n",
      " [611 141]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[588 244]\n",
      " [579  47]\n",
      " [611 141]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0679\n",
      "Image 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg: point, Data: [[124 498]\n",
      " [ 79 474]\n",
      " [ 32 576]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[124 498]\n",
      " [ 79 474]\n",
      " [ 32 576]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0315\n",
      "Image 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg: box, Data: [[ 48   0 176 178]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 48   0 176 178]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6918\n",
      "Image images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg: point, Data: [[433 394]\n",
      " [343 343]\n",
      " [416 348]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[433 394]\n",
      " [343 343]\n",
      " [416 348]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6645\n",
      "Image 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg: point, Data: [[ 40 116]\n",
      " [225 272]\n",
      " [108 253]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 40 116]\n",
      " [225 272]\n",
      " [108 253]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1444\n",
      "Image Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg: box, Data: [[198   0 638 607]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[198   0 638 607]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6901\n",
      "Image 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg: point, Data: [[485 209]\n",
      " [348 223]\n",
      " [213 227]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[485 209]\n",
      " [348 223]\n",
      " [213 227]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8043\n",
      "Image 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg: box, Data: [[  0 102 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0 102 639 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5382\n",
      "Image 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg: point, Data: [[101 326]\n",
      " [226  30]\n",
      " [135 345]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[101 326]\n",
      " [226  30]\n",
      " [135 345]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9622\n",
      "Image 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg: point, Data: [[422 247]\n",
      " [242 133]\n",
      " [123  51]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[422 247]\n",
      " [242 133]\n",
      " [123  51]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2069\n",
      "Image 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg: box, Data: [[  0   0 523 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 523 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6861\n",
      "Image Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg: point, Data: [[162 180]\n",
      " [167 248]\n",
      " [182 342]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[162 180]\n",
      " [167 248]\n",
      " [182 342]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7037\n",
      "Image 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg: point, Data: [[ 59 582]\n",
      " [ 56 603]\n",
      " [ 80 614]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 59 582]\n",
      " [ 56 603]\n",
      " [ 80 614]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6911\n",
      "Image 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg: box, Data: [[ 68   2 172 422]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 68   2 172 422]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6893\n",
      "Image 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg: box, Data: [[  1 149 600 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  1 149 600 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9886\n",
      "Image 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg: box, Data: [[ 12   0 498 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 12   0 498 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7543\n",
      "Image 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg: box, Data: [[  3   4 636 565]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  3   4 636 565]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6830\n",
      "Image 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg: box, Data: [[ 90  47 345 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 90  47 345 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6906\n",
      "Image 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg: point, Data: [[532 212]\n",
      " [577 126]\n",
      " [549 120]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[532 212]\n",
      " [577 126]\n",
      " [549 120]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6948\n",
      "Image 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg: box, Data: [[231 374 533 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[231 374 533 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6820\n",
      "Image istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg: point, Data: [[375 382]\n",
      " [332 245]\n",
      " [340 217]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[375 382]\n",
      " [332 245]\n",
      " [340 217]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6629\n",
      "Image crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg: box, Data: [[156  37 556 414]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[156  37 556 414]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6602\n",
      "Image 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg: point, Data: [[130 175]\n",
      " [549 105]\n",
      " [374 117]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[130 175]\n",
      " [549 105]\n",
      " [374 117]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9245\n",
      "Image 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg: box, Data: [[283   0 637 229]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[283   0 637 229]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6877\n",
      "Image 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg: point, Data: [[462 382]\n",
      " [148 593]\n",
      " [153 561]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[462 382]\n",
      " [148 593]\n",
      " [153 561]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8349\n",
      "Image Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg: point, Data: [[604 313]\n",
      " [282  40]\n",
      " [280 453]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[604 313]\n",
      " [282  40]\n",
      " [280 453]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7010\n",
      "Image 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg: box, Data: [[ 13 256 359 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 13 256 359 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6799\n",
      "Image images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg: box, Data: [[116   0 382 466]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[116   0 382 466]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6428\n",
      "Image 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg: point, Data: [[ 69  86]\n",
      " [119  22]\n",
      " [123 141]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 69  86]\n",
      " [119  22]\n",
      " [123 141]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2587\n",
      "Image images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg: box, Data: [[ 65   0 513 417]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 65   0 513 417]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6888\n",
      "Image weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg: box, Data: [[283   0 352 638]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[283   0 352 638]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6919\n",
      "Image what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg: point, Data: [[550 156]\n",
      " [198 179]\n",
      " [432 249]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[550 156]\n",
      " [198 179]\n",
      " [432 249]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0156\n",
      "Image 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg: box, Data: [[ 63 349 461 547]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 63 349 461 547]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6904\n",
      "Image 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg: box, Data: [[288   0 637 495]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[288   0 637 495]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6927\n",
      "Image 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg: box, Data: [[ 63  32 639 467]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 63  32 639 467]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6953\n",
      "Image 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg: point, Data: [[380 532]\n",
      " [332 483]\n",
      " [350 438]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[380 532]\n",
      " [332 483]\n",
      " [350 438]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0730\n",
      "Image 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg: point, Data: [[518 456]\n",
      " [470 335]\n",
      " [423 463]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[518 456]\n",
      " [470 335]\n",
      " [423 463]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0131\n",
      "Image 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg: box, Data: [[  3 207 639 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  3 207 639 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6869\n",
      "Image card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg: point, Data: [[399 189]\n",
      " [481 150]\n",
      " [429  77]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[399 189]\n",
      " [481 150]\n",
      " [429  77]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9970\n",
      "Image 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg: box, Data: [[  1   0 639 304]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  1   0 639 304]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6856\n",
      "Image 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg: box, Data: [[282  68 312 173]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[282  68 312 173]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6918\n",
      "Image 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg: point, Data: [[354 611]\n",
      " [233 217]\n",
      " [185 308]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[354 611]\n",
      " [233 217]\n",
      " [185 308]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5736\n",
      "Image 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg: box, Data: [[285 188 327 397]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[285 188 327 397]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6912\n",
      "Image istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg: box, Data: [[130 110 494 452]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[130 110 494 452]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6531\n",
      "Image 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg: point, Data: [[ 55 490]\n",
      " [ 26 442]\n",
      " [ 26 470]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 55 490]\n",
      " [ 26 442]\n",
      " [ 26 470]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6905\n",
      "Image Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg: box, Data: [[  0   0 636 612]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 636 612]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2551\n",
      "Image images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg: point, Data: [[370 118]\n",
      " [211 434]\n",
      " [201 507]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[370 118]\n",
      " [211 434]\n",
      " [201 507]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6839\n",
      "Image CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg: point, Data: [[222  60]\n",
      " [231 171]\n",
      " [146 422]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[222  60]\n",
      " [231 171]\n",
      " [146 422]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6616\n",
      "Image 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg: box, Data: [[319 371 639 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[319 371 639 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6886\n",
      "Image 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg: box, Data: [[ 66 156 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 66 156 639 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8246\n",
      "Image IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg: point, Data: [[479 269]\n",
      " [402 202]\n",
      " [475  47]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[479 269]\n",
      " [402 202]\n",
      " [475  47]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7125\n",
      "Image 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg: point, Data: [[445 162]\n",
      " [495  59]\n",
      " [395 111]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[445 162]\n",
      " [495  59]\n",
      " [395 111]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9569\n",
      "Image 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg: point, Data: [[ 80 118]\n",
      " [143 132]\n",
      " [474 259]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 80 118]\n",
      " [143 132]\n",
      " [474 259]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6260\n",
      "Image Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg: box, Data: [[469 484 535 555]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[469 484 535 555]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6902\n",
      "Image 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg: point, Data: [[415 619]\n",
      " [374 621]\n",
      " [343 270]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[415 619]\n",
      " [374 621]\n",
      " [343 270]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6700\n",
      "Image 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg: point, Data: [[106 321]\n",
      " [340 247]\n",
      " [316 260]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[106 321]\n",
      " [340 247]\n",
      " [316 260]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8758\n",
      "Image 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg: box, Data: [[103   0 310 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[103   0 310 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6858\n",
      "Image 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg: point, Data: [[ 44 162]\n",
      " [472 556]\n",
      " [483 478]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 44 162]\n",
      " [472 556]\n",
      " [483 478]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2340\n",
      "Image 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg: point, Data: [[  6 592]\n",
      " [322 542]\n",
      " [114 525]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[  6 592]\n",
      " [322 542]\n",
      " [114 525]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1834\n",
      "Image 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg: box, Data: [[280   0 637 424]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[280   0 637 424]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6832\n",
      "Image 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg: box, Data: [[ 76   0 257 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 76   0 257 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6896\n",
      "Image concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg: point, Data: [[267 467]\n",
      " [313 180]\n",
      " [451 241]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[267 467]\n",
      " [313 180]\n",
      " [451 241]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6269\n",
      "Image 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg: point, Data: [[ 52 474]\n",
      " [234 206]\n",
      " [ 33 447]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 52 474]\n",
      " [234 206]\n",
      " [ 33 447]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8674\n",
      "Image 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg: point, Data: [[532 104]\n",
      " [554  64]\n",
      " [550 170]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[532 104]\n",
      " [554  64]\n",
      " [550 170]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6909\n",
      "Image Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg: box, Data: [[  0   0 610 638]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 610 638]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7143\n",
      "Image How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg: point, Data: [[258 343]\n",
      " [197 306]\n",
      " [192 132]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[258 343]\n",
      " [197 306]\n",
      " [192 132]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6697\n",
      "Image 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg: point, Data: [[574 555]\n",
      " [609 559]\n",
      " [470 539]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[574 555]\n",
      " [609 559]\n",
      " [470 539]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6723\n",
      "Image Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg: point, Data: [[144 376]\n",
      " [125 349]\n",
      " [285 560]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[144 376]\n",
      " [125 349]\n",
      " [285 560]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6780\n",
      "Image 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg: box, Data: [[166  11 211 148]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[166  11 211 148]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6918\n",
      "Image 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg: box, Data: [[ 95 298 151 401]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 95 298 151 401]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6921\n",
      "Image 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg: box, Data: [[  6  10 632 510]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  6  10 632 510]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6970\n",
      "Image patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg: box, Data: [[268 209 406 474]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[268 209 406 474]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6824\n",
      "Image 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg: point, Data: [[610 169]\n",
      " [239   8]\n",
      " [319  69]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[610 169]\n",
      " [239   8]\n",
      " [319  69]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6933\n",
      "Image 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg: box, Data: [[  0  89 513 495]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0  89 513 495]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6914\n",
      "Image 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg: box, Data: [[568 300 637 576]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[568 300 637 576]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6898\n",
      "Image 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg: box, Data: [[167 345 192 392]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[167 345 192 392]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6927\n",
      "Image 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg: point, Data: [[191 121]\n",
      " [138 101]\n",
      " [127 124]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[191 121]\n",
      " [138 101]\n",
      " [127 124]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1895\n",
      "Image 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg: box, Data: [[125   0 506 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[125   0 506 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6912\n",
      "Image old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg: box, Data: [[126 118 231 630]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[126 118 231 630]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6709\n",
      "Image 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg: point, Data: [[535 365]\n",
      " [586 234]\n",
      " [316 316]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[535 365]\n",
      " [586 234]\n",
      " [316 316]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7022\n",
      "Image 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg: point, Data: [[431 157]\n",
      " [449 166]\n",
      " [467 178]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[431 157]\n",
      " [449 166]\n",
      " [467 178]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6933\n",
      "Image 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg: point, Data: [[325  75]\n",
      " [102 148]\n",
      " [ 22 121]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[325  75]\n",
      " [102 148]\n",
      " [ 22 121]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6513\n",
      "Image 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg: box, Data: [[ 92 490 190 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 92 490 190 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6910\n",
      "Image 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg: point, Data: [[138  38]\n",
      " [100 138]\n",
      " [102  41]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[138  38]\n",
      " [100 138]\n",
      " [102  41]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6923\n",
      "Image 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg: box, Data: [[ 13   9 249 249]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 13   9 249 249]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6909\n",
      "Image 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg: box, Data: [[ 56 335  89 421]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 56 335  89 421]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6922\n",
      "Image 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg: point, Data: [[292 319]\n",
      " [536 308]\n",
      " [400  94]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[292 319]\n",
      " [536 308]\n",
      " [400  94]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0454\n",
      "Image images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg: point, Data: [[206 209]\n",
      " [177 249]\n",
      " [265 264]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[206 209]\n",
      " [177 249]\n",
      " [265 264]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7111\n",
      "Image 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg: point, Data: [[419 375]\n",
      " [371 620]\n",
      " [ 52 415]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[419 375]\n",
      " [371 620]\n",
      " [ 52 415]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7869\n",
      "Image concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg: box, Data: [[285 131 358 417]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[285 131 358 417]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6902\n",
      "Image 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg: box, Data: [[  0   4 363 518]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   4 363 518]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7005\n",
      "Image 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg: point, Data: [[525 487]\n",
      " [484 470]\n",
      " [498 371]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[525 487]\n",
      " [484 470]\n",
      " [498 371]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5777\n",
      "Image 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg: box, Data: [[278 218 428 414]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[278 218 428 414]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6804\n",
      "Epoch 9/10, Loss: 0.7695\n",
      "Saving checkpoint: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\checkpoints\\sam2_epoch_9.pt\n",
      "Successfully saved checkpoint: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\checkpoints\\sam2_epoch_9.pt\n",
      "Image 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg: point, Data: [[ 30 267]\n",
      " [489 164]\n",
      " [ 51  45]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10012_jpg.rf.fe5f5ae3da496c549e9a47bd381e1c17.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 30 267]\n",
      " [489 164]\n",
      " [ 51  45]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7225\n",
      "Image 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg: point, Data: [[321 167]\n",
      " [497 374]\n",
      " [328 383]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10028_jpg.rf.9d62ce4aca261a41324d595e8eec3d84.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[321 167]\n",
      " [497 374]\n",
      " [328 383]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8033\n",
      "Image Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg: point, Data: [[349 282]\n",
      " [257 384]\n",
      " [341 321]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Getty-1155592658-Resize-Crop-DH-FHM-Why-Are-There-Holes-in-my-Concrete_jpg.rf.f38026a1d040465e11e402cfb4dfbe33.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[349 282]\n",
      " [257 384]\n",
      " [341 321]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6688\n",
      "Image Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg: point, Data: [[300 340]\n",
      " [264 294]\n",
      " [320 600]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Moderate-popout-that-occurred-on-a-concrete-slab-on-grade_Q320_jpg.rf.60898f714c2d580ce314896eaa7575c4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[300 340]\n",
      " [264 294]\n",
      " [320 600]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7305\n",
      "Image 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg: point, Data: [[ 46 329]\n",
      " [ 59 536]\n",
      " [151 385]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10006_jpg.rf.3c4a82458408d18787219182266973ef.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 46 329]\n",
      " [ 59 536]\n",
      " [151 385]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6259\n",
      "Image Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg: box, Data: [[ 62   0 638 558]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Efflorescence__FitMaxWzEyMDAsMTIwMF0_jpg.rf.b56150db663e9af84d3d95de238be2fe.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 62   0 638 558]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6567\n",
      "Image 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg: box, Data: [[ 13 256 359 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-17_jpg.rf.dbb9237691069bd68bd76f666eeca658.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 13 256 359 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6799\n",
      "Image 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg: point, Data: [[296 373]\n",
      " [632  86]\n",
      " [620  53]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-66_jpg.rf.83a0ebf0135d6662f1a3a043da2306d3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[296 373]\n",
      " [632  86]\n",
      " [620  53]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6918\n",
      "Image 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg: box, Data: [[  5  89 591 603]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10042_jpg.rf.39a6245e2a10843d22fa0eb72b33a086.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  5  89 591 603]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6932\n",
      "Image patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg: box, Data: [[268 209 406 474]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image patching-smalls-holes-in-concrete-driveway-v0-j0cmlj7q071c1_webp.rf.408f2c3fee7b4d992046de65ea6db4e4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[268 209 406 474]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6824\n",
      "Image Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg: point, Data: [[589 106]\n",
      " [ 73 322]\n",
      " [245 290]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Soumission_Renovation_efflorescence_webp.rf.36bca51105d00bfb91d794f007a92bc3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[589 106]\n",
      " [ 73 322]\n",
      " [245 290]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.4306\n",
      "Image CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg: box, Data: [[ 76  25 310 448]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image CIP_40_jpg.rf.2766f9f5ea21a3bdc5cf278966df2810.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 76  25 310 448]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6410\n",
      "Image 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg: point, Data: [[283 486]\n",
      " [265 510]\n",
      " [299 561]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-99_jpg.rf.7b21d023b6e9d8a601dcb0ce8017e6d6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[283 486]\n",
      " [265 510]\n",
      " [299 561]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6895\n",
      "Image PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg: box, Data: [[ 85 127 573 576]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image PO2_jpg.rf.9b5a5f66d612ec47a01ed17072fe7c5c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 85 127 573 576]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5823\n",
      "Image 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg: point, Data: [[ 30 159]\n",
      " [ 75  41]\n",
      " [233  52]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-226_jpg.rf.2f25c87f7e70017b3d041453e0e46119.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 30 159]\n",
      " [ 75  41]\n",
      " [233  52]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6902\n",
      "Image 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg: point, Data: [[255  19]\n",
      " [301  66]\n",
      " [298  39]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-47_jpg.rf.aa81cf15fa85b6dd9326d85c27d97e76.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[255  19]\n",
      " [301  66]\n",
      " [298  39]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6912\n",
      "Image 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg: point, Data: [[ 52 581]\n",
      " [555 576]\n",
      " [  9 626]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-22_jpg.rf.041d2b1a79ac4c3f5d29c3596f9420b2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 52 581]\n",
      " [555 576]\n",
      " [  9 626]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6842\n",
      "Image dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg: box, Data: [[ 80   0 637 609]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image dalle-de-beton-02_webp.rf.b72126c2aab14180eaf1c8d7f59be1f8.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 80   0 637 609]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8884\n",
      "Image crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg: box, Data: [[156  37 556 414]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image crack-repair-047_jpg.rf.ecbd779bd652ac131f02b97399a70fd2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[156  37 556 414]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6602\n",
      "Image 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg: box, Data: [[ 29   1 147 143]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-59_jpg.rf.d6328bc61a815208f5014aef59d69ec3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 29   1 147 143]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6910\n",
      "Image weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg: point, Data: [[310  23]\n",
      " [311   9]\n",
      " [288  21]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image weird-atlantic-ocean-holes-meta_png.rf.a42546a08d506e5bc8ad20692d0a85b5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[310  23]\n",
      " [311   9]\n",
      " [288  21]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6924\n",
      "Image 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg: point, Data: [[167 126]\n",
      " [210  89]\n",
      " [209  33]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-74_jpg.rf.8c5da04ac849377325d337f9196433cd.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[167 126]\n",
      " [210  89]\n",
      " [209  33]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7864\n",
      "Image 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg: point, Data: [[ 76  84]\n",
      " [ 63  31]\n",
      " [157  10]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-134_jpg.rf.5633e347afd34c995f61d40326030493.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 76  84]\n",
      " [ 63  31]\n",
      " [157  10]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6952\n",
      "Image 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg: box, Data: [[  1   0 639 304]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-47_jpg.rf.d02b9590fc8602dcc11aedbccdaca19b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  1   0 639 304]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6856\n",
      "Image old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg: point, Data: [[182 382]\n",
      " [199 317]\n",
      " [226 595]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image old-concrete-floor-in-black-and-white-color-cement-broken-dirty-background-texture-free-photo_jpg.rf.14de89113df450dcf069eb781df16ede.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[182 382]\n",
      " [199 317]\n",
      " [226 595]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7069\n",
      "Image Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg: box, Data: [[198   0 638 607]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Concrete-Distress-Investigation-pavement_jpg.rf.b6ff1ae846949835c44c409b2b67c437.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[198   0 638 607]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6901\n",
      "Image 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg: point, Data: [[268 224]\n",
      " [252 243]\n",
      " [392 137]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10103_jpg.rf.05b064360326cde4d0a10a3986864bc0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[268 224]\n",
      " [252 243]\n",
      " [392 137]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9094\n",
      "Image 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg: point, Data: [[186 127]\n",
      " [110 187]\n",
      " [233 190]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-60_jpg.rf.c2cf91709854053190c05acedcd4dd8f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[186 127]\n",
      " [110 187]\n",
      " [233 190]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8759\n",
      "Image card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg: point, Data: [[487 170]\n",
      " [445  92]\n",
      " [424 237]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image card_herstellen-betonrot-voorbereiding_jpg.rf.67079a63723155af2c185b6b5608f989.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[487 170]\n",
      " [445  92]\n",
      " [424 237]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0056\n",
      "Image 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg: box, Data: [[280   0 637 424]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-35_jpg.rf.eb645014074f24285a3b6e2da90ca6de.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[280   0 637 424]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6832\n",
      "Image 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg: point, Data: [[109 160]\n",
      " [121 138]\n",
      " [111  51]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10099_jpg.rf.257f87f5894f00dc629a2dd1ee9ac202.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[109 160]\n",
      " [121 138]\n",
      " [111  51]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6901\n",
      "Image 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg: point, Data: [[163 550]\n",
      " [ 44 491]\n",
      " [ 97 559]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-17_jpg.rf.7b3135b19bc804731c652e81f66445d7.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[163 550]\n",
      " [ 44 491]\n",
      " [ 97 559]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6835\n",
      "Image 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg: point, Data: [[ 67 398]\n",
      " [ 56 365]\n",
      " [ 89 410]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-216_jpg.rf.c8912d6f4678ceb3c59e7a841eb1057a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 67 398]\n",
      " [ 56 365]\n",
      " [ 89 410]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6925\n",
      "Image 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg: box, Data: [[ 63 349 461 547]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-34_jpg.rf.61cd410c8179e51f18dfa12d55b50835.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 63 349 461 547]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6904\n",
      "Image 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg: box, Data: [[ 68   2 172 422]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-68_jpg.rf.8b6e0cc2c239390872bdf163ad8d96ae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 68   2 172 422]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6893\n",
      "Image 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg: point, Data: [[464 151]\n",
      " [442 143]\n",
      " [394 186]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-157_jpg.rf.468c4be894105237200860ae9de277a3.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[464 151]\n",
      " [442 143]\n",
      " [394 186]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8513\n",
      "Image 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg: box, Data: [[ 99  22 139 139]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-166_jpg.rf.df95208cedfc533af093230eed01d645.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 99  22 139 139]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6916\n",
      "Image istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg: point, Data: [[452 314]\n",
      " [435 298]\n",
      " [405 253]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image istockphoto-1430389981-2048x2048_jpg.rf.2048bd942394fef2b2e05633dbbc8b8d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[452 314]\n",
      " [435 298]\n",
      " [405 253]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6581\n",
      "Image 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg: point, Data: [[167 519]\n",
      " [123 494]\n",
      " [168 542]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-86_jpg.rf.e3518fdc2155e0517f64edf36214ebdd.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[167 519]\n",
      " [123 494]\n",
      " [168 542]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0228\n",
      "Image 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg: box, Data: [[ 36  23 116 577]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-208_jpg.rf.4ff59494c09c973178f748731972bfbe.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 36  23 116 577]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6919\n",
      "Image 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg: point, Data: [[ 14 558]\n",
      " [ 41 538]\n",
      " [ 43 236]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-163_jpg.rf.2675337056a31dc1f477ded968eacd81.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 14 558]\n",
      " [ 41 538]\n",
      " [ 43 236]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6909\n",
      "Image 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg: point, Data: [[142 377]\n",
      " [ 43 389]\n",
      " [503 353]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10024_jpg.rf.2bd988779647013de4c94d6312a3025b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[142 377]\n",
      " [ 43 389]\n",
      " [503 353]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9812\n",
      "Image 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg: box, Data: [[227  16 589 216]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-86_jpg.rf.747bc80d79a5351f88402719a537cc7b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[227  16 589 216]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6939\n",
      "Image 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg: box, Data: [[  0 173 636 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-48_jpg.rf.f2406eb390d3a4bb42336e0d491049eb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0 173 636 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6815\n",
      "Image concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg: box, Data: [[140 120 495 503]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-rock-pop-outs-typically-appear_jpeg.rf.4d9d418c1fa2d4b4b09928f0d559cd8f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[140 120 495 503]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6378\n",
      "Image 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg: point, Data: [[147  29]\n",
      " [149  32]\n",
      " [155  64]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-60_jpg.rf.0a68f693ca022402341e1f88f41be013.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[147  29]\n",
      " [149  32]\n",
      " [155  64]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6923\n",
      "Image 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg: box, Data: [[125   0 506 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-86_jpg.rf.a3faabd5c7e817929c20c6f76e6c3279.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[125   0 506 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6912\n",
      "Image 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg: box, Data: [[  0   0 639 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10007_jpg.rf.1416d6bd069f949ea04c90aa9b2883df.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 639 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7873\n",
      "Image 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg: point, Data: [[499 249]\n",
      " [467 439]\n",
      " [359 401]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10166_jpg.rf.098905046557acae10743723662ff2c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[499 249]\n",
      " [467 439]\n",
      " [359 401]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6937\n",
      "Image 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg: box, Data: [[  0 101 639 591]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 150621-9814169_jpg.rf.a56d598785853f82ce07044c9c140d49.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0 101 639 591]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6836\n",
      "Image 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg: box, Data: [[  6  10 632 510]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 4740-1610944637-115-bonnessinc1_jpg.rf.46fdd3274f61955312325faab08e1f4c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  6  10 632 510]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6970\n",
      "Image 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg: point, Data: [[396 482]\n",
      " [312 571]\n",
      " [425 502]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-62_jpg.rf.24ff3e487a7332a943d713353c2f78a6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[396 482]\n",
      " [312 571]\n",
      " [425 502]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2641\n",
      "Image 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg: box, Data: [[405   0 518 480]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-173_jpg.rf.598b2610183ae4d1b8fc62802dbc6b5d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[405   0 518 480]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6924\n",
      "Image Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg: box, Data: [[  0   0 638 580]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Paint-Peeling-Concrete-Efflorescence-v1_jpg.rf.9ff720154c51142e6198f812c3104635.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 638 580]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7183\n",
      "Image 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg: box, Data: [[  4 563 104 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-106_jpg.rf.da17d16744f1d74ae9a018ecdfd2edae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  4 563 104 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6922\n",
      "Image 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg: point, Data: [[119 334]\n",
      " [130 374]\n",
      " [148 372]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10008_jpg.rf.45f3d5ba55cb6d0da9d0bbf74f6f1e61.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[119 334]\n",
      " [130 374]\n",
      " [148 372]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7441\n",
      "Image 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg: point, Data: [[457 521]\n",
      " [407 622]\n",
      " [493 561]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-25_jpg.rf.89fedda422544fa940d360198840dd36.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[457 521]\n",
      " [407 622]\n",
      " [493 561]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6604\n",
      "Image 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg: point, Data: [[209  64]\n",
      " [ 88  96]\n",
      " [198  75]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-252_jpg.rf.7763fecc99192a2851b40df969da48b6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[209  64]\n",
      " [ 88  96]\n",
      " [198  75]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1404\n",
      "Image 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg: point, Data: [[161 327]\n",
      " [ 26 227]\n",
      " [207 482]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-28_jpg.rf.589cec475204ebd24ea3dde02dd74ad0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[161 327]\n",
      " [ 26 227]\n",
      " [207 482]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6800\n",
      "Image what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg: point, Data: [[436 158]\n",
      " [306 405]\n",
      " [283 260]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image what-precautions-can-be-taken-to-prevent-efflorescence-1_png.rf.c0709503ca576fc78095821d06688099.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[436 158]\n",
      " [306 405]\n",
      " [283 260]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9904\n",
      "Image 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg: point, Data: [[290 235]\n",
      " [316 317]\n",
      " [294 249]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10087_jpg.rf.3e5aacf0b3e6e54505915accbfc4f353.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[290 235]\n",
      " [316 317]\n",
      " [294 249]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6913\n",
      "Image 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg: box, Data: [[579 438 607 539]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-68_jpg.rf.1b26f2395d650f9d5ebec91176fd837f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[579 438 607 539]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6920\n",
      "Image 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg: box, Data: [[321 260 455 638]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10178_jpg.rf.1f78ea66ce0947b3d01710bfd8682a1f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[321 260 455 638]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6692\n",
      "Image 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg: point, Data: [[528  23]\n",
      " [193 321]\n",
      " [475 317]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-2_jpg.rf.d9e064a9798705f7a4e53be7de682da9.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[528  23]\n",
      " [193 321]\n",
      " [475 317]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0038\n",
      "Image 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg: box, Data: [[231 374 533 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-231_jpg.rf.5222674aaf4683b2ac788f894bc5e2a1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[231 374 533 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6820\n",
      "Image 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg: box, Data: [[ 11  46 232 248]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-64_jpg.rf.383207926cadd063fcb7babe2a41cae1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 11  46 232 248]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6904\n",
      "Image 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg: box, Data: [[513   0 637 274]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-85_jpg.rf.5b5d3a99dc4b160c2c9b798b78c0fc7e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[513   0 637 274]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6906\n",
      "Image 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg: box, Data: [[ 74 311 439 491]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-178_jpg.rf.181cec74b3e97d40ee0751443f037ed6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 74 311 439 491]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6933\n",
      "Image 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg: point, Data: [[577 611]\n",
      " [469 505]\n",
      " [397 434]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-98_jpg.rf.4362f948e96ba2cf93e6065f4279c591.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[577 611]\n",
      " [469 505]\n",
      " [397 434]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7396\n",
      "Image How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg: box, Data: [[177 129 345 494]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image How-To-Avoid-Defects-In-Concrete-Slabs03-650x488_jpg.rf.da838ec3346f390169265ee56dccd17a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[177 129 345 494]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6565\n",
      "Image 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg: point, Data: [[542  73]\n",
      " [537  33]\n",
      " [558  37]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-176_jpg.rf.83b7e7afd7088b0970c937e3b2cbea17.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[542  73]\n",
      " [537  33]\n",
      " [558  37]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6911\n",
      "Image 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg: box, Data: [[  0   3 436 309]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-43_jpg.rf.0215b2bdf6fd35f46ae1eb53963ef0ba.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   3 436 309]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6900\n",
      "Image istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg: point, Data: [[416 389]\n",
      " [375 328]\n",
      " [370 281]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image istockphoto-1414289046-2048x2048_jpg.rf.fd6b2571cd5315a34d953c2e230960cb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[416 389]\n",
      " [375 328]\n",
      " [370 281]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6636\n",
      "Image 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg: box, Data: [[114   0 382 496]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-119_jpg.rf.e0ce37f1bff76f9b58b38d68f3cee873.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[114   0 382 496]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7025\n",
      "Image 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg: box, Data: [[127 101 260 134]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-57_jpg.rf.d39f172752759974ce03b45eab8ad424.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[127 101 260 134]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6922\n",
      "Image 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg: box, Data: [[416 148 555 225]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-40_jpg.rf.540526189895291a0886c216e70f5d07.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[416 148 555 225]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6933\n",
      "Image 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg: box, Data: [[  2 387 412 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-145_jpg.rf.08dca9d2806d367ccb12235ef3582322.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  2 387 412 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6886\n",
      "Image Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg: point, Data: [[498 505]\n",
      " [486 530]\n",
      " [503 521]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Mold-and-moss-growths-on-the-exposed-surface-of-a-concrete-retaining-wall_Q320_jpg.rf.09825a87dc48bca3348d593832f87f95.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[498 505]\n",
      " [486 530]\n",
      " [503 521]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6911\n",
      "Image 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg: box, Data: [[  0   0 639 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10112_jpg.rf.d735da20eab14c3a0330469b87e773c4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 639 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7416\n",
      "Image 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg: point, Data: [[378 238]\n",
      " [413 146]\n",
      " [211 445]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 1-s2_0-S0950061807002760-gr14_jpg.rf.659df79f6004d29096cb4968687f85c5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[378 238]\n",
      " [413 146]\n",
      " [211 445]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1194\n",
      "Image 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg: point, Data: [[469 209]\n",
      " [622  11]\n",
      " [363  67]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-27_jpg.rf.32b872fee0a47bfc46012e1953ad37b7.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[469 209]\n",
      " [622  11]\n",
      " [363  67]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0904\n",
      "Image efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg: box, Data: [[  0   0 638 520]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image efflorescence_jpg.rf.c1e01a6e84431ec5512e760103980104.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 638 520]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5794\n",
      "Image 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg: box, Data: [[  1 128 639 587]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-49_jpg.rf.c84e866e49dcd21b6ee3c4918930abc2.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  1 128 639 587]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6845\n",
      "Image 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg: box, Data: [[  4 415 338 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-36_jpg.rf.36315cfbdd8e94483c450aef485975d1.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  4 415 338 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6889\n",
      "Image 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg: point, Data: [[ 76 205]\n",
      " [471  99]\n",
      " [179 193]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10040_jpg.rf.2ea388b09e957a78f8ae53f250b0e327.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 76 205]\n",
      " [471  99]\n",
      " [179 193]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1512\n",
      "Image 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg: point, Data: [[ 81  27]\n",
      " [157  81]\n",
      " [147 118]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-50_jpg.rf.14d0aed71874b2c407566e89dbe6e574.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 81  27]\n",
      " [157  81]\n",
      " [147 118]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2280\n",
      "Image d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg: box, Data: [[152   1 380 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image d9JA_pYYkhvr2MaAyxf5yrXu-abFjaVLU2y8fzVM-UM_webp.rf.889df38050be2df070875b1bc65d5fbb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[152   1 380 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6867\n",
      "Image images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg: point, Data: [[242 192]\n",
      " [433 276]\n",
      " [359 332]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-5-_jpg.rf.b50e9e1540f817fcde7a837086b2a721.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[242 192]\n",
      " [433 276]\n",
      " [359 332]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6796\n",
      "Image 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg: point, Data: [[ 56 337]\n",
      " [638 369]\n",
      " [501 205]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10190_jpg.rf.32f5f21232b3650430fce2d67c9d174e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 56 337]\n",
      " [638 369]\n",
      " [501 205]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6164\n",
      "Image 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg: point, Data: [[564 248]\n",
      " [550 102]\n",
      " [550 234]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-78_jpg.rf.6fd31327ef91eafeb7f7b7c0e8e868c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[564 248]\n",
      " [550 102]\n",
      " [550 234]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6915\n",
      "Image 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg: point, Data: [[310  75]\n",
      " [327 109]\n",
      " [592 627]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-33_jpg.rf.c351d630b6236d86f228d9d9d1a89e3e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[310  75]\n",
      " [327 109]\n",
      " [592 627]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7019\n",
      "Image concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg: point, Data: [[334 146]\n",
      " [298 415]\n",
      " [320 162]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-pop-outs-v0-g9qip9omq14d1_webp.rf.1b4105ad530699d4a1c46f6792288f5d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[334 146]\n",
      " [298 415]\n",
      " [320 162]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6902\n",
      "Image images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg: point, Data: [[391 458]\n",
      " [416 447]\n",
      " [400 524]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-7-_jpg.rf.22ef7b37ede5270626a074a33c9d3ee4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[391 458]\n",
      " [416 447]\n",
      " [400 524]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6653\n",
      "Image 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg: point, Data: [[225 479]\n",
      " [325 205]\n",
      " [223 157]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10000_jpg.rf.6e0ac7380eef2de55104eb7318f7c764.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[225 479]\n",
      " [325 205]\n",
      " [223 157]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9917\n",
      "Image 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg: point, Data: [[467 332]\n",
      " [458 497]\n",
      " [240 228]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10114_jpg.rf.ec3e67a01d702e0c9717730fb202af51.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[467 332]\n",
      " [458 497]\n",
      " [240 228]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8743\n",
      "Image 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg: point, Data: [[385  90]\n",
      " [231  65]\n",
      " [420 286]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-21_jpg.rf.660d65698532d4bc772194b59fd88ec6.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[385  90]\n",
      " [231  65]\n",
      " [420 286]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.4532\n",
      "Image 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg: point, Data: [[352 227]\n",
      " [349 136]\n",
      " [616 429]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-54_jpg.rf.4dd7cf9aefa8dc0a6bdf5d4d92adfcf5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[352 227]\n",
      " [349 136]\n",
      " [616 429]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9353\n",
      "Image 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg: point, Data: [[ 18 131]\n",
      " [454 331]\n",
      " [470 470]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10029_jpg.rf.5676ba96c6ffe9900166982e30d20100.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 18 131]\n",
      " [454 331]\n",
      " [470 470]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.9774\n",
      "Image 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg: box, Data: [[  5 171 318 605]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-79_jpg.rf.b3a69831f811821d75684580f822e747.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  5 171 318 605]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6923\n",
      "Image stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg: box, Data: [[ 17  73 524 595]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image stickers-hole-in-the-concrete_jpg_jpg.rf.08a7f1627d3741cdbf5e749326cffd34.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 17  73 524 595]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6681\n",
      "Image 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg: box, Data: [[  0  98 637 524]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-41_jpg.rf.c812fb8db902c33844c6508bec50723b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0  98 637 524]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6829\n",
      "Image 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg: point, Data: [[482 496]\n",
      " [ 82 151]\n",
      " [173 463]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10158_jpg.rf.f7edb0f544762ceb2aeef1484f315431.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[482 496]\n",
      " [ 82 151]\n",
      " [173 463]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0463\n",
      "Image 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg: box, Data: [[319 371 639 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-32_jpg.rf.e29ad449318c84248a2276567d899c00.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[319 371 639 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6886\n",
      "Image 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg: point, Data: [[293  81]\n",
      " [305 111]\n",
      " [286  95]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-117_jpg.rf.4a74e72152ec21acdb23307c96afa741.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[293  81]\n",
      " [305 111]\n",
      " [286  95]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6920\n",
      "Image images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg: box, Data: [[ 65   0 513 417]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-4-_jpg.rf.27dd99d567f86c87ac08b35cf5e2c5c0.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 65   0 513 417]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6888\n",
      "Image 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg: point, Data: [[493 262]\n",
      " [322 284]\n",
      " [460 311]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10043_jpg.rf.4daaee7ffb5a45ced625aff50d6e5102.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[493 262]\n",
      " [322 284]\n",
      " [460 311]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.8101\n",
      "Image images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg: box, Data: [[ 84  38 319 507]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-2-_jpg.rf.5f0e3f99b9fc05cc42f0b52ac42edd29.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 84  38 319 507]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6351\n",
      "Image 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg: point, Data: [[187 371]\n",
      " [179 375]\n",
      " [136 361]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-151_jpg.rf.2019ed506b6bdc98377f8f8b1b07a01c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[187 371]\n",
      " [179 375]\n",
      " [136 361]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2322\n",
      "Image 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg: point, Data: [[122 189]\n",
      " [ 77 443]\n",
      " [113 435]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-97_jpg.rf.016e30f947f5c5cce9df2db39e9464ef.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[122 189]\n",
      " [ 77 443]\n",
      " [113 435]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7905\n",
      "Image lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg: box, Data: [[242  65 439 419]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image lignite-problems_jpg.rf.7f45b5cbd0f4997672dc9e28a6a6377f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[242  65 439 419]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6908\n",
      "Image 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg: point, Data: [[138 541]\n",
      " [292  90]\n",
      " [176 565]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-112_jpg.rf.9df6408555e4538f02a3c2d3cd3a7464.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[138 541]\n",
      " [292  90]\n",
      " [176 565]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2886\n",
      "Image 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg: point, Data: [[596 627]\n",
      " [579 590]\n",
      " [512 568]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10185_jpg.rf.b559324b8c49d2bad5fba8cc8b1f4514.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[596 627]\n",
      " [579 590]\n",
      " [512 568]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7074\n",
      "Image 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg: point, Data: [[596 345]\n",
      " [571 312]\n",
      " [629 396]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-225_jpg.rf.c643b70185f5b8e3683a1c4902dac8ac.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[596 345]\n",
      " [571 312]\n",
      " [629 396]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2638\n",
      "Image 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg: point, Data: [[523  91]\n",
      " [408  31]\n",
      " [449 138]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-236_jpg.rf.17cb303a0fb24961e2aa9e0e5d44a4e4.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[523  91]\n",
      " [408  31]\n",
      " [449 138]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.1197\n",
      "Image 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg: box, Data: [[  0   0 523 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-67_jpg.rf.f9205680af41fb62e4c6c8ed89d20c72.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 523 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6861\n",
      "Image 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg: point, Data: [[ 35 462]\n",
      " [ 10 502]\n",
      " [590 511]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10140_jpg.rf.6f4995090d26ee306664928da0f1c67c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 35 462]\n",
      " [ 10 502]\n",
      " [590 511]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.0363\n",
      "Image images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg: box, Data: [[116   0 382 466]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-8-_jpg.rf.2521cc1c4b93646a10257ba7b4240f0d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[116   0 382 466]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6428\n",
      "Image concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg: point, Data: [[360 403]\n",
      " [365 386]\n",
      " [371 364]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image concrete-popout-2_jpg.rf.f4a343f9a7590e1f50e217fa61e88793.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[360 403]\n",
      " [365 386]\n",
      " [371 364]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6922\n",
      "Image Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg: point, Data: [[224 300]\n",
      " [147 296]\n",
      " [269 228]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Concrete-Popout-Best-Practices-Header-Photo_jpg.rf.c62149f8f99880a81cec08325f421374.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[224 300]\n",
      " [147 296]\n",
      " [269 228]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6783\n",
      "Image images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg: point, Data: [[471 262]\n",
      " [563 511]\n",
      " [ 39 527]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image images-1-_jpg.rf.7a4eb6b609b3316a9cd782b8b6c6846e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[471 262]\n",
      " [563 511]\n",
      " [ 39 527]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6787\n",
      "Image 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg: point, Data: [[171 366]\n",
      " [178 389]\n",
      " [189 349]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-55_jpg.rf.2088ae25f0fd7db4bc69e1a8ff3c486f.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[171 366]\n",
      " [178 389]\n",
      " [189 349]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6930\n",
      "Image old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg: point, Data: [[201 360]\n",
      " [221 349]\n",
      " [296 202]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image old-cement-concrete-wall-texture-vintage-wall-background_526818-1094_avif.rf.ce7be1cc0a63f5e8f2b27406ef5c1325.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[201 360]\n",
      " [221 349]\n",
      " [296 202]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6868\n",
      "Image 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg: point, Data: [[242 282]\n",
      " [507 490]\n",
      " [ 81 371]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7001-42_jpg.rf.a58e84acd5380ffe44aa164af7582f1c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[242 282]\n",
      " [507 490]\n",
      " [ 81 371]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7024\n",
      "Image 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg: point, Data: [[258 238]\n",
      " [257  82]\n",
      " [261 431]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-104_jpg.rf.e04b07f398cbabf34cb0ca257b85a05b.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[258 238]\n",
      " [257  82]\n",
      " [261 431]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7188\n",
      "Image 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg: box, Data: [[103   0 310 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-50_jpg.rf.7ccc223d2f30facb120fe6494ce561ae.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[103   0 310 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6858\n",
      "Image 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg: point, Data: [[ 82 510]\n",
      " [ 64 463]\n",
      " [ 23 511]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-14_jpg.rf.f16859ed6130c5b594f7fe08f11321cb.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[ 82 510]\n",
      " [ 64 463]\n",
      " [ 23 511]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6910\n",
      "Image IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg: point, Data: [[133 218]\n",
      " [273  34]\n",
      " [433 265]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image IMG_6977-1200x900_jpg.rf.1d5010d5b4291f5adac85d100fd1b1e9.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[133 218]\n",
      " [273  34]\n",
      " [433 265]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.7064\n",
      "Image 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg: box, Data: [[  0   0 512 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10122_jpg.rf.6d94ed03ab946ad9415e7ca267078664.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 512 639]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.5652\n",
      "Image Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg: box, Data: [[  0   0 636 612]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image Gray-porous-concrete-507681168-_avif.rf.21682f5c31c9395afd65d4890426422c.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0   0 636 612]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2551\n",
      "Image 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg: box, Data: [[ 51   0 612 395]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-195_jpg.rf.148ac24bbb383d9d65e857c70483660a.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 51   0 612 395]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6981\n",
      "Image 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg: box, Data: [[  0  89 513 495]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 10180_jpg.rf.68ad33fe9f4ad8837e9b5ae6c3f287b5.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[  0  89 513 495]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6914\n",
      "Image 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg: point, Data: [[265 493]\n",
      " [ 66 224]\n",
      " [ 51 220]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7003-211_jpg.rf.5d4f750da83a91f566a44f18a2476b1d.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: point, Data: [[265 493]\n",
      " [ 66 224]\n",
      " [ 51 220]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 1.2393\n",
      "Image 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg: box, Data: [[ 65   0 293 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 7002-9_jpg.rf.38554ed93152dbdd3eefc3f0b8c13f84.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[ 65   0 293 637]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6845\n",
      "Image 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg: box, Data: [[278 218 428 414]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Training: Image 80Gd9_jpg.rf.d519536f86bd60b55bdda97ca14c921e.jpg\n",
      "Training: gt_mask unique: tensor([0., 1.], device='cuda:0'), Shape: torch.Size([640, 640])\n",
      "Training: Prompt type: box, Data: [[278 218 428 414]]\n",
      "Training: pred_mask unique: tensor([False,  True], device='cuda:0'), Shape: torch.Size([640, 640]), Loss: 0.6804\n",
      "Epoch 10/10, Loss: 0.7611\n",
      "Saving checkpoint: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\checkpoints\\sam2_epoch_10.pt\n",
      "Successfully saved checkpoint: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\checkpoints\\sam2_epoch_10.pt\n",
      "\n",
      "Evaluating final model on test set...\n",
      "Image 7003-229_jpg.rf.ccc1d1d5c07631705c3fb7133d2615cc.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-229_jpg.rf.ccc1d1d5c07631705c3fb7133d2615cc.jpg: point, Data: [[126 552]\n",
      " [186 236]\n",
      " [121 563]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7003-229_jpg.rf.ccc1d1d5c07631705c3fb7133d2615cc.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7003-229_jpg.rf.ccc1d1d5c07631705c3fb7133d2615cc.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_7003-229_jpg.rf.ccc1d1d5c07631705c3fb7133d2615cc.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_7003-229_jpg.rf.ccc1d1d5c07631705c3fb7133d2615cc.jpg\n",
      "Image 7002-32_jpg.rf.213c0213080f977d4f234ef16c735c19.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-32_jpg.rf.213c0213080f977d4f234ef16c735c19.jpg: point, Data: [[500 513]\n",
      " [391 545]\n",
      " [328 563]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-32_jpg.rf.213c0213080f977d4f234ef16c735c19.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-32_jpg.rf.213c0213080f977d4f234ef16c735c19.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_7002-32_jpg.rf.213c0213080f977d4f234ef16c735c19.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_7002-32_jpg.rf.213c0213080f977d4f234ef16c735c19.jpg\n",
      "Image images_jpg.rf.4620688301a466b2b8e39a7c90cf336c.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images_jpg.rf.4620688301a466b2b8e39a7c90cf336c.jpg: box, Data: [[222 257 439 314]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image images_jpg.rf.4620688301a466b2b8e39a7c90cf336c.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image images_jpg.rf.4620688301a466b2b8e39a7c90cf336c.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_images_jpg.rf.4620688301a466b2b8e39a7c90cf336c.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_images_jpg.rf.4620688301a466b2b8e39a7c90cf336c.jpg\n",
      "Image 7001-237_jpg.rf.cde393ce6f56079c4d8e0b402911a737.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-237_jpg.rf.cde393ce6f56079c4d8e0b402911a737.jpg: box, Data: [[  4 108 146 573]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7001-237_jpg.rf.cde393ce6f56079c4d8e0b402911a737.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7001-237_jpg.rf.cde393ce6f56079c4d8e0b402911a737.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_7001-237_jpg.rf.cde393ce6f56079c4d8e0b402911a737.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_7001-237_jpg.rf.cde393ce6f56079c4d8e0b402911a737.jpg\n",
      "Image 7003-210_jpg.rf.bb1d19d4f9c091fda06ab90d2c45f360.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-210_jpg.rf.bb1d19d4f9c091fda06ab90d2c45f360.jpg: point, Data: [[635  59]\n",
      " [636  85]\n",
      " [575  61]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7003-210_jpg.rf.bb1d19d4f9c091fda06ab90d2c45f360.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7003-210_jpg.rf.bb1d19d4f9c091fda06ab90d2c45f360.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_7003-210_jpg.rf.bb1d19d4f9c091fda06ab90d2c45f360.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_7003-210_jpg.rf.bb1d19d4f9c091fda06ab90d2c45f360.jpg\n",
      "Image 10027_jpg.rf.55e2d7bb65337d9e70834777394f5685.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10027_jpg.rf.55e2d7bb65337d9e70834777394f5685.jpg: box, Data: [[218 203 304 569]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 10027_jpg.rf.55e2d7bb65337d9e70834777394f5685.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 10027_jpg.rf.55e2d7bb65337d9e70834777394f5685.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_10027_jpg.rf.55e2d7bb65337d9e70834777394f5685.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_10027_jpg.rf.55e2d7bb65337d9e70834777394f5685.jpg\n",
      "Image what-factors-cause-ffflorescence-in-concrete_png.rf.08778673141d33f4a8c9a006f39d4558.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for what-factors-cause-ffflorescence-in-concrete_png.rf.08778673141d33f4a8c9a006f39d4558.jpg: box, Data: [[  0   0 506 637]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image what-factors-cause-ffflorescence-in-concrete_png.rf.08778673141d33f4a8c9a006f39d4558.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image what-factors-cause-ffflorescence-in-concrete_png.rf.08778673141d33f4a8c9a006f39d4558.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_what-factors-cause-ffflorescence-in-concrete_png.rf.08778673141d33f4a8c9a006f39d4558.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_what-factors-cause-ffflorescence-in-concrete_png.rf.08778673141d33f4a8c9a006f39d4558.jpg\n",
      "Image images-9-_jpg.rf.2ebbcb3e4302a362204396cc561c1042.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for images-9-_jpg.rf.2ebbcb3e4302a362204396cc561c1042.jpg: box, Data: [[242 189 563 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image images-9-_jpg.rf.2ebbcb3e4302a362204396cc561c1042.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image images-9-_jpg.rf.2ebbcb3e4302a362204396cc561c1042.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_images-9-_jpg.rf.2ebbcb3e4302a362204396cc561c1042.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_images-9-_jpg.rf.2ebbcb3e4302a362204396cc561c1042.jpg\n",
      "Image 7002-212_jpg.rf.1afab48f456eb3ad4068215a60941b60.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-212_jpg.rf.1afab48f456eb3ad4068215a60941b60.jpg: box, Data: [[533  85 632 572]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-212_jpg.rf.1afab48f456eb3ad4068215a60941b60.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-212_jpg.rf.1afab48f456eb3ad4068215a60941b60.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_7002-212_jpg.rf.1afab48f456eb3ad4068215a60941b60.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_7002-212_jpg.rf.1afab48f456eb3ad4068215a60941b60.jpg\n",
      "Image 3-different-cement-plaster-walls-checking-for-asbestos-how-v0-s0xlgmba8a3a1_webp.rf.408560d6847a08307f7fc5aabc7b2892.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 3-different-cement-plaster-walls-checking-for-asbestos-how-v0-s0xlgmba8a3a1_webp.rf.408560d6847a08307f7fc5aabc7b2892.jpg: point, Data: [[338 347]\n",
      " [343 330]\n",
      " [280 311]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 3-different-cement-plaster-walls-checking-for-asbestos-how-v0-s0xlgmba8a3a1_webp.rf.408560d6847a08307f7fc5aabc7b2892.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 3-different-cement-plaster-walls-checking-for-asbestos-how-v0-s0xlgmba8a3a1_webp.rf.408560d6847a08307f7fc5aabc7b2892.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_3-different-cement-plaster-walls-checking-for-asbestos-how-v0-s0xlgmba8a3a1_webp.rf.408560d6847a08307f7fc5aabc7b2892.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_3-different-cement-plaster-walls-checking-for-asbestos-how-v0-s0xlgmba8a3a1_webp.rf.408560d6847a08307f7fc5aabc7b2892.jpg\n",
      "Image efflorescence-through-warehouse-floor-v0-y84sh8ptya1d1_webp.rf.d757adf7e82c8ccc3ae74ec247a09e1d.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for efflorescence-through-warehouse-floor-v0-y84sh8ptya1d1_webp.rf.d757adf7e82c8ccc3ae74ec247a09e1d.jpg: box, Data: [[204   0 392 638]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image efflorescence-through-warehouse-floor-v0-y84sh8ptya1d1_webp.rf.d757adf7e82c8ccc3ae74ec247a09e1d.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image efflorescence-through-warehouse-floor-v0-y84sh8ptya1d1_webp.rf.d757adf7e82c8ccc3ae74ec247a09e1d.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_efflorescence-through-warehouse-floor-v0-y84sh8ptya1d1_webp.rf.d757adf7e82c8ccc3ae74ec247a09e1d.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_efflorescence-through-warehouse-floor-v0-y84sh8ptya1d1_webp.rf.d757adf7e82c8ccc3ae74ec247a09e1d.jpg\n",
      "Image 7001-169_jpg.rf.ef42c68ee85480eb7be668f5f42a8130.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-169_jpg.rf.ef42c68ee85480eb7be668f5f42a8130.jpg: point, Data: [[177 193]\n",
      " [168 192]\n",
      " [167 155]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7001-169_jpg.rf.ef42c68ee85480eb7be668f5f42a8130.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7001-169_jpg.rf.ef42c68ee85480eb7be668f5f42a8130.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_7001-169_jpg.rf.ef42c68ee85480eb7be668f5f42a8130.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_7001-169_jpg.rf.ef42c68ee85480eb7be668f5f42a8130.jpg\n",
      "Image old-cement-wall-background_633872-2023_avif.rf.a1bfb742d0a28f617bfee60cf1af0e18.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for old-cement-wall-background_633872-2023_avif.rf.a1bfb742d0a28f617bfee60cf1af0e18.jpg: box, Data: [[ 85  50 494 473]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image old-cement-wall-background_633872-2023_avif.rf.a1bfb742d0a28f617bfee60cf1af0e18.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image old-cement-wall-background_633872-2023_avif.rf.a1bfb742d0a28f617bfee60cf1af0e18.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_old-cement-wall-background_633872-2023_avif.rf.a1bfb742d0a28f617bfee60cf1af0e18.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_old-cement-wall-background_633872-2023_avif.rf.a1bfb742d0a28f617bfee60cf1af0e18.jpg\n",
      "Image 7003-227_jpg.rf.2267d6d3995551fbb56e2d98a04c6a44.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7003-227_jpg.rf.2267d6d3995551fbb56e2d98a04c6a44.jpg: box, Data: [[146 326 343 639]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7003-227_jpg.rf.2267d6d3995551fbb56e2d98a04c6a44.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7003-227_jpg.rf.2267d6d3995551fbb56e2d98a04c6a44.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_7003-227_jpg.rf.2267d6d3995551fbb56e2d98a04c6a44.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_7003-227_jpg.rf.2267d6d3995551fbb56e2d98a04c6a44.jpg\n",
      "Image capture-decran-2021-09-16-a-09_58_16_png.rf.a0530ef4f6f0d8d78941865997b83a1b.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for capture-decran-2021-09-16-a-09_58_16_png.rf.a0530ef4f6f0d8d78941865997b83a1b.jpg: point, Data: [[ 96 199]\n",
      " [517 538]\n",
      " [245 311]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image capture-decran-2021-09-16-a-09_58_16_png.rf.a0530ef4f6f0d8d78941865997b83a1b.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image capture-decran-2021-09-16-a-09_58_16_png.rf.a0530ef4f6f0d8d78941865997b83a1b.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_capture-decran-2021-09-16-a-09_58_16_png.rf.a0530ef4f6f0d8d78941865997b83a1b.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_capture-decran-2021-09-16-a-09_58_16_png.rf.a0530ef4f6f0d8d78941865997b83a1b.jpg\n",
      "Image 7001-187_jpg.rf.fa890a6f61c1013fd75a2db824977067.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-187_jpg.rf.fa890a6f61c1013fd75a2db824977067.jpg: box, Data: [[303 228 361 609]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7001-187_jpg.rf.fa890a6f61c1013fd75a2db824977067.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7001-187_jpg.rf.fa890a6f61c1013fd75a2db824977067.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_7001-187_jpg.rf.fa890a6f61c1013fd75a2db824977067.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_7001-187_jpg.rf.fa890a6f61c1013fd75a2db824977067.jpg\n",
      "Image 10154_jpg.rf.70dd4ba4075dce8ca613f032491b0084.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 10154_jpg.rf.70dd4ba4075dce8ca613f032491b0084.jpg: box, Data: [[  0   0 639 363]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 10154_jpg.rf.70dd4ba4075dce8ca613f032491b0084.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 10154_jpg.rf.70dd4ba4075dce8ca613f032491b0084.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_10154_jpg.rf.70dd4ba4075dce8ca613f032491b0084.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_10154_jpg.rf.70dd4ba4075dce8ca613f032491b0084.jpg\n",
      "Image 7001-115_jpg.rf.8fe54a3b7cd04ee6fec4d3962f74a187.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7001-115_jpg.rf.8fe54a3b7cd04ee6fec4d3962f74a187.jpg: point, Data: [[162 570]\n",
      " [179 467]\n",
      " [150 520]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7001-115_jpg.rf.8fe54a3b7cd04ee6fec4d3962f74a187.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7001-115_jpg.rf.8fe54a3b7cd04ee6fec4d3962f74a187.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_7001-115_jpg.rf.8fe54a3b7cd04ee6fec4d3962f74a187.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_7001-115_jpg.rf.8fe54a3b7cd04ee6fec4d3962f74a187.jpg\n",
      "Image 7002-190_jpg.rf.81b40fc7537b293d7a8923af293d7deb.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for 7002-190_jpg.rf.81b40fc7537b293d7a8923af293d7deb.jpg: box, Data: [[453  70 506 291]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image 7002-190_jpg.rf.81b40fc7537b293d7a8923af293d7deb.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image 7002-190_jpg.rf.81b40fc7537b293d7a8923af293d7deb.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_7002-190_jpg.rf.81b40fc7537b293d7a8923af293d7deb.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_7002-190_jpg.rf.81b40fc7537b293d7a8923af293d7deb.jpg\n",
      "Image a-Pop-outs-near-the-drain-channel-b-crushed-aggregate-particles-with-gel-in-the-bottom_Q320_jpg.rf.d656099cd7f5757aec7b1d3e03f27fb4.jpg: Mask unique values: [0 1], Shape: (640, 640)\n",
      "Prompt for a-Pop-outs-near-the-drain-channel-b-crushed-aggregate-particles-with-gel-in-the-bottom_Q320_jpg.rf.d656099cd7f5757aec7b1d3e03f27fb4.jpg: point, Data: [[304 247]\n",
      " [492 308]\n",
      " [404 183]]\n",
      "Transformed mask unique values: tensor([0., 1.]), Shape: torch.Size([640, 640])\n",
      "Image a-Pop-outs-near-the-drain-channel-b-crushed-aggregate-particles-with-gel-in-the-bottom_Q320_jpg.rf.d656099cd7f5757aec7b1d3e03f27fb4.jpg: gt_mask unique: [0 1], Shape: (640, 640)\n",
      "Image a-Pop-outs-near-the-drain-channel-b-crushed-aggregate-particles-with-gel-in-the-bottom_Q320_jpg.rf.d656099cd7f5757aec7b1d3e03f27fb4.jpg: pred_mask unique: [0 1], Shape: (640, 640)\n",
      "Saving masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_a-Pop-outs-near-the-drain-channel-b-crushed-aggregate-particles-with-gel-in-the-bottom_Q320_jpg.rf.d656099cd7f5757aec7b1d3e03f27fb4.jpg\n",
      "Successfully saved masked image: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_predictions\\pred_a-Pop-outs-near-the-drain-channel-b-crushed-aggregate-particles-with-gel-in-the-bottom_Q320_jpg.rf.d656099cd7f5757aec7b1d3e03f27fb4.jpg\n",
      "Test Set Metrics with point prompts:\n",
      "Mean IoU: 0.3732\n",
      "Mean Precision: 0.8435\n",
      "Mean Recall: 0.4824\n",
      "Mean F1: 0.4831\n",
      "Saving plot: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_metrics.png\n",
      "Successfully saved plot: D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\\outputs_20250418_022540\\test_metrics.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pycocotools.coco import COCO\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import random\n",
    "import supervision as sv\n",
    "from PIL import Image\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from hydra.core.global_hydra import GlobalHydra\n",
    "import matplotlib\n",
    "# Set non-interactive backend for matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "# Suppress deprecation warnings\n",
    "try:\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "except AttributeError as e:\n",
    "    print(f\"Warning: Could not suppress deprecation warnings: {e}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Function to sanitize filenames\n",
    "def sanitize_filename(filename):\n",
    "    # Replace invalid characters with underscores\n",
    "    return re.sub(r'[^\\w\\-\\.]', '_', filename)\n",
    "\n",
    "# Dataset class for COCO segmentation\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, root_dir, annotation_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.image_ids = self.coco.getImgIds()\n",
    "        self.cat_ids = self.coco.getCatIds()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(self.root_dir, img_info['file_name'])\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            print(f\"Failed to load image: {img_path}\")\n",
    "            return None\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=self.cat_ids, iscrowd=None)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        \n",
    "        masks = []\n",
    "        for ann in anns:\n",
    "            mask = self.coco.annToMask(ann)\n",
    "            masks.append(mask)\n",
    "        if masks:\n",
    "            mask = np.sum(masks, axis=0).clip(0, 1).astype(np.uint8)\n",
    "        else:\n",
    "            mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)\n",
    "        \n",
    "        # Debug: Log mask statistics\n",
    "        print(f\"Image {img_info['file_name']}: Mask unique values: {np.unique(mask)}, Shape: {mask.shape}\")\n",
    "\n",
    "        # Generate prompt (multiple points or box)\n",
    "        if np.any(mask):\n",
    "            if random.random() < 0.5:\n",
    "                # Multiple point prompts (up to 3 points)\n",
    "                y, x = np.where(mask > 0)\n",
    "                if len(x) == 0:\n",
    "                    print(f\"Warning: Empty mask for {img_info['file_name']}\")\n",
    "                    prompt = {'type': 'point', 'data': np.array([[0, 0]]), 'label': np.array([0])}\n",
    "                else:\n",
    "                    num_points = min(3, len(x))\n",
    "                    indices = random.sample(range(len(x)), num_points)\n",
    "                    points = np.array([[x[i], y[i]] for i in indices])\n",
    "                    labels = np.ones(num_points, dtype=np.int32)\n",
    "                    prompt = {'type': 'point', 'data': points, 'label': labels}\n",
    "            else:\n",
    "                # Box prompt\n",
    "                y, x = np.where(mask > 0)\n",
    "                if len(x) == 0:\n",
    "                    print(f\"Warning: Empty mask for {img_info['file_name']}\")\n",
    "                    prompt = {'type': 'point', 'data': np.array([[0, 0]]), 'label': np.array([0])}\n",
    "                else:\n",
    "                    x_min, x_max = x.min(), x.max()\n",
    "                    y_min, y_max = y.min(), y.max()\n",
    "                    prompt = {'type': 'box', 'data': np.array([[x_min, y_min, x_max, y_max]])}\n",
    "        else:\n",
    "            prompt = {'type': 'point', 'data': np.array([[0, 0]]), 'label': np.array([0])}\n",
    "        \n",
    "        print(f\"Prompt for {img_info['file_name']}: {prompt['type']}, Data: {prompt['data']}\")\n",
    "\n",
    "        if self.transform:\n",
    "            image_pil = Image.fromarray(image)\n",
    "            mask_pil = Image.fromarray(mask)\n",
    "            image_pil, mask_pil = self.transform(image_pil, mask_pil)\n",
    "            image = image_pil\n",
    "            mask = mask_pil\n",
    "\n",
    "        return image, mask, prompt, img_info['file_name']\n",
    "\n",
    "# Custom collate function to preserve NumPy arrays for prompts\n",
    "def custom_collate_fn(batch):\n",
    "    # Filter out None items\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        return None\n",
    "    \n",
    "    images, masks, prompts, fnames = zip(*batch)\n",
    "    \n",
    "    # Stack images and masks as tensors\n",
    "    images = torch.stack(images)\n",
    "    masks = torch.stack(masks)\n",
    "    \n",
    "    # Keep prompts as list to preserve NumPy arrays\n",
    "    return images, masks, prompts, fnames\n",
    "\n",
    "# Data augmentation transforms\n",
    "def get_transform(augmentation=False):\n",
    "    if augmentation:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def dual_transform(img, mask):\n",
    "        img = transform(img)\n",
    "        # Ensure mask is binary uint8 [0, 1]\n",
    "        mask = np.array(mask, dtype=np.uint8)\n",
    "        mask = mask.clip(0, 1)\n",
    "        mask = torch.from_numpy(mask).float()  # Convert to float tensor for training\n",
    "        print(f\"Transformed mask unique values: {torch.unique(mask)}, Shape: {mask.shape}\")\n",
    "        return img, mask\n",
    "    return dual_transform\n",
    "\n",
    "# Function to evaluate prompt strategy\n",
    "def evaluate_prompt_strategy(predictor, data_loader, prompt_type, device, output_dir=None):\n",
    "    predictor.model.eval()\n",
    "    ious, precisions, recalls, f1s = [], [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image, gt_mask, prompt, fname in data_loader:\n",
    "            if image is None:\n",
    "                continue\n",
    "            image = image.to(device)\n",
    "            gt_mask = gt_mask.to(device).squeeze().float()  # Ensure float for consistency\n",
    "            gt_mask_np = gt_mask.cpu().numpy().astype(np.uint8)\n",
    "            gt_mask_np = (gt_mask_np > 0).astype(np.uint8)  # Ensure binary\n",
    "            \n",
    "            # Debug: Log mask stats\n",
    "            print(f\"Image {fname[0]}: gt_mask unique: {np.unique(gt_mask_np)}, Shape: {gt_mask_np.shape}\")\n",
    "            \n",
    "            predictor.set_image(image[0].permute(1, 2, 0).cpu().numpy())\n",
    "            masks, scores, _ = predictor.predict(\n",
    "                point_coords=prompt[0]['data'] if prompt[0]['type'] == 'point' else None,\n",
    "                point_labels=prompt[0]['label'] if prompt[0]['type'] == 'point' else None,\n",
    "                box=prompt[0]['data'][0] if prompt[0]['type'] == 'box' else None,\n",
    "                multimask_output=False\n",
    "            )\n",
    "            \n",
    "            pred_mask = masks[0].astype(np.uint8)\n",
    "            \n",
    "            print(f\"Image {fname[0]}: pred_mask unique: {np.unique(pred_mask)}, Shape: {pred_mask.shape}\")\n",
    "            \n",
    "            intersection = np.logical_and(gt_mask_np, pred_mask).sum()\n",
    "            union = np.logical_or(gt_mask_np, pred_mask).sum()\n",
    "            iou = intersection / union if union > 0 else 0\n",
    "            ious.append(iou)\n",
    "            \n",
    "            precision = precision_score(gt_mask_np.flatten(), pred_mask.flatten(), zero_division=0)\n",
    "            recall = recall_score(gt_mask_np.flatten(), pred_mask.flatten(), zero_division=0)\n",
    "            f1 = f1_score(gt_mask_np.flatten(), pred_mask.flatten(), zero_division=0)\n",
    "            \n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            f1s.append(f1)\n",
    "            \n",
    "            if output_dir:\n",
    "                img = image[0].permute(1, 2, 0).cpu().numpy()\n",
    "                img = (img * np.array([0.229, 0.224, 0.225])[None, None, :] + np.array([0.485, 0.456, 0.406])[None, None, :]) * 255\n",
    "                img = img.astype(np.uint8)\n",
    "                mask_colored = np.zeros_like(img)\n",
    "                mask_colored[:, :, 1] = pred_mask * 255  # Green mask\n",
    "                overlay = cv2.addWeighted(img, 0.7, mask_colored, 0.3, 0)\n",
    "                \n",
    "                label = f\"IoU: {iou:.2f}, F1: {f1:.2f}\"\n",
    "                cv2.putText(overlay, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "                \n",
    "                # Sanitize filename\n",
    "                sanitized_fname = sanitize_filename(fname[0])\n",
    "                output_path = os.path.join(output_dir, f\"pred_{sanitized_fname}\")\n",
    "                print(f\"Saving masked image: {output_path}\")\n",
    "                try:\n",
    "                    success = cv2.imwrite(output_path, cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n",
    "                    if success:\n",
    "                        print(f\"Successfully saved masked image: {output_path}\")\n",
    "                    else:\n",
    "                        print(f\"Failed to save masked image: {output_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving masked image {output_path}: {str(e)}\")\n",
    "    \n",
    "    return {\n",
    "        'mean_iou': np.mean(ious) if ious else 0,\n",
    "        'mean_precision': np.mean(precisions) if precisions else 0,\n",
    "        'mean_recall': np.mean(recalls) if recalls else 0,\n",
    "        'mean_f1': np.mean(f1s) if f1s else 0\n",
    "    }\n",
    "\n",
    "# Training function\n",
    "def train_sam2(predictor, train_loader, optimizer, device, epochs, checkpoint_dir):\n",
    "    model = predictor.model\n",
    "    model.train()\n",
    "    \n",
    "    # Inspect model attributes\n",
    "    print(\"Model attributes:\", dir(model))\n",
    "    has_encoder = hasattr(model, 'image_encoder')\n",
    "    print(f\"Model has image_encoder: {has_encoder}\")\n",
    "    \n",
    "    if not has_encoder:\n",
    "        raise AttributeError(\"Model missing image_encoder\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for image, gt_mask, prompt, fname in train_loader:\n",
    "            if image is None:\n",
    "                continue\n",
    "            image = image.to(device)\n",
    "            gt_mask = gt_mask.to(device).squeeze().float()  # Ensure float for loss\n",
    "            prompt = prompt[0]  # Unpack batch\n",
    "            \n",
    "            print(f\"Training: Image {fname[0]}\")\n",
    "            print(f\"Training: gt_mask unique: {torch.unique(gt_mask)}, Shape: {gt_mask.shape}\")\n",
    "            print(f\"Training: Prompt type: {prompt['type']}, Data: {prompt['data']}\")\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Preprocess image\n",
    "            with torch.enable_grad():\n",
    "                img = image[0].permute(1, 2, 0).cpu().numpy() * np.array([0.229, 0.224, 0.225])[None, None, :] + np.array([0.485, 0.456, 0.406])[None, None, :]\n",
    "                img = torch.from_numpy(img).permute(2, 0, 1).float().to(device)[None]\n",
    "                \n",
    "                # Prepare prompts\n",
    "                if prompt['type'] == 'point':\n",
    "                    if isinstance(prompt['data'], torch.Tensor):\n",
    "                        point_coords = prompt['data'].float().to(device)[None]\n",
    "                        point_labels = prompt['label'].float().to(device)[None] if isinstance(prompt['label'], torch.Tensor) else torch.from_numpy(prompt['label']).float().to(device)[None]\n",
    "                    else:\n",
    "                        point_coords = torch.from_numpy(prompt['data']).float().to(device)[None]\n",
    "                        point_labels = torch.from_numpy(prompt['label']).float().to(device)[None]\n",
    "                    box = None\n",
    "                else:\n",
    "                    if isinstance(prompt['data'], torch.Tensor):\n",
    "                        box = prompt['data'].float().to(device)[None]\n",
    "                    else:\n",
    "                        box = torch.from_numpy(prompt['data'][0]).float().to(device)[None]\n",
    "                    point_coords = None\n",
    "                    point_labels = None\n",
    "                \n",
    "                # Use predictor.predict for training\n",
    "                try:\n",
    "                    predictor.set_image(img[0].permute(1, 2, 0).cpu().numpy())\n",
    "                    with torch.enable_grad():\n",
    "                        masks, scores, logits = predictor.predict(\n",
    "                            point_coords=point_coords[0].cpu().numpy() if point_coords is not None else None,\n",
    "                            point_labels=point_labels[0].cpu().numpy() if point_labels is not None else None,\n",
    "                            box=box[0].cpu().numpy() if box is not None else None,\n",
    "                            multimask_output=False\n",
    "                        )\n",
    "                    pred_mask = torch.from_numpy(masks[0]).float().to(device).requires_grad_(True)\n",
    "                    pred_mask = torch.sigmoid(pred_mask)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Predictor failed: {str(e)}\")\n",
    "                    raise\n",
    "                \n",
    "                # Ensure pred_mask shape\n",
    "                if pred_mask.shape != gt_mask.shape:\n",
    "                    pred_mask = torch.nn.functional.interpolate(\n",
    "                        pred_mask[None, None], size=gt_mask.shape, mode='bilinear', align_corners=False\n",
    "                    ).squeeze()\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = torch.nn.functional.binary_cross_entropy(pred_mask, gt_mask)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            print(f\"Training: pred_mask unique: {torch.unique(pred_mask > 0.5)}, Shape: {pred_mask.shape}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        if checkpoint_dir:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"sam2_epoch_{epoch+1}.pt\")\n",
    "            print(f\"Saving checkpoint: {checkpoint_path}\")\n",
    "            try:\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(f\"Successfully saved checkpoint: {checkpoint_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving checkpoint {checkpoint_path}: {str(e)}\")\n",
    "\n",
    "# Plot scores\n",
    "def plot_scores(metrics_dict, title, output_path):\n",
    "    metrics = ['mean_iou', 'mean_precision', 'mean_recall', 'mean_f1']\n",
    "    labels = ['IoU', 'Precision', 'Recall', 'F1']\n",
    "    values = [float(metrics_dict[m]) for m in metrics]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    bars = plt.bar(labels, values, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "    plt.title(title)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval + 0.02, f\"{yval:.2f}\", ha='center', va='bottom')\n",
    "    \n",
    "    print(f\"Saving plot: {output_path}\")\n",
    "    try:\n",
    "        plt.savefig(output_path, bbox_inches='tight')\n",
    "        print(f\"Successfully saved plot: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving plot {output_path}: {str(e)}\")\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    dataset_root = \"D:/College/George Brown/industry project/Concrete Dataset Sample/SAM2 dataset\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    checkpoint_path = \"D:/College/George Brown/industry project/Concrete Dataset Sample/segment-anything-2/checkpoints/sam2_hiera_base_plus.pt\"\n",
    "    model_cfg = \"sam2_hiera_b+.yaml\"\n",
    "    config_dir = \"segment-anything-2\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_root = os.path.join(dataset_root, f\"outputs_{timestamp}\")\n",
    "    os.makedirs(output_root, exist_ok=True)\n",
    "    val_output_dir = os.path.join(output_root, \"val_predictions\")\n",
    "    test_output_dir = os.path.join(output_root, \"test_predictions\")\n",
    "    checkpoint_dir = os.path.join(output_root, \"checkpoints\")\n",
    "    os.makedirs(val_output_dir, exist_ok=True)\n",
    "    os.makedirs(test_output_dir, exist_ok=True)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Log output directories\n",
    "    print(f\"Output root: {output_root}\")\n",
    "    print(f\"Validation predictions: {val_output_dir}\")\n",
    "    print(f\"Test predictions: {test_output_dir}\")\n",
    "    print(f\"Checkpoints: {checkpoint_dir}\")\n",
    "    \n",
    "    # Clear Hydra global state\n",
    "    GlobalHydra.instance().clear()\n",
    "    \n",
    "    with initialize(config_path=config_dir, version_base=None):\n",
    "        cfg = compose(config_name=model_cfg)\n",
    "        print(\"Hydra config loaded successfully!\")\n",
    "        \n",
    "        sam2_model = build_sam2(model_cfg, checkpoint_path, device=device)\n",
    "        predictor = SAM2ImagePredictor(sam2_model)\n",
    "        \n",
    "        train_dataset = COCODataset(\n",
    "            root_dir=os.path.join(dataset_root, \"train\"),\n",
    "            annotation_file=os.path.join(dataset_root, \"train\", \"_annotations.coco.json\"),\n",
    "            transform=get_transform(augmentation=False)\n",
    "        )\n",
    "        val_dataset = COCODataset(\n",
    "            root_dir=os.path.join(dataset_root, \"valid\"),\n",
    "            annotation_file=os.path.join(dataset_root, \"valid\", \"_annotations.coco.json\"),\n",
    "            transform=get_transform(augmentation=False)\n",
    "        )\n",
    "        test_dataset = COCODataset(\n",
    "            root_dir=os.path.join(dataset_root, \"test\"),\n",
    "            annotation_file=os.path.join(dataset_root, \"test\", \"_annotations.coco.json\"),\n",
    "            transform=get_transform(augmentation=False)\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0, collate_fn=custom_collate_fn)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=custom_collate_fn)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=custom_collate_fn)\n",
    "        \n",
    "        print(\"Evaluating prompt strategies on validation set...\")\n",
    "        point_metrics = evaluate_prompt_strategy(predictor, val_loader, 'point', device, val_output_dir)\n",
    "        box_metrics = evaluate_prompt_strategy(predictor, val_loader, 'box', device, val_output_dir)\n",
    "        \n",
    "        print(\"Point Prompt Metrics:\", point_metrics)\n",
    "        print(\"Box Prompt Metrics:\", box_metrics)\n",
    "        \n",
    "        plot_scores(point_metrics, \"Validation Point Prompt Metrics\", os.path.join(output_root, \"val_point_metrics.png\"))\n",
    "        plot_scores(box_metrics, \"Validation Box Prompt Metrics\", os.path.join(output_root, \"val_box_metrics.png\"))\n",
    "        \n",
    "        best_prompt = 'point' if point_metrics['mean_f1'] > box_metrics['mean_f1'] else 'box'\n",
    "        print(f\"Selected prompt strategy: {best_prompt}\")\n",
    "        \n",
    "        print(\"\\nTesting data augmentation...\")\n",
    "        val_dataset_aug = COCODataset(\n",
    "            root_dir=os.path.join(dataset_root, \"valid\"),\n",
    "            annotation_file=os.path.join(dataset_root, \"valid\", \"_annotations.coco.json\"),\n",
    "            transform=get_transform(augmentation=True)\n",
    "        )\n",
    "        val_loader_aug = DataLoader(val_dataset_aug, batch_size=1, shuffle=False, num_workers=0, collate_fn=custom_collate_fn)\n",
    "        \n",
    "        aug_metrics = evaluate_prompt_strategy(predictor, val_loader_aug, best_prompt, device, val_output_dir)\n",
    "        print(\"Augmented Validation Metrics:\", aug_metrics)\n",
    "        plot_scores(aug_metrics, f\"Validation {best_prompt.capitalize()} Prompt with Augmentation\", \n",
    "                    os.path.join(output_root, \"val_aug_metrics.png\"))\n",
    "        \n",
    "        baseline_f1 = point_metrics['mean_f1'] if best_prompt == 'point' else box_metrics['mean_f1']\n",
    "        use_augmentation = aug_metrics['mean_f1'] > baseline_f1\n",
    "        print(f\"Using data augmentation: {use_augmentation}\")\n",
    "        \n",
    "        if use_augmentation:\n",
    "            train_dataset.transform = get_transform(augmentation=True)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0, collate_fn=custom_collate_fn)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(sam2_model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "        print(\"\\nTraining SAM2...\")\n",
    "        train_sam2(predictor, train_loader, optimizer, device, epochs=10, checkpoint_dir=checkpoint_dir)\n",
    "        \n",
    "        print(\"\\nEvaluating final model on test set...\")\n",
    "        test_metrics = evaluate_prompt_strategy(predictor, test_loader, best_prompt, device, test_output_dir)\n",
    "        print(f\"Test Set Metrics with {best_prompt} prompts:\")\n",
    "        print(f\"Mean IoU: {test_metrics['mean_iou']:.4f}\")\n",
    "        print(f\"Mean Precision: {test_metrics['mean_precision']:.4f}\")\n",
    "        print(f\"Mean Recall: {test_metrics['mean_recall']:.4f}\")\n",
    "        print(f\"Mean F1: {test_metrics['mean_f1']:.4f}\")\n",
    "        \n",
    "        plot_scores(test_metrics, f\"Test {best_prompt.capitalize()} Prompt Metrics\", \n",
    "                    os.path.join(output_root, \"test_metrics.png\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
